{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torcheval.metrics import BinaryAccuracy\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "data_train = datasets.MNIST(root=\"./dataset\",train=True,download=True,transform=transforms.ToTensor())\n",
    "data_test = datasets.MNIST(root=\"./dataset\",train=False,download=True,transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "image, label = data_train[0]\n",
    "\n",
    "plt.imshow(image.squeeze().numpy(),cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = []\n",
    "\n",
    "for x , y in data_train:\n",
    "\n",
    "    list = [0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "    list[y] = 1\n",
    "    \n",
    "    y = np.array(list)\n",
    "\n",
    "    x = x.squeeze().reshape(-1).numpy()\n",
    "\n",
    "    train_dataset.append([x,y])\n",
    "\n",
    "test_dataset = []\n",
    "\n",
    "for x , y in data_test:\n",
    "\n",
    "    list = [0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "    list[y] = 1\n",
    "    \n",
    "    y = np.array(list)\n",
    "\n",
    "    x = x.squeeze().reshape(-1).numpy()\n",
    "\n",
    "    test_dataset.append([x,y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import dataset, DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=600,shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=100,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class UnKownSoftMaxNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(UnKownSoftMaxNet, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(784,10)\n",
    "        self.activation = nn.Softmax(1)\n",
    "        \n",
    "    def forward(self,input):\n",
    "        \n",
    "        rfc1 = self.fc1(input)\n",
    "        \n",
    "        output = self.activation(rfc1)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "net = UnKownSoftMaxNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(),lr=0.01)\n",
    "\n",
    "loss_fc = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_tensor(tensor):\n",
    "    \n",
    "    v_min, v_max = tensor.min(), tensor.max()\n",
    "    \n",
    "    new_min,new_max = 0,255\n",
    "    \n",
    "    v_p = (tensor - v_min)/(v_max - v_min)*(new_max - new_min) + new_min\n",
    "    \n",
    "    return v_p\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheval.metrics import MulticlassAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    2/2000 train_loss: 2.293874 acc: 0.170400\n",
      "Epoch    3/2000 train_loss: 2.276844 acc: 0.264500\n",
      "Epoch    4/2000 train_loss: 2.253322 acc: 0.260600\n",
      "Epoch    5/2000 train_loss: 2.229254 acc: 0.342300\n",
      "Epoch    6/2000 train_loss: 2.205744 acc: 0.473500\n",
      "Epoch    7/2000 train_loss: 2.179636 acc: 0.560200\n",
      "Epoch    8/2000 train_loss: 2.149758 acc: 0.596900\n",
      "Epoch    9/2000 train_loss: 2.117160 acc: 0.604700\n",
      "Epoch   10/2000 train_loss: 2.084366 acc: 0.609600\n",
      "Epoch   11/2000 train_loss: 2.053657 acc: 0.623500\n",
      "Epoch   12/2000 train_loss: 2.025968 acc: 0.643400\n",
      "Epoch   13/2000 train_loss: 2.001244 acc: 0.665800\n",
      "Epoch   14/2000 train_loss: 1.979060 acc: 0.691300\n",
      "Epoch   15/2000 train_loss: 1.958950 acc: 0.714700\n",
      "Epoch   16/2000 train_loss: 1.940591 acc: 0.733800\n",
      "Epoch   17/2000 train_loss: 1.923859 acc: 0.746300\n",
      "Epoch   18/2000 train_loss: 1.908713 acc: 0.756800\n",
      "Epoch   19/2000 train_loss: 1.895064 acc: 0.764700\n",
      "Epoch   20/2000 train_loss: 1.882762 acc: 0.768300\n",
      "Epoch   21/2000 train_loss: 1.871638 acc: 0.772300\n",
      "Epoch   22/2000 train_loss: 1.861548 acc: 0.775000\n",
      "Epoch   23/2000 train_loss: 1.852366 acc: 0.778000\n",
      "Epoch   24/2000 train_loss: 1.843983 acc: 0.779500\n",
      "Epoch   25/2000 train_loss: 1.836304 acc: 0.781900\n",
      "Epoch   26/2000 train_loss: 1.829246 acc: 0.783300\n",
      "Epoch   27/2000 train_loss: 1.822739 acc: 0.784700\n",
      "Epoch   28/2000 train_loss: 1.816719 acc: 0.787000\n",
      "Epoch   29/2000 train_loss: 1.811134 acc: 0.788700\n",
      "Epoch   30/2000 train_loss: 1.805937 acc: 0.790600\n",
      "Epoch   31/2000 train_loss: 1.801087 acc: 0.791800\n",
      "Epoch   32/2000 train_loss: 1.796553 acc: 0.793600\n",
      "Epoch   33/2000 train_loss: 1.792300 acc: 0.794200\n",
      "Epoch   34/2000 train_loss: 1.788304 acc: 0.794900\n",
      "Epoch   35/2000 train_loss: 1.784537 acc: 0.796100\n",
      "Epoch   36/2000 train_loss: 1.780984 acc: 0.797100\n",
      "Epoch   37/2000 train_loss: 1.777626 acc: 0.797600\n",
      "Epoch   38/2000 train_loss: 1.774443 acc: 0.798800\n",
      "Epoch   39/2000 train_loss: 1.771422 acc: 0.799400\n",
      "Epoch   40/2000 train_loss: 1.768553 acc: 0.800800\n",
      "Epoch   41/2000 train_loss: 1.765820 acc: 0.801300\n",
      "Epoch   42/2000 train_loss: 1.763218 acc: 0.802100\n",
      "Epoch   43/2000 train_loss: 1.760732 acc: 0.802900\n",
      "Epoch   44/2000 train_loss: 1.758356 acc: 0.803900\n",
      "Epoch   45/2000 train_loss: 1.756084 acc: 0.804300\n",
      "Epoch   46/2000 train_loss: 1.753908 acc: 0.805200\n",
      "Epoch   47/2000 train_loss: 1.751820 acc: 0.805500\n",
      "Epoch   48/2000 train_loss: 1.749816 acc: 0.806500\n",
      "Epoch   49/2000 train_loss: 1.747890 acc: 0.806600\n",
      "Epoch   50/2000 train_loss: 1.746038 acc: 0.807000\n",
      "Epoch   51/2000 train_loss: 1.744255 acc: 0.807600\n",
      "Epoch   52/2000 train_loss: 1.742536 acc: 0.807900\n",
      "Epoch   53/2000 train_loss: 1.740879 acc: 0.808800\n",
      "Epoch   54/2000 train_loss: 1.739280 acc: 0.809300\n",
      "Epoch   55/2000 train_loss: 1.737735 acc: 0.810200\n",
      "Epoch   56/2000 train_loss: 1.736244 acc: 0.810300\n",
      "Epoch   57/2000 train_loss: 1.734800 acc: 0.811200\n",
      "Epoch   58/2000 train_loss: 1.733403 acc: 0.811600\n",
      "Epoch   59/2000 train_loss: 1.732049 acc: 0.812600\n",
      "Epoch   60/2000 train_loss: 1.730737 acc: 0.812600\n",
      "Epoch   61/2000 train_loss: 1.729465 acc: 0.813500\n",
      "Epoch   62/2000 train_loss: 1.728230 acc: 0.813300\n",
      "Epoch   63/2000 train_loss: 1.727033 acc: 0.813800\n",
      "Epoch   64/2000 train_loss: 1.725869 acc: 0.813800\n",
      "Epoch   65/2000 train_loss: 1.724738 acc: 0.814100\n",
      "Epoch   66/2000 train_loss: 1.723638 acc: 0.814600\n",
      "Epoch   67/2000 train_loss: 1.722568 acc: 0.815000\n",
      "Epoch   68/2000 train_loss: 1.721526 acc: 0.815600\n",
      "Epoch   69/2000 train_loss: 1.720513 acc: 0.815900\n",
      "Epoch   70/2000 train_loss: 1.719525 acc: 0.816200\n",
      "Epoch   71/2000 train_loss: 1.718562 acc: 0.816600\n",
      "Epoch   72/2000 train_loss: 1.717624 acc: 0.816900\n",
      "Epoch   73/2000 train_loss: 1.716708 acc: 0.816700\n",
      "Epoch   74/2000 train_loss: 1.715814 acc: 0.817000\n",
      "Epoch   75/2000 train_loss: 1.714942 acc: 0.817600\n",
      "Epoch   76/2000 train_loss: 1.714089 acc: 0.817600\n",
      "Epoch   77/2000 train_loss: 1.713257 acc: 0.818100\n",
      "Epoch   78/2000 train_loss: 1.712444 acc: 0.818300\n",
      "Epoch   79/2000 train_loss: 1.711649 acc: 0.818400\n",
      "Epoch   80/2000 train_loss: 1.710872 acc: 0.818700\n",
      "Epoch   81/2000 train_loss: 1.710112 acc: 0.819100\n",
      "Epoch   82/2000 train_loss: 1.709368 acc: 0.819400\n",
      "Epoch   83/2000 train_loss: 1.708639 acc: 0.819700\n",
      "Epoch   84/2000 train_loss: 1.707926 acc: 0.819700\n",
      "Epoch   85/2000 train_loss: 1.707227 acc: 0.820300\n",
      "Epoch   86/2000 train_loss: 1.706544 acc: 0.820400\n",
      "Epoch   87/2000 train_loss: 1.705874 acc: 0.820300\n",
      "Epoch   88/2000 train_loss: 1.705216 acc: 0.820600\n",
      "Epoch   89/2000 train_loss: 1.704571 acc: 0.820900\n",
      "Epoch   90/2000 train_loss: 1.703940 acc: 0.821000\n",
      "Epoch   91/2000 train_loss: 1.703320 acc: 0.821200\n",
      "Epoch   92/2000 train_loss: 1.702711 acc: 0.821400\n",
      "Epoch   93/2000 train_loss: 1.702114 acc: 0.821300\n",
      "Epoch   94/2000 train_loss: 1.701528 acc: 0.821700\n",
      "Epoch   95/2000 train_loss: 1.700953 acc: 0.821800\n",
      "Epoch   96/2000 train_loss: 1.700387 acc: 0.822000\n",
      "Epoch   97/2000 train_loss: 1.699831 acc: 0.822200\n",
      "Epoch   98/2000 train_loss: 1.699286 acc: 0.822400\n",
      "Epoch   99/2000 train_loss: 1.698751 acc: 0.822700\n",
      "Epoch  100/2000 train_loss: 1.698223 acc: 0.823000\n",
      "Epoch  101/2000 train_loss: 1.697705 acc: 0.823200\n",
      "Epoch  102/2000 train_loss: 1.697196 acc: 0.822900\n",
      "Epoch  103/2000 train_loss: 1.696695 acc: 0.823300\n",
      "Epoch  104/2000 train_loss: 1.696202 acc: 0.823200\n",
      "Epoch  105/2000 train_loss: 1.695716 acc: 0.823200\n",
      "Epoch  106/2000 train_loss: 1.695239 acc: 0.823600\n",
      "Epoch  107/2000 train_loss: 1.694769 acc: 0.823600\n",
      "Epoch  108/2000 train_loss: 1.694306 acc: 0.824100\n",
      "Epoch  109/2000 train_loss: 1.693851 acc: 0.824000\n",
      "Epoch  110/2000 train_loss: 1.693403 acc: 0.824300\n",
      "Epoch  111/2000 train_loss: 1.692962 acc: 0.824400\n",
      "Epoch  112/2000 train_loss: 1.692527 acc: 0.824500\n",
      "Epoch  113/2000 train_loss: 1.692099 acc: 0.825000\n",
      "Epoch  114/2000 train_loss: 1.691675 acc: 0.825200\n",
      "Epoch  115/2000 train_loss: 1.691260 acc: 0.825200\n",
      "Epoch  116/2000 train_loss: 1.690849 acc: 0.825200\n",
      "Epoch  117/2000 train_loss: 1.690444 acc: 0.825500\n",
      "Epoch  118/2000 train_loss: 1.690046 acc: 0.825800\n",
      "Epoch  119/2000 train_loss: 1.689653 acc: 0.826000\n",
      "Epoch  120/2000 train_loss: 1.689265 acc: 0.826000\n",
      "Epoch  121/2000 train_loss: 1.688883 acc: 0.825900\n",
      "Epoch  122/2000 train_loss: 1.688505 acc: 0.826000\n",
      "Epoch  123/2000 train_loss: 1.688132 acc: 0.826200\n",
      "Epoch  124/2000 train_loss: 1.687765 acc: 0.826400\n",
      "Epoch  125/2000 train_loss: 1.687403 acc: 0.826500\n",
      "Epoch  126/2000 train_loss: 1.687045 acc: 0.826400\n",
      "Epoch  127/2000 train_loss: 1.686692 acc: 0.826800\n",
      "Epoch  128/2000 train_loss: 1.686343 acc: 0.826700\n",
      "Epoch  129/2000 train_loss: 1.685999 acc: 0.826600\n",
      "Epoch  130/2000 train_loss: 1.685659 acc: 0.826700\n",
      "Epoch  131/2000 train_loss: 1.685322 acc: 0.826900\n",
      "Epoch  132/2000 train_loss: 1.684991 acc: 0.826900\n",
      "Epoch  133/2000 train_loss: 1.684664 acc: 0.826900\n",
      "Epoch  134/2000 train_loss: 1.684341 acc: 0.827000\n",
      "Epoch  135/2000 train_loss: 1.684020 acc: 0.827000\n",
      "Epoch  136/2000 train_loss: 1.683704 acc: 0.827000\n",
      "Epoch  137/2000 train_loss: 1.683393 acc: 0.827200\n",
      "Epoch  138/2000 train_loss: 1.683083 acc: 0.827600\n",
      "Epoch  139/2000 train_loss: 1.682778 acc: 0.827500\n",
      "Epoch  140/2000 train_loss: 1.682477 acc: 0.827300\n",
      "Epoch  141/2000 train_loss: 1.682179 acc: 0.827700\n",
      "Epoch  142/2000 train_loss: 1.681884 acc: 0.827800\n",
      "Epoch  143/2000 train_loss: 1.681593 acc: 0.827800\n",
      "Epoch  144/2000 train_loss: 1.681305 acc: 0.827900\n",
      "Epoch  145/2000 train_loss: 1.681019 acc: 0.828000\n",
      "Epoch  146/2000 train_loss: 1.680737 acc: 0.828100\n",
      "Epoch  147/2000 train_loss: 1.680457 acc: 0.828000\n",
      "Epoch  148/2000 train_loss: 1.680183 acc: 0.828100\n",
      "Epoch  149/2000 train_loss: 1.679909 acc: 0.828200\n",
      "Epoch  150/2000 train_loss: 1.679638 acc: 0.828100\n",
      "Epoch  151/2000 train_loss: 1.679371 acc: 0.828100\n",
      "Epoch  152/2000 train_loss: 1.679107 acc: 0.828300\n",
      "Epoch  153/2000 train_loss: 1.678844 acc: 0.828500\n",
      "Epoch  154/2000 train_loss: 1.678586 acc: 0.828700\n",
      "Epoch  155/2000 train_loss: 1.678329 acc: 0.828800\n",
      "Epoch  156/2000 train_loss: 1.678075 acc: 0.828800\n",
      "Epoch  157/2000 train_loss: 1.677822 acc: 0.829100\n",
      "Epoch  158/2000 train_loss: 1.677574 acc: 0.829200\n",
      "Epoch  159/2000 train_loss: 1.677328 acc: 0.829100\n",
      "Epoch  160/2000 train_loss: 1.677083 acc: 0.829400\n",
      "Epoch  161/2000 train_loss: 1.676841 acc: 0.829800\n",
      "Epoch  162/2000 train_loss: 1.676602 acc: 0.829800\n",
      "Epoch  163/2000 train_loss: 1.676363 acc: 0.829900\n",
      "Epoch  164/2000 train_loss: 1.676129 acc: 0.829600\n",
      "Epoch  165/2000 train_loss: 1.675895 acc: 0.829900\n",
      "Epoch  166/2000 train_loss: 1.675664 acc: 0.830000\n",
      "Epoch  167/2000 train_loss: 1.675436 acc: 0.829900\n",
      "Epoch  168/2000 train_loss: 1.675208 acc: 0.830100\n",
      "Epoch  169/2000 train_loss: 1.674984 acc: 0.830400\n",
      "Epoch  170/2000 train_loss: 1.674761 acc: 0.830300\n",
      "Epoch  171/2000 train_loss: 1.674539 acc: 0.830500\n",
      "Epoch  172/2000 train_loss: 1.674321 acc: 0.830500\n",
      "Epoch  173/2000 train_loss: 1.674103 acc: 0.830600\n",
      "Epoch  174/2000 train_loss: 1.673888 acc: 0.830500\n",
      "Epoch  175/2000 train_loss: 1.673675 acc: 0.831000\n",
      "Epoch  176/2000 train_loss: 1.673463 acc: 0.830800\n",
      "Epoch  177/2000 train_loss: 1.673253 acc: 0.830600\n",
      "Epoch  178/2000 train_loss: 1.673044 acc: 0.830800\n",
      "Epoch  179/2000 train_loss: 1.672838 acc: 0.830900\n",
      "Epoch  180/2000 train_loss: 1.672633 acc: 0.830900\n",
      "Epoch  181/2000 train_loss: 1.672429 acc: 0.831000\n",
      "Epoch  182/2000 train_loss: 1.672226 acc: 0.831200\n",
      "Epoch  183/2000 train_loss: 1.672027 acc: 0.831100\n",
      "Epoch  184/2000 train_loss: 1.671827 acc: 0.830800\n",
      "Epoch  185/2000 train_loss: 1.671630 acc: 0.831200\n",
      "Epoch  186/2000 train_loss: 1.671434 acc: 0.831400\n",
      "Epoch  187/2000 train_loss: 1.671239 acc: 0.831300\n",
      "Epoch  188/2000 train_loss: 1.671045 acc: 0.831600\n",
      "Epoch  189/2000 train_loss: 1.670853 acc: 0.831600\n",
      "Epoch  190/2000 train_loss: 1.670663 acc: 0.831600\n",
      "Epoch  191/2000 train_loss: 1.670473 acc: 0.831500\n",
      "Epoch  192/2000 train_loss: 1.670284 acc: 0.831800\n",
      "Epoch  193/2000 train_loss: 1.670097 acc: 0.831600\n",
      "Epoch  194/2000 train_loss: 1.669911 acc: 0.831900\n",
      "Epoch  195/2000 train_loss: 1.669726 acc: 0.831900\n",
      "Epoch  196/2000 train_loss: 1.669541 acc: 0.831800\n",
      "Epoch  197/2000 train_loss: 1.669358 acc: 0.832000\n",
      "Epoch  198/2000 train_loss: 1.669177 acc: 0.832100\n",
      "Epoch  199/2000 train_loss: 1.668996 acc: 0.832100\n",
      "Epoch  200/2000 train_loss: 1.668816 acc: 0.832200\n",
      "Epoch  201/2000 train_loss: 1.668636 acc: 0.832300\n",
      "Epoch  202/2000 train_loss: 1.668457 acc: 0.832600\n",
      "Epoch  203/2000 train_loss: 1.668279 acc: 0.832400\n",
      "Epoch  204/2000 train_loss: 1.668102 acc: 0.832600\n",
      "Epoch  205/2000 train_loss: 1.667925 acc: 0.832700\n",
      "Epoch  206/2000 train_loss: 1.667749 acc: 0.832600\n",
      "Epoch  207/2000 train_loss: 1.667574 acc: 0.832600\n",
      "Epoch  208/2000 train_loss: 1.667398 acc: 0.832700\n",
      "Epoch  209/2000 train_loss: 1.667223 acc: 0.832800\n",
      "Epoch  210/2000 train_loss: 1.667049 acc: 0.832800\n",
      "Epoch  211/2000 train_loss: 1.666873 acc: 0.832900\n",
      "Epoch  212/2000 train_loss: 1.666699 acc: 0.832800\n",
      "Epoch  213/2000 train_loss: 1.666525 acc: 0.832900\n",
      "Epoch  214/2000 train_loss: 1.666349 acc: 0.833100\n",
      "Epoch  215/2000 train_loss: 1.666175 acc: 0.833100\n",
      "Epoch  216/2000 train_loss: 1.665998 acc: 0.833300\n",
      "Epoch  217/2000 train_loss: 1.665822 acc: 0.833400\n",
      "Epoch  218/2000 train_loss: 1.665645 acc: 0.833600\n",
      "Epoch  219/2000 train_loss: 1.665466 acc: 0.833600\n",
      "Epoch  220/2000 train_loss: 1.665286 acc: 0.833700\n",
      "Epoch  221/2000 train_loss: 1.665105 acc: 0.833800\n",
      "Epoch  222/2000 train_loss: 1.664921 acc: 0.833800\n",
      "Epoch  223/2000 train_loss: 1.664735 acc: 0.833800\n",
      "Epoch  224/2000 train_loss: 1.664547 acc: 0.834000\n",
      "Epoch  225/2000 train_loss: 1.664356 acc: 0.833900\n",
      "Epoch  226/2000 train_loss: 1.664160 acc: 0.833900\n",
      "Epoch  227/2000 train_loss: 1.663960 acc: 0.833900\n",
      "Epoch  228/2000 train_loss: 1.663756 acc: 0.834300\n",
      "Epoch  229/2000 train_loss: 1.663544 acc: 0.834400\n",
      "Epoch  230/2000 train_loss: 1.663327 acc: 0.834600\n",
      "Epoch  231/2000 train_loss: 1.663101 acc: 0.834400\n",
      "Epoch  232/2000 train_loss: 1.662864 acc: 0.834500\n",
      "Epoch  233/2000 train_loss: 1.662617 acc: 0.834700\n",
      "Epoch  234/2000 train_loss: 1.662355 acc: 0.834900\n",
      "Epoch  235/2000 train_loss: 1.662077 acc: 0.835400\n",
      "Epoch  236/2000 train_loss: 1.661779 acc: 0.835700\n",
      "Epoch  237/2000 train_loss: 1.661459 acc: 0.835900\n",
      "Epoch  238/2000 train_loss: 1.661110 acc: 0.836400\n",
      "Epoch  239/2000 train_loss: 1.660728 acc: 0.836700\n",
      "Epoch  240/2000 train_loss: 1.660307 acc: 0.837300\n",
      "Epoch  241/2000 train_loss: 1.659839 acc: 0.837800\n",
      "Epoch  242/2000 train_loss: 1.659317 acc: 0.838600\n",
      "Epoch  243/2000 train_loss: 1.658733 acc: 0.839600\n",
      "Epoch  244/2000 train_loss: 1.658078 acc: 0.840600\n",
      "Epoch  245/2000 train_loss: 1.657348 acc: 0.841700\n",
      "Epoch  246/2000 train_loss: 1.656541 acc: 0.843800\n",
      "Epoch  247/2000 train_loss: 1.655661 acc: 0.844400\n",
      "Epoch  248/2000 train_loss: 1.654721 acc: 0.845600\n",
      "Epoch  249/2000 train_loss: 1.653738 acc: 0.847000\n",
      "Epoch  250/2000 train_loss: 1.652739 acc: 0.848300\n",
      "Epoch  251/2000 train_loss: 1.651749 acc: 0.849600\n",
      "Epoch  252/2000 train_loss: 1.650791 acc: 0.851500\n",
      "Epoch  253/2000 train_loss: 1.649886 acc: 0.853000\n",
      "Epoch  254/2000 train_loss: 1.649041 acc: 0.854600\n",
      "Epoch  255/2000 train_loss: 1.648263 acc: 0.855900\n",
      "Epoch  256/2000 train_loss: 1.647543 acc: 0.857000\n",
      "Epoch  257/2000 train_loss: 1.646876 acc: 0.858600\n",
      "Epoch  258/2000 train_loss: 1.646250 acc: 0.859800\n",
      "Epoch  259/2000 train_loss: 1.645656 acc: 0.860900\n",
      "Epoch  260/2000 train_loss: 1.645086 acc: 0.861600\n",
      "Epoch  261/2000 train_loss: 1.644533 acc: 0.862200\n",
      "Epoch  262/2000 train_loss: 1.643992 acc: 0.862800\n",
      "Epoch  263/2000 train_loss: 1.643460 acc: 0.863000\n",
      "Epoch  264/2000 train_loss: 1.642936 acc: 0.863700\n",
      "Epoch  265/2000 train_loss: 1.642416 acc: 0.864200\n",
      "Epoch  266/2000 train_loss: 1.641899 acc: 0.864900\n",
      "Epoch  267/2000 train_loss: 1.641386 acc: 0.865300\n",
      "Epoch  268/2000 train_loss: 1.640876 acc: 0.865800\n",
      "Epoch  269/2000 train_loss: 1.640367 acc: 0.866200\n",
      "Epoch  270/2000 train_loss: 1.639862 acc: 0.867100\n",
      "Epoch  271/2000 train_loss: 1.639357 acc: 0.867400\n",
      "Epoch  272/2000 train_loss: 1.638855 acc: 0.867700\n",
      "Epoch  273/2000 train_loss: 1.638353 acc: 0.868300\n",
      "Epoch  274/2000 train_loss: 1.637854 acc: 0.868300\n",
      "Epoch  275/2000 train_loss: 1.637355 acc: 0.869000\n",
      "Epoch  276/2000 train_loss: 1.636858 acc: 0.869400\n",
      "Epoch  277/2000 train_loss: 1.636362 acc: 0.869300\n",
      "Epoch  278/2000 train_loss: 1.635868 acc: 0.869700\n",
      "Epoch  279/2000 train_loss: 1.635373 acc: 0.870400\n",
      "Epoch  280/2000 train_loss: 1.634881 acc: 0.870800\n",
      "Epoch  281/2000 train_loss: 1.634390 acc: 0.871300\n",
      "Epoch  282/2000 train_loss: 1.633900 acc: 0.871700\n",
      "Epoch  283/2000 train_loss: 1.633410 acc: 0.872600\n",
      "Epoch  284/2000 train_loss: 1.632922 acc: 0.873100\n",
      "Epoch  285/2000 train_loss: 1.632436 acc: 0.873600\n",
      "Epoch  286/2000 train_loss: 1.631951 acc: 0.874800\n",
      "Epoch  287/2000 train_loss: 1.631466 acc: 0.875100\n",
      "Epoch  288/2000 train_loss: 1.630984 acc: 0.876000\n",
      "Epoch  289/2000 train_loss: 1.630503 acc: 0.876500\n",
      "Epoch  290/2000 train_loss: 1.630023 acc: 0.877100\n",
      "Epoch  291/2000 train_loss: 1.629546 acc: 0.877700\n",
      "Epoch  292/2000 train_loss: 1.629070 acc: 0.878600\n",
      "Epoch  293/2000 train_loss: 1.628595 acc: 0.878800\n",
      "Epoch  294/2000 train_loss: 1.628123 acc: 0.879600\n",
      "Epoch  295/2000 train_loss: 1.627653 acc: 0.880200\n",
      "Epoch  296/2000 train_loss: 1.627185 acc: 0.880400\n",
      "Epoch  297/2000 train_loss: 1.626720 acc: 0.880800\n",
      "Epoch  298/2000 train_loss: 1.626257 acc: 0.880800\n",
      "Epoch  299/2000 train_loss: 1.625798 acc: 0.881000\n",
      "Epoch  300/2000 train_loss: 1.625341 acc: 0.881600\n",
      "Epoch  301/2000 train_loss: 1.624887 acc: 0.881900\n",
      "Epoch  302/2000 train_loss: 1.624436 acc: 0.882800\n",
      "Epoch  303/2000 train_loss: 1.623989 acc: 0.883300\n",
      "Epoch  304/2000 train_loss: 1.623546 acc: 0.883200\n",
      "Epoch  305/2000 train_loss: 1.623106 acc: 0.883600\n",
      "Epoch  306/2000 train_loss: 1.622671 acc: 0.883900\n",
      "Epoch  307/2000 train_loss: 1.622239 acc: 0.884600\n",
      "Epoch  308/2000 train_loss: 1.621809 acc: 0.884900\n",
      "Epoch  309/2000 train_loss: 1.621387 acc: 0.885600\n",
      "Epoch  310/2000 train_loss: 1.620968 acc: 0.885900\n",
      "Epoch  311/2000 train_loss: 1.620553 acc: 0.886800\n",
      "Epoch  312/2000 train_loss: 1.620144 acc: 0.887000\n",
      "Epoch  313/2000 train_loss: 1.619739 acc: 0.886900\n",
      "Epoch  314/2000 train_loss: 1.619339 acc: 0.887300\n",
      "Epoch  315/2000 train_loss: 1.618944 acc: 0.887500\n",
      "Epoch  316/2000 train_loss: 1.618556 acc: 0.888000\n",
      "Epoch  317/2000 train_loss: 1.618171 acc: 0.887800\n",
      "Epoch  318/2000 train_loss: 1.617793 acc: 0.888000\n",
      "Epoch  319/2000 train_loss: 1.617421 acc: 0.888400\n",
      "Epoch  320/2000 train_loss: 1.617054 acc: 0.889000\n",
      "Epoch  321/2000 train_loss: 1.616692 acc: 0.889500\n",
      "Epoch  322/2000 train_loss: 1.616336 acc: 0.889600\n",
      "Epoch  323/2000 train_loss: 1.615986 acc: 0.889900\n",
      "Epoch  324/2000 train_loss: 1.615640 acc: 0.890500\n",
      "Epoch  325/2000 train_loss: 1.615303 acc: 0.891000\n",
      "Epoch  326/2000 train_loss: 1.614971 acc: 0.891300\n",
      "Epoch  327/2000 train_loss: 1.614644 acc: 0.891800\n",
      "Epoch  328/2000 train_loss: 1.614323 acc: 0.892300\n",
      "Epoch  329/2000 train_loss: 1.614007 acc: 0.892400\n",
      "Epoch  330/2000 train_loss: 1.613698 acc: 0.892600\n",
      "Epoch  331/2000 train_loss: 1.613395 acc: 0.893000\n",
      "Epoch  332/2000 train_loss: 1.613096 acc: 0.892900\n",
      "Epoch  333/2000 train_loss: 1.612803 acc: 0.893300\n",
      "Epoch  334/2000 train_loss: 1.612516 acc: 0.893400\n",
      "Epoch  335/2000 train_loss: 1.612233 acc: 0.893900\n",
      "Epoch  336/2000 train_loss: 1.611956 acc: 0.894300\n",
      "Epoch  337/2000 train_loss: 1.611684 acc: 0.894800\n",
      "Epoch  338/2000 train_loss: 1.611416 acc: 0.894900\n",
      "Epoch  339/2000 train_loss: 1.611154 acc: 0.895300\n",
      "Epoch  340/2000 train_loss: 1.610896 acc: 0.895400\n",
      "Epoch  341/2000 train_loss: 1.610643 acc: 0.895700\n",
      "Epoch  342/2000 train_loss: 1.610394 acc: 0.895800\n",
      "Epoch  343/2000 train_loss: 1.610149 acc: 0.896000\n",
      "Epoch  344/2000 train_loss: 1.609908 acc: 0.896000\n",
      "Epoch  345/2000 train_loss: 1.609672 acc: 0.896000\n",
      "Epoch  346/2000 train_loss: 1.609439 acc: 0.896000\n",
      "Epoch  347/2000 train_loss: 1.609210 acc: 0.895900\n",
      "Epoch  348/2000 train_loss: 1.608984 acc: 0.896500\n",
      "Epoch  349/2000 train_loss: 1.608763 acc: 0.896600\n",
      "Epoch  350/2000 train_loss: 1.608545 acc: 0.897000\n",
      "Epoch  351/2000 train_loss: 1.608330 acc: 0.897100\n",
      "Epoch  352/2000 train_loss: 1.608118 acc: 0.897500\n",
      "Epoch  353/2000 train_loss: 1.607910 acc: 0.897800\n",
      "Epoch  354/2000 train_loss: 1.607704 acc: 0.898000\n",
      "Epoch  355/2000 train_loss: 1.607501 acc: 0.898100\n",
      "Epoch  356/2000 train_loss: 1.607302 acc: 0.898300\n",
      "Epoch  357/2000 train_loss: 1.607104 acc: 0.898700\n",
      "Epoch  358/2000 train_loss: 1.606910 acc: 0.898800\n",
      "Epoch  359/2000 train_loss: 1.606718 acc: 0.898800\n",
      "Epoch  360/2000 train_loss: 1.606530 acc: 0.898900\n",
      "Epoch  361/2000 train_loss: 1.606342 acc: 0.898900\n",
      "Epoch  362/2000 train_loss: 1.606158 acc: 0.899300\n",
      "Epoch  363/2000 train_loss: 1.605975 acc: 0.899800\n",
      "Epoch  364/2000 train_loss: 1.605796 acc: 0.899600\n",
      "Epoch  365/2000 train_loss: 1.605618 acc: 0.899500\n",
      "Epoch  366/2000 train_loss: 1.605442 acc: 0.899800\n",
      "Epoch  367/2000 train_loss: 1.605269 acc: 0.899700\n",
      "Epoch  368/2000 train_loss: 1.605097 acc: 0.899800\n",
      "Epoch  369/2000 train_loss: 1.604927 acc: 0.899900\n",
      "Epoch  370/2000 train_loss: 1.604760 acc: 0.899900\n",
      "Epoch  371/2000 train_loss: 1.604594 acc: 0.899800\n",
      "Epoch  372/2000 train_loss: 1.604430 acc: 0.899800\n",
      "Epoch  373/2000 train_loss: 1.604267 acc: 0.899900\n",
      "Epoch  374/2000 train_loss: 1.604106 acc: 0.900000\n",
      "Epoch  375/2000 train_loss: 1.603947 acc: 0.900100\n",
      "Epoch  376/2000 train_loss: 1.603790 acc: 0.900100\n",
      "Epoch  377/2000 train_loss: 1.603633 acc: 0.900200\n",
      "Epoch  378/2000 train_loss: 1.603479 acc: 0.900400\n",
      "Epoch  379/2000 train_loss: 1.603327 acc: 0.900400\n",
      "Epoch  380/2000 train_loss: 1.603175 acc: 0.900300\n",
      "Epoch  381/2000 train_loss: 1.603025 acc: 0.900700\n",
      "Epoch  382/2000 train_loss: 1.602876 acc: 0.900600\n",
      "Epoch  383/2000 train_loss: 1.602729 acc: 0.900900\n",
      "Epoch  384/2000 train_loss: 1.602583 acc: 0.900900\n",
      "Epoch  385/2000 train_loss: 1.602438 acc: 0.900900\n",
      "Epoch  386/2000 train_loss: 1.602294 acc: 0.901000\n",
      "Epoch  387/2000 train_loss: 1.602153 acc: 0.900900\n",
      "Epoch  388/2000 train_loss: 1.602011 acc: 0.900900\n",
      "Epoch  389/2000 train_loss: 1.601871 acc: 0.901000\n",
      "Epoch  390/2000 train_loss: 1.601733 acc: 0.901000\n",
      "Epoch  391/2000 train_loss: 1.601596 acc: 0.901000\n",
      "Epoch  392/2000 train_loss: 1.601459 acc: 0.901100\n",
      "Epoch  393/2000 train_loss: 1.601324 acc: 0.901100\n",
      "Epoch  394/2000 train_loss: 1.601190 acc: 0.901300\n",
      "Epoch  395/2000 train_loss: 1.601057 acc: 0.901300\n",
      "Epoch  396/2000 train_loss: 1.600925 acc: 0.901500\n",
      "Epoch  397/2000 train_loss: 1.600793 acc: 0.901300\n",
      "Epoch  398/2000 train_loss: 1.600664 acc: 0.901700\n",
      "Epoch  399/2000 train_loss: 1.600535 acc: 0.901600\n",
      "Epoch  400/2000 train_loss: 1.600407 acc: 0.901800\n",
      "Epoch  401/2000 train_loss: 1.600281 acc: 0.902000\n",
      "Epoch  402/2000 train_loss: 1.600154 acc: 0.901900\n",
      "Epoch  403/2000 train_loss: 1.600028 acc: 0.902100\n",
      "Epoch  404/2000 train_loss: 1.599904 acc: 0.902200\n",
      "Epoch  405/2000 train_loss: 1.599781 acc: 0.902200\n",
      "Epoch  406/2000 train_loss: 1.599658 acc: 0.902400\n",
      "Epoch  407/2000 train_loss: 1.599537 acc: 0.902200\n",
      "Epoch  408/2000 train_loss: 1.599416 acc: 0.902500\n",
      "Epoch  409/2000 train_loss: 1.599297 acc: 0.902200\n",
      "Epoch  410/2000 train_loss: 1.599177 acc: 0.902300\n",
      "Epoch  411/2000 train_loss: 1.599058 acc: 0.902400\n",
      "Epoch  412/2000 train_loss: 1.598941 acc: 0.902800\n",
      "Epoch  413/2000 train_loss: 1.598823 acc: 0.902700\n",
      "Epoch  414/2000 train_loss: 1.598708 acc: 0.902900\n",
      "Epoch  415/2000 train_loss: 1.598593 acc: 0.902800\n",
      "Epoch  416/2000 train_loss: 1.598479 acc: 0.903000\n",
      "Epoch  417/2000 train_loss: 1.598365 acc: 0.903200\n",
      "Epoch  418/2000 train_loss: 1.598252 acc: 0.903200\n",
      "Epoch  419/2000 train_loss: 1.598140 acc: 0.903300\n",
      "Epoch  420/2000 train_loss: 1.598029 acc: 0.903300\n",
      "Epoch  421/2000 train_loss: 1.597918 acc: 0.903300\n",
      "Epoch  422/2000 train_loss: 1.597807 acc: 0.903200\n",
      "Epoch  423/2000 train_loss: 1.597697 acc: 0.903300\n",
      "Epoch  424/2000 train_loss: 1.597589 acc: 0.903400\n",
      "Epoch  425/2000 train_loss: 1.597481 acc: 0.903500\n",
      "Epoch  426/2000 train_loss: 1.597373 acc: 0.903400\n",
      "Epoch  427/2000 train_loss: 1.597268 acc: 0.903400\n",
      "Epoch  428/2000 train_loss: 1.597160 acc: 0.903500\n",
      "Epoch  429/2000 train_loss: 1.597055 acc: 0.903700\n",
      "Epoch  430/2000 train_loss: 1.596950 acc: 0.903600\n",
      "Epoch  431/2000 train_loss: 1.596846 acc: 0.903700\n",
      "Epoch  432/2000 train_loss: 1.596742 acc: 0.903600\n",
      "Epoch  433/2000 train_loss: 1.596639 acc: 0.903700\n",
      "Epoch  434/2000 train_loss: 1.596536 acc: 0.903700\n",
      "Epoch  435/2000 train_loss: 1.596435 acc: 0.903900\n",
      "Epoch  436/2000 train_loss: 1.596334 acc: 0.904100\n",
      "Epoch  437/2000 train_loss: 1.596233 acc: 0.904100\n",
      "Epoch  438/2000 train_loss: 1.596133 acc: 0.904200\n",
      "Epoch  439/2000 train_loss: 1.596033 acc: 0.904100\n",
      "Epoch  440/2000 train_loss: 1.595933 acc: 0.904200\n",
      "Epoch  441/2000 train_loss: 1.595835 acc: 0.904200\n",
      "Epoch  442/2000 train_loss: 1.595737 acc: 0.904300\n",
      "Epoch  443/2000 train_loss: 1.595640 acc: 0.904400\n",
      "Epoch  444/2000 train_loss: 1.595543 acc: 0.904400\n",
      "Epoch  445/2000 train_loss: 1.595446 acc: 0.904600\n",
      "Epoch  446/2000 train_loss: 1.595351 acc: 0.904500\n",
      "Epoch  447/2000 train_loss: 1.595255 acc: 0.904400\n",
      "Epoch  448/2000 train_loss: 1.595160 acc: 0.904500\n",
      "Epoch  449/2000 train_loss: 1.595067 acc: 0.904600\n",
      "Epoch  450/2000 train_loss: 1.594972 acc: 0.904600\n",
      "Epoch  451/2000 train_loss: 1.594880 acc: 0.904900\n",
      "Epoch  452/2000 train_loss: 1.594786 acc: 0.904800\n",
      "Epoch  453/2000 train_loss: 1.594694 acc: 0.904900\n",
      "Epoch  454/2000 train_loss: 1.594602 acc: 0.904900\n",
      "Epoch  455/2000 train_loss: 1.594510 acc: 0.905000\n",
      "Epoch  456/2000 train_loss: 1.594419 acc: 0.904900\n",
      "Epoch  457/2000 train_loss: 1.594329 acc: 0.905100\n",
      "Epoch  458/2000 train_loss: 1.594239 acc: 0.904900\n",
      "Epoch  459/2000 train_loss: 1.594149 acc: 0.905000\n",
      "Epoch  460/2000 train_loss: 1.594060 acc: 0.905000\n",
      "Epoch  461/2000 train_loss: 1.593972 acc: 0.905000\n",
      "Epoch  462/2000 train_loss: 1.593883 acc: 0.905200\n",
      "Epoch  463/2000 train_loss: 1.593796 acc: 0.904900\n",
      "Epoch  464/2000 train_loss: 1.593709 acc: 0.905200\n",
      "Epoch  465/2000 train_loss: 1.593621 acc: 0.905000\n",
      "Epoch  466/2000 train_loss: 1.593534 acc: 0.905100\n",
      "Epoch  467/2000 train_loss: 1.593448 acc: 0.905100\n",
      "Epoch  468/2000 train_loss: 1.593362 acc: 0.905200\n",
      "Epoch  469/2000 train_loss: 1.593277 acc: 0.905300\n",
      "Epoch  470/2000 train_loss: 1.593192 acc: 0.905300\n",
      "Epoch  471/2000 train_loss: 1.593108 acc: 0.905400\n",
      "Epoch  472/2000 train_loss: 1.593024 acc: 0.905300\n",
      "Epoch  473/2000 train_loss: 1.592940 acc: 0.905400\n",
      "Epoch  474/2000 train_loss: 1.592856 acc: 0.905400\n",
      "Epoch  475/2000 train_loss: 1.592774 acc: 0.905400\n",
      "Epoch  476/2000 train_loss: 1.592691 acc: 0.905400\n",
      "Epoch  477/2000 train_loss: 1.592609 acc: 0.905500\n",
      "Epoch  478/2000 train_loss: 1.592527 acc: 0.905600\n",
      "Epoch  479/2000 train_loss: 1.592446 acc: 0.905500\n",
      "Epoch  480/2000 train_loss: 1.592364 acc: 0.905800\n",
      "Epoch  481/2000 train_loss: 1.592284 acc: 0.905700\n",
      "Epoch  482/2000 train_loss: 1.592204 acc: 0.905400\n",
      "Epoch  483/2000 train_loss: 1.592123 acc: 0.905500\n",
      "Epoch  484/2000 train_loss: 1.592044 acc: 0.905600\n",
      "Epoch  485/2000 train_loss: 1.591965 acc: 0.905500\n",
      "Epoch  486/2000 train_loss: 1.591885 acc: 0.905600\n",
      "Epoch  487/2000 train_loss: 1.591808 acc: 0.905700\n",
      "Epoch  488/2000 train_loss: 1.591729 acc: 0.905700\n",
      "Epoch  489/2000 train_loss: 1.591651 acc: 0.905900\n",
      "Epoch  490/2000 train_loss: 1.591574 acc: 0.905900\n",
      "Epoch  491/2000 train_loss: 1.591496 acc: 0.906000\n",
      "Epoch  492/2000 train_loss: 1.591419 acc: 0.906000\n",
      "Epoch  493/2000 train_loss: 1.591343 acc: 0.906000\n",
      "Epoch  494/2000 train_loss: 1.591266 acc: 0.906000\n",
      "Epoch  495/2000 train_loss: 1.591191 acc: 0.906300\n",
      "Epoch  496/2000 train_loss: 1.591115 acc: 0.906100\n",
      "Epoch  497/2000 train_loss: 1.591040 acc: 0.906400\n",
      "Epoch  498/2000 train_loss: 1.590965 acc: 0.906300\n",
      "Epoch  499/2000 train_loss: 1.590890 acc: 0.906400\n",
      "Epoch  500/2000 train_loss: 1.590816 acc: 0.906500\n",
      "Epoch  501/2000 train_loss: 1.590741 acc: 0.906500\n",
      "Epoch  502/2000 train_loss: 1.590668 acc: 0.906600\n",
      "Epoch  503/2000 train_loss: 1.590595 acc: 0.906500\n",
      "Epoch  504/2000 train_loss: 1.590522 acc: 0.906700\n",
      "Epoch  505/2000 train_loss: 1.590449 acc: 0.906700\n",
      "Epoch  506/2000 train_loss: 1.590377 acc: 0.907000\n",
      "Epoch  507/2000 train_loss: 1.590304 acc: 0.906700\n",
      "Epoch  508/2000 train_loss: 1.590232 acc: 0.906900\n",
      "Epoch  509/2000 train_loss: 1.590161 acc: 0.907100\n",
      "Epoch  510/2000 train_loss: 1.590089 acc: 0.907100\n",
      "Epoch  511/2000 train_loss: 1.590019 acc: 0.907100\n",
      "Epoch  512/2000 train_loss: 1.589948 acc: 0.907100\n",
      "Epoch  513/2000 train_loss: 1.589877 acc: 0.907100\n",
      "Epoch  514/2000 train_loss: 1.589807 acc: 0.907200\n",
      "Epoch  515/2000 train_loss: 1.589737 acc: 0.907200\n",
      "Epoch  516/2000 train_loss: 1.589667 acc: 0.907100\n",
      "Epoch  517/2000 train_loss: 1.589598 acc: 0.907100\n",
      "Epoch  518/2000 train_loss: 1.589529 acc: 0.907200\n",
      "Epoch  519/2000 train_loss: 1.589460 acc: 0.907000\n",
      "Epoch  520/2000 train_loss: 1.589391 acc: 0.907000\n",
      "Epoch  521/2000 train_loss: 1.589324 acc: 0.907200\n",
      "Epoch  522/2000 train_loss: 1.589255 acc: 0.907100\n",
      "Epoch  523/2000 train_loss: 1.589187 acc: 0.907100\n",
      "Epoch  524/2000 train_loss: 1.589120 acc: 0.907200\n",
      "Epoch  525/2000 train_loss: 1.589053 acc: 0.907200\n",
      "Epoch  526/2000 train_loss: 1.588986 acc: 0.907300\n",
      "Epoch  527/2000 train_loss: 1.588919 acc: 0.907300\n",
      "Epoch  528/2000 train_loss: 1.588852 acc: 0.907300\n",
      "Epoch  529/2000 train_loss: 1.588786 acc: 0.907300\n",
      "Epoch  530/2000 train_loss: 1.588720 acc: 0.907300\n",
      "Epoch  531/2000 train_loss: 1.588655 acc: 0.907300\n",
      "Epoch  532/2000 train_loss: 1.588589 acc: 0.907400\n",
      "Epoch  533/2000 train_loss: 1.588523 acc: 0.907500\n",
      "Epoch  534/2000 train_loss: 1.588459 acc: 0.907400\n",
      "Epoch  535/2000 train_loss: 1.588394 acc: 0.907400\n",
      "Epoch  536/2000 train_loss: 1.588329 acc: 0.907600\n",
      "Epoch  537/2000 train_loss: 1.588265 acc: 0.907500\n",
      "Epoch  538/2000 train_loss: 1.588201 acc: 0.907600\n",
      "Epoch  539/2000 train_loss: 1.588137 acc: 0.907600\n",
      "Epoch  540/2000 train_loss: 1.588073 acc: 0.907600\n",
      "Epoch  541/2000 train_loss: 1.588009 acc: 0.907600\n",
      "Epoch  542/2000 train_loss: 1.587947 acc: 0.907700\n",
      "Epoch  543/2000 train_loss: 1.587884 acc: 0.907700\n",
      "Epoch  544/2000 train_loss: 1.587821 acc: 0.907800\n",
      "Epoch  545/2000 train_loss: 1.587758 acc: 0.907700\n",
      "Epoch  546/2000 train_loss: 1.587696 acc: 0.907800\n",
      "Epoch  547/2000 train_loss: 1.587634 acc: 0.907700\n",
      "Epoch  548/2000 train_loss: 1.587573 acc: 0.907700\n",
      "Epoch  549/2000 train_loss: 1.587511 acc: 0.907600\n",
      "Epoch  550/2000 train_loss: 1.587450 acc: 0.907700\n",
      "Epoch  551/2000 train_loss: 1.587388 acc: 0.907700\n",
      "Epoch  552/2000 train_loss: 1.587327 acc: 0.907800\n",
      "Epoch  553/2000 train_loss: 1.587267 acc: 0.907800\n",
      "Epoch  554/2000 train_loss: 1.587206 acc: 0.907800\n",
      "Epoch  555/2000 train_loss: 1.587146 acc: 0.907900\n",
      "Epoch  556/2000 train_loss: 1.587085 acc: 0.907900\n",
      "Epoch  557/2000 train_loss: 1.587026 acc: 0.907900\n",
      "Epoch  558/2000 train_loss: 1.586966 acc: 0.908000\n",
      "Epoch  559/2000 train_loss: 1.586906 acc: 0.908100\n",
      "Epoch  560/2000 train_loss: 1.586847 acc: 0.908000\n",
      "Epoch  561/2000 train_loss: 1.586788 acc: 0.908200\n",
      "Epoch  562/2000 train_loss: 1.586729 acc: 0.908200\n",
      "Epoch  563/2000 train_loss: 1.586670 acc: 0.908200\n",
      "Epoch  564/2000 train_loss: 1.586612 acc: 0.908400\n",
      "Epoch  565/2000 train_loss: 1.586553 acc: 0.908300\n",
      "Epoch  566/2000 train_loss: 1.586495 acc: 0.908400\n",
      "Epoch  567/2000 train_loss: 1.586437 acc: 0.908400\n",
      "Epoch  568/2000 train_loss: 1.586380 acc: 0.908400\n",
      "Epoch  569/2000 train_loss: 1.586322 acc: 0.908400\n",
      "Epoch  570/2000 train_loss: 1.586265 acc: 0.908400\n",
      "Epoch  571/2000 train_loss: 1.586207 acc: 0.908400\n",
      "Epoch  572/2000 train_loss: 1.586151 acc: 0.908400\n",
      "Epoch  573/2000 train_loss: 1.586094 acc: 0.908300\n",
      "Epoch  574/2000 train_loss: 1.586037 acc: 0.908300\n",
      "Epoch  575/2000 train_loss: 1.585981 acc: 0.908400\n",
      "Epoch  576/2000 train_loss: 1.585925 acc: 0.908400\n",
      "Epoch  577/2000 train_loss: 1.585868 acc: 0.908200\n",
      "Epoch  578/2000 train_loss: 1.585813 acc: 0.908400\n",
      "Epoch  579/2000 train_loss: 1.585757 acc: 0.908500\n",
      "Epoch  580/2000 train_loss: 1.585702 acc: 0.908400\n",
      "Epoch  581/2000 train_loss: 1.585646 acc: 0.908300\n",
      "Epoch  582/2000 train_loss: 1.585591 acc: 0.908200\n",
      "Epoch  583/2000 train_loss: 1.585536 acc: 0.908300\n",
      "Epoch  584/2000 train_loss: 1.585481 acc: 0.908200\n",
      "Epoch  585/2000 train_loss: 1.585426 acc: 0.908100\n",
      "Epoch  586/2000 train_loss: 1.585373 acc: 0.908200\n",
      "Epoch  587/2000 train_loss: 1.585318 acc: 0.908300\n",
      "Epoch  588/2000 train_loss: 1.585265 acc: 0.908200\n",
      "Epoch  589/2000 train_loss: 1.585210 acc: 0.908100\n",
      "Epoch  590/2000 train_loss: 1.585157 acc: 0.908300\n",
      "Epoch  591/2000 train_loss: 1.585103 acc: 0.908200\n",
      "Epoch  592/2000 train_loss: 1.585049 acc: 0.908300\n",
      "Epoch  593/2000 train_loss: 1.584996 acc: 0.908200\n",
      "Epoch  594/2000 train_loss: 1.584943 acc: 0.908200\n",
      "Epoch  595/2000 train_loss: 1.584890 acc: 0.908100\n",
      "Epoch  596/2000 train_loss: 1.584837 acc: 0.908200\n",
      "Epoch  597/2000 train_loss: 1.584784 acc: 0.908300\n",
      "Epoch  598/2000 train_loss: 1.584732 acc: 0.908200\n",
      "Epoch  599/2000 train_loss: 1.584680 acc: 0.908500\n",
      "Epoch  600/2000 train_loss: 1.584628 acc: 0.908400\n",
      "Epoch  601/2000 train_loss: 1.584576 acc: 0.908400\n",
      "Epoch  602/2000 train_loss: 1.584524 acc: 0.908300\n",
      "Epoch  603/2000 train_loss: 1.584472 acc: 0.908500\n",
      "Epoch  604/2000 train_loss: 1.584421 acc: 0.908500\n",
      "Epoch  605/2000 train_loss: 1.584369 acc: 0.908400\n",
      "Epoch  606/2000 train_loss: 1.584318 acc: 0.908300\n",
      "Epoch  607/2000 train_loss: 1.584267 acc: 0.908500\n",
      "Epoch  608/2000 train_loss: 1.584216 acc: 0.908300\n",
      "Epoch  609/2000 train_loss: 1.584165 acc: 0.908500\n",
      "Epoch  610/2000 train_loss: 1.584115 acc: 0.908400\n",
      "Epoch  611/2000 train_loss: 1.584064 acc: 0.908500\n",
      "Epoch  612/2000 train_loss: 1.584014 acc: 0.908400\n",
      "Epoch  613/2000 train_loss: 1.583964 acc: 0.908400\n",
      "Epoch  614/2000 train_loss: 1.583914 acc: 0.908400\n",
      "Epoch  615/2000 train_loss: 1.583864 acc: 0.908400\n",
      "Epoch  616/2000 train_loss: 1.583814 acc: 0.908500\n",
      "Epoch  617/2000 train_loss: 1.583765 acc: 0.908600\n",
      "Epoch  618/2000 train_loss: 1.583716 acc: 0.908500\n",
      "Epoch  619/2000 train_loss: 1.583667 acc: 0.908500\n",
      "Epoch  620/2000 train_loss: 1.583617 acc: 0.908500\n",
      "Epoch  621/2000 train_loss: 1.583568 acc: 0.908600\n",
      "Epoch  622/2000 train_loss: 1.583519 acc: 0.908400\n",
      "Epoch  623/2000 train_loss: 1.583471 acc: 0.908400\n",
      "Epoch  624/2000 train_loss: 1.583423 acc: 0.908500\n",
      "Epoch  625/2000 train_loss: 1.583374 acc: 0.908400\n",
      "Epoch  626/2000 train_loss: 1.583325 acc: 0.908500\n",
      "Epoch  627/2000 train_loss: 1.583277 acc: 0.908600\n",
      "Epoch  628/2000 train_loss: 1.583230 acc: 0.908600\n",
      "Epoch  629/2000 train_loss: 1.583181 acc: 0.908500\n",
      "Epoch  630/2000 train_loss: 1.583133 acc: 0.908500\n",
      "Epoch  631/2000 train_loss: 1.583087 acc: 0.908600\n",
      "Epoch  632/2000 train_loss: 1.583038 acc: 0.908500\n",
      "Epoch  633/2000 train_loss: 1.582991 acc: 0.908700\n",
      "Epoch  634/2000 train_loss: 1.582944 acc: 0.908700\n",
      "Epoch  635/2000 train_loss: 1.582897 acc: 0.908800\n",
      "Epoch  636/2000 train_loss: 1.582849 acc: 0.908700\n",
      "Epoch  637/2000 train_loss: 1.582803 acc: 0.908800\n",
      "Epoch  638/2000 train_loss: 1.582757 acc: 0.908800\n",
      "Epoch  639/2000 train_loss: 1.582710 acc: 0.908900\n",
      "Epoch  640/2000 train_loss: 1.582664 acc: 0.908800\n",
      "Epoch  641/2000 train_loss: 1.582617 acc: 0.908800\n",
      "Epoch  642/2000 train_loss: 1.582571 acc: 0.908800\n",
      "Epoch  643/2000 train_loss: 1.582525 acc: 0.908700\n",
      "Epoch  644/2000 train_loss: 1.582479 acc: 0.908700\n",
      "Epoch  645/2000 train_loss: 1.582434 acc: 0.908700\n",
      "Epoch  646/2000 train_loss: 1.582388 acc: 0.908700\n",
      "Epoch  647/2000 train_loss: 1.582342 acc: 0.908700\n",
      "Epoch  648/2000 train_loss: 1.582297 acc: 0.908700\n",
      "Epoch  649/2000 train_loss: 1.582251 acc: 0.908700\n",
      "Epoch  650/2000 train_loss: 1.582207 acc: 0.908900\n",
      "Epoch  651/2000 train_loss: 1.582162 acc: 0.908700\n",
      "Epoch  652/2000 train_loss: 1.582117 acc: 0.908700\n",
      "Epoch  653/2000 train_loss: 1.582072 acc: 0.908900\n",
      "Epoch  654/2000 train_loss: 1.582027 acc: 0.909200\n",
      "Epoch  655/2000 train_loss: 1.581982 acc: 0.908900\n",
      "Epoch  656/2000 train_loss: 1.581938 acc: 0.908900\n",
      "Epoch  657/2000 train_loss: 1.581894 acc: 0.909000\n",
      "Epoch  658/2000 train_loss: 1.581849 acc: 0.909100\n",
      "Epoch  659/2000 train_loss: 1.581805 acc: 0.909000\n",
      "Epoch  660/2000 train_loss: 1.581762 acc: 0.909000\n",
      "Epoch  661/2000 train_loss: 1.581717 acc: 0.909100\n",
      "Epoch  662/2000 train_loss: 1.581673 acc: 0.909200\n",
      "Epoch  663/2000 train_loss: 1.581630 acc: 0.909200\n",
      "Epoch  664/2000 train_loss: 1.581587 acc: 0.909300\n",
      "Epoch  665/2000 train_loss: 1.581544 acc: 0.909300\n",
      "Epoch  666/2000 train_loss: 1.581500 acc: 0.909400\n",
      "Epoch  667/2000 train_loss: 1.581456 acc: 0.909500\n",
      "Epoch  668/2000 train_loss: 1.581413 acc: 0.909600\n",
      "Epoch  669/2000 train_loss: 1.581371 acc: 0.909600\n",
      "Epoch  670/2000 train_loss: 1.581328 acc: 0.909500\n",
      "Epoch  671/2000 train_loss: 1.581285 acc: 0.909700\n",
      "Epoch  672/2000 train_loss: 1.581242 acc: 0.909700\n",
      "Epoch  673/2000 train_loss: 1.581200 acc: 0.909600\n",
      "Epoch  674/2000 train_loss: 1.581158 acc: 0.909700\n",
      "Epoch  675/2000 train_loss: 1.581115 acc: 0.909800\n",
      "Epoch  676/2000 train_loss: 1.581073 acc: 0.909800\n",
      "Epoch  677/2000 train_loss: 1.581031 acc: 0.909900\n",
      "Epoch  678/2000 train_loss: 1.580989 acc: 0.909800\n",
      "Epoch  679/2000 train_loss: 1.580947 acc: 0.909900\n",
      "Epoch  680/2000 train_loss: 1.580906 acc: 0.910000\n",
      "Epoch  681/2000 train_loss: 1.580864 acc: 0.909900\n",
      "Epoch  682/2000 train_loss: 1.580823 acc: 0.910000\n",
      "Epoch  683/2000 train_loss: 1.580781 acc: 0.910100\n",
      "Epoch  684/2000 train_loss: 1.580740 acc: 0.910100\n",
      "Epoch  685/2000 train_loss: 1.580698 acc: 0.910000\n",
      "Epoch  686/2000 train_loss: 1.580657 acc: 0.910100\n",
      "Epoch  687/2000 train_loss: 1.580616 acc: 0.910000\n",
      "Epoch  688/2000 train_loss: 1.580575 acc: 0.910100\n",
      "Epoch  689/2000 train_loss: 1.580535 acc: 0.910100\n",
      "Epoch  690/2000 train_loss: 1.580494 acc: 0.910100\n",
      "Epoch  691/2000 train_loss: 1.580453 acc: 0.909900\n",
      "Epoch  692/2000 train_loss: 1.580413 acc: 0.910100\n",
      "Epoch  693/2000 train_loss: 1.580372 acc: 0.910000\n",
      "Epoch  694/2000 train_loss: 1.580332 acc: 0.910000\n",
      "Epoch  695/2000 train_loss: 1.580292 acc: 0.910100\n",
      "Epoch  696/2000 train_loss: 1.580252 acc: 0.910200\n",
      "Epoch  697/2000 train_loss: 1.580211 acc: 0.910200\n",
      "Epoch  698/2000 train_loss: 1.580172 acc: 0.910300\n",
      "Epoch  699/2000 train_loss: 1.580132 acc: 0.910100\n",
      "Epoch  700/2000 train_loss: 1.580092 acc: 0.910200\n",
      "Epoch  701/2000 train_loss: 1.580053 acc: 0.910200\n",
      "Epoch  702/2000 train_loss: 1.580013 acc: 0.910100\n",
      "Epoch  703/2000 train_loss: 1.579974 acc: 0.910200\n",
      "Epoch  704/2000 train_loss: 1.579934 acc: 0.910300\n",
      "Epoch  705/2000 train_loss: 1.579895 acc: 0.910300\n",
      "Epoch  706/2000 train_loss: 1.579856 acc: 0.910300\n",
      "Epoch  707/2000 train_loss: 1.579817 acc: 0.910300\n",
      "Epoch  708/2000 train_loss: 1.579778 acc: 0.910500\n",
      "Epoch  709/2000 train_loss: 1.579739 acc: 0.910600\n",
      "Epoch  710/2000 train_loss: 1.579700 acc: 0.910500\n",
      "Epoch  711/2000 train_loss: 1.579661 acc: 0.910500\n",
      "Epoch  712/2000 train_loss: 1.579623 acc: 0.910500\n",
      "Epoch  713/2000 train_loss: 1.579585 acc: 0.910600\n",
      "Epoch  714/2000 train_loss: 1.579546 acc: 0.910600\n",
      "Epoch  715/2000 train_loss: 1.579508 acc: 0.910700\n",
      "Epoch  716/2000 train_loss: 1.579469 acc: 0.910800\n",
      "Epoch  717/2000 train_loss: 1.579432 acc: 0.910700\n",
      "Epoch  718/2000 train_loss: 1.579393 acc: 0.910800\n",
      "Epoch  719/2000 train_loss: 1.579355 acc: 0.910900\n",
      "Epoch  720/2000 train_loss: 1.579317 acc: 0.910900\n",
      "Epoch  721/2000 train_loss: 1.579280 acc: 0.910900\n",
      "Epoch  722/2000 train_loss: 1.579242 acc: 0.910800\n",
      "Epoch  723/2000 train_loss: 1.579204 acc: 0.910900\n",
      "Epoch  724/2000 train_loss: 1.579167 acc: 0.910900\n",
      "Epoch  725/2000 train_loss: 1.579130 acc: 0.910900\n",
      "Epoch  726/2000 train_loss: 1.579092 acc: 0.910900\n",
      "Epoch  727/2000 train_loss: 1.579055 acc: 0.910900\n",
      "Epoch  728/2000 train_loss: 1.579018 acc: 0.910900\n",
      "Epoch  729/2000 train_loss: 1.578981 acc: 0.910900\n",
      "Epoch  730/2000 train_loss: 1.578944 acc: 0.911000\n",
      "Epoch  731/2000 train_loss: 1.578907 acc: 0.910900\n",
      "Epoch  732/2000 train_loss: 1.578871 acc: 0.910900\n",
      "Epoch  733/2000 train_loss: 1.578833 acc: 0.911000\n",
      "Epoch  734/2000 train_loss: 1.578797 acc: 0.911000\n",
      "Epoch  735/2000 train_loss: 1.578760 acc: 0.911000\n",
      "Epoch  736/2000 train_loss: 1.578724 acc: 0.911000\n",
      "Epoch  737/2000 train_loss: 1.578687 acc: 0.911000\n",
      "Epoch  738/2000 train_loss: 1.578651 acc: 0.911200\n",
      "Epoch  739/2000 train_loss: 1.578615 acc: 0.911100\n",
      "Epoch  740/2000 train_loss: 1.578578 acc: 0.911200\n",
      "Epoch  741/2000 train_loss: 1.578542 acc: 0.911200\n",
      "Epoch  742/2000 train_loss: 1.578507 acc: 0.911200\n",
      "Epoch  743/2000 train_loss: 1.578470 acc: 0.911200\n",
      "Epoch  744/2000 train_loss: 1.578435 acc: 0.911200\n",
      "Epoch  745/2000 train_loss: 1.578399 acc: 0.911200\n",
      "Epoch  746/2000 train_loss: 1.578363 acc: 0.911200\n",
      "Epoch  747/2000 train_loss: 1.578328 acc: 0.911200\n",
      "Epoch  748/2000 train_loss: 1.578292 acc: 0.911200\n",
      "Epoch  749/2000 train_loss: 1.578257 acc: 0.911200\n",
      "Epoch  750/2000 train_loss: 1.578221 acc: 0.911300\n",
      "Epoch  751/2000 train_loss: 1.578186 acc: 0.911300\n",
      "Epoch  752/2000 train_loss: 1.578151 acc: 0.911300\n",
      "Epoch  753/2000 train_loss: 1.578115 acc: 0.911200\n",
      "Epoch  754/2000 train_loss: 1.578081 acc: 0.911300\n",
      "Epoch  755/2000 train_loss: 1.578045 acc: 0.911200\n",
      "Epoch  756/2000 train_loss: 1.578011 acc: 0.911200\n",
      "Epoch  757/2000 train_loss: 1.577976 acc: 0.911200\n",
      "Epoch  758/2000 train_loss: 1.577941 acc: 0.911100\n",
      "Epoch  759/2000 train_loss: 1.577906 acc: 0.911100\n",
      "Epoch  760/2000 train_loss: 1.577873 acc: 0.911100\n",
      "Epoch  761/2000 train_loss: 1.577837 acc: 0.911100\n",
      "Epoch  762/2000 train_loss: 1.577803 acc: 0.911100\n",
      "Epoch  763/2000 train_loss: 1.577769 acc: 0.911100\n",
      "Epoch  764/2000 train_loss: 1.577734 acc: 0.911100\n",
      "Epoch  765/2000 train_loss: 1.577700 acc: 0.911100\n",
      "Epoch  766/2000 train_loss: 1.577666 acc: 0.911100\n",
      "Epoch  767/2000 train_loss: 1.577632 acc: 0.911100\n",
      "Epoch  768/2000 train_loss: 1.577598 acc: 0.911100\n",
      "Epoch  769/2000 train_loss: 1.577564 acc: 0.911000\n",
      "Epoch  770/2000 train_loss: 1.577530 acc: 0.911000\n",
      "Epoch  771/2000 train_loss: 1.577497 acc: 0.911100\n",
      "Epoch  772/2000 train_loss: 1.577463 acc: 0.911100\n",
      "Epoch  773/2000 train_loss: 1.577429 acc: 0.911100\n",
      "Epoch  774/2000 train_loss: 1.577396 acc: 0.911100\n",
      "Epoch  775/2000 train_loss: 1.577362 acc: 0.911100\n",
      "Epoch  776/2000 train_loss: 1.577329 acc: 0.911200\n",
      "Epoch  777/2000 train_loss: 1.577296 acc: 0.911000\n",
      "Epoch  778/2000 train_loss: 1.577262 acc: 0.911200\n",
      "Epoch  779/2000 train_loss: 1.577229 acc: 0.911000\n",
      "Epoch  780/2000 train_loss: 1.577196 acc: 0.911300\n",
      "Epoch  781/2000 train_loss: 1.577163 acc: 0.911200\n",
      "Epoch  782/2000 train_loss: 1.577130 acc: 0.911200\n",
      "Epoch  783/2000 train_loss: 1.577097 acc: 0.911200\n",
      "Epoch  784/2000 train_loss: 1.577064 acc: 0.911300\n",
      "Epoch  785/2000 train_loss: 1.577031 acc: 0.911300\n",
      "Epoch  786/2000 train_loss: 1.576999 acc: 0.911400\n",
      "Epoch  787/2000 train_loss: 1.576966 acc: 0.911300\n",
      "Epoch  788/2000 train_loss: 1.576933 acc: 0.911300\n",
      "Epoch  789/2000 train_loss: 1.576900 acc: 0.911300\n",
      "Epoch  790/2000 train_loss: 1.576869 acc: 0.911300\n",
      "Epoch  791/2000 train_loss: 1.576836 acc: 0.911300\n",
      "Epoch  792/2000 train_loss: 1.576803 acc: 0.911300\n",
      "Epoch  793/2000 train_loss: 1.576771 acc: 0.911300\n",
      "Epoch  794/2000 train_loss: 1.576739 acc: 0.911300\n",
      "Epoch  795/2000 train_loss: 1.576707 acc: 0.911500\n",
      "Epoch  796/2000 train_loss: 1.576675 acc: 0.911400\n",
      "Epoch  797/2000 train_loss: 1.576643 acc: 0.911500\n",
      "Epoch  798/2000 train_loss: 1.576611 acc: 0.911500\n",
      "Epoch  799/2000 train_loss: 1.576580 acc: 0.911400\n",
      "Epoch  800/2000 train_loss: 1.576548 acc: 0.911400\n",
      "Epoch  801/2000 train_loss: 1.576516 acc: 0.911700\n",
      "Epoch  802/2000 train_loss: 1.576484 acc: 0.911900\n",
      "Epoch  803/2000 train_loss: 1.576452 acc: 0.911800\n",
      "Epoch  804/2000 train_loss: 1.576421 acc: 0.911800\n",
      "Epoch  805/2000 train_loss: 1.576389 acc: 0.911900\n",
      "Epoch  806/2000 train_loss: 1.576358 acc: 0.911800\n",
      "Epoch  807/2000 train_loss: 1.576327 acc: 0.911900\n",
      "Epoch  808/2000 train_loss: 1.576296 acc: 0.911800\n",
      "Epoch  809/2000 train_loss: 1.576264 acc: 0.911900\n",
      "Epoch  810/2000 train_loss: 1.576234 acc: 0.911800\n",
      "Epoch  811/2000 train_loss: 1.576202 acc: 0.911900\n",
      "Epoch  812/2000 train_loss: 1.576171 acc: 0.911800\n",
      "Epoch  813/2000 train_loss: 1.576140 acc: 0.911900\n",
      "Epoch  814/2000 train_loss: 1.576110 acc: 0.911900\n",
      "Epoch  815/2000 train_loss: 1.576078 acc: 0.911900\n",
      "Epoch  816/2000 train_loss: 1.576048 acc: 0.911900\n",
      "Epoch  817/2000 train_loss: 1.576017 acc: 0.911900\n",
      "Epoch  818/2000 train_loss: 1.575986 acc: 0.912000\n",
      "Epoch  819/2000 train_loss: 1.575956 acc: 0.911900\n",
      "Epoch  820/2000 train_loss: 1.575925 acc: 0.912000\n",
      "Epoch  821/2000 train_loss: 1.575895 acc: 0.911900\n",
      "Epoch  822/2000 train_loss: 1.575864 acc: 0.912000\n",
      "Epoch  823/2000 train_loss: 1.575834 acc: 0.911900\n",
      "Epoch  824/2000 train_loss: 1.575804 acc: 0.912000\n",
      "Epoch  825/2000 train_loss: 1.575774 acc: 0.912000\n",
      "Epoch  826/2000 train_loss: 1.575744 acc: 0.911900\n",
      "Epoch  827/2000 train_loss: 1.575714 acc: 0.912000\n",
      "Epoch  828/2000 train_loss: 1.575683 acc: 0.911900\n",
      "Epoch  829/2000 train_loss: 1.575654 acc: 0.911900\n",
      "Epoch  830/2000 train_loss: 1.575624 acc: 0.911900\n",
      "Epoch  831/2000 train_loss: 1.575594 acc: 0.911900\n",
      "Epoch  832/2000 train_loss: 1.575564 acc: 0.911900\n",
      "Epoch  833/2000 train_loss: 1.575534 acc: 0.911900\n",
      "Epoch  834/2000 train_loss: 1.575504 acc: 0.911900\n",
      "Epoch  835/2000 train_loss: 1.575475 acc: 0.912000\n",
      "Epoch  836/2000 train_loss: 1.575445 acc: 0.911900\n",
      "Epoch  837/2000 train_loss: 1.575416 acc: 0.911900\n",
      "Epoch  838/2000 train_loss: 1.575387 acc: 0.912000\n",
      "Epoch  839/2000 train_loss: 1.575357 acc: 0.912000\n",
      "Epoch  840/2000 train_loss: 1.575328 acc: 0.912000\n",
      "Epoch  841/2000 train_loss: 1.575298 acc: 0.912100\n",
      "Epoch  842/2000 train_loss: 1.575269 acc: 0.912100\n",
      "Epoch  843/2000 train_loss: 1.575240 acc: 0.912100\n",
      "Epoch  844/2000 train_loss: 1.575211 acc: 0.912100\n",
      "Epoch  845/2000 train_loss: 1.575182 acc: 0.912100\n",
      "Epoch  846/2000 train_loss: 1.575153 acc: 0.912000\n",
      "Epoch  847/2000 train_loss: 1.575123 acc: 0.912100\n",
      "Epoch  848/2000 train_loss: 1.575094 acc: 0.912000\n",
      "Epoch  849/2000 train_loss: 1.575066 acc: 0.912000\n",
      "Epoch  850/2000 train_loss: 1.575037 acc: 0.912000\n",
      "Epoch  851/2000 train_loss: 1.575008 acc: 0.912100\n",
      "Epoch  852/2000 train_loss: 1.574979 acc: 0.912200\n",
      "Epoch  853/2000 train_loss: 1.574950 acc: 0.912000\n",
      "Epoch  854/2000 train_loss: 1.574922 acc: 0.912100\n",
      "Epoch  855/2000 train_loss: 1.574894 acc: 0.912000\n",
      "Epoch  856/2000 train_loss: 1.574865 acc: 0.912000\n",
      "Epoch  857/2000 train_loss: 1.574837 acc: 0.912000\n",
      "Epoch  858/2000 train_loss: 1.574809 acc: 0.911900\n",
      "Epoch  859/2000 train_loss: 1.574780 acc: 0.911900\n",
      "Epoch  860/2000 train_loss: 1.574751 acc: 0.911800\n",
      "Epoch  861/2000 train_loss: 1.574723 acc: 0.911700\n",
      "Epoch  862/2000 train_loss: 1.574696 acc: 0.911900\n",
      "Epoch  863/2000 train_loss: 1.574667 acc: 0.911900\n",
      "Epoch  864/2000 train_loss: 1.574639 acc: 0.911700\n",
      "Epoch  865/2000 train_loss: 1.574612 acc: 0.911800\n",
      "Epoch  866/2000 train_loss: 1.574583 acc: 0.911700\n",
      "Epoch  867/2000 train_loss: 1.574556 acc: 0.911700\n",
      "Epoch  868/2000 train_loss: 1.574528 acc: 0.911800\n",
      "Epoch  869/2000 train_loss: 1.574500 acc: 0.911800\n",
      "Epoch  870/2000 train_loss: 1.574472 acc: 0.911800\n",
      "Epoch  871/2000 train_loss: 1.574445 acc: 0.911900\n",
      "Epoch  872/2000 train_loss: 1.574417 acc: 0.911900\n",
      "Epoch  873/2000 train_loss: 1.574389 acc: 0.911900\n",
      "Epoch  874/2000 train_loss: 1.574362 acc: 0.912000\n",
      "Epoch  875/2000 train_loss: 1.574334 acc: 0.911900\n",
      "Epoch  876/2000 train_loss: 1.574307 acc: 0.911900\n",
      "Epoch  877/2000 train_loss: 1.574279 acc: 0.911900\n",
      "Epoch  878/2000 train_loss: 1.574252 acc: 0.912000\n",
      "Epoch  879/2000 train_loss: 1.574225 acc: 0.912100\n",
      "Epoch  880/2000 train_loss: 1.574197 acc: 0.912100\n",
      "Epoch  881/2000 train_loss: 1.574171 acc: 0.912000\n",
      "Epoch  882/2000 train_loss: 1.574143 acc: 0.912000\n",
      "Epoch  883/2000 train_loss: 1.574116 acc: 0.912000\n",
      "Epoch  884/2000 train_loss: 1.574089 acc: 0.912000\n",
      "Epoch  885/2000 train_loss: 1.574062 acc: 0.912000\n",
      "Epoch  886/2000 train_loss: 1.574035 acc: 0.912100\n",
      "Epoch  887/2000 train_loss: 1.574008 acc: 0.912000\n",
      "Epoch  888/2000 train_loss: 1.573981 acc: 0.912000\n",
      "Epoch  889/2000 train_loss: 1.573954 acc: 0.912100\n",
      "Epoch  890/2000 train_loss: 1.573928 acc: 0.912000\n",
      "Epoch  891/2000 train_loss: 1.573901 acc: 0.912200\n",
      "Epoch  892/2000 train_loss: 1.573874 acc: 0.912200\n",
      "Epoch  893/2000 train_loss: 1.573848 acc: 0.912100\n",
      "Epoch  894/2000 train_loss: 1.573821 acc: 0.912300\n",
      "Epoch  895/2000 train_loss: 1.573795 acc: 0.912300\n",
      "Epoch  896/2000 train_loss: 1.573768 acc: 0.912200\n",
      "Epoch  897/2000 train_loss: 1.573741 acc: 0.912400\n",
      "Epoch  898/2000 train_loss: 1.573716 acc: 0.912500\n",
      "Epoch  899/2000 train_loss: 1.573689 acc: 0.912500\n",
      "Epoch  900/2000 train_loss: 1.573662 acc: 0.912500\n",
      "Epoch  901/2000 train_loss: 1.573637 acc: 0.912600\n",
      "Epoch  902/2000 train_loss: 1.573610 acc: 0.912500\n",
      "Epoch  903/2000 train_loss: 1.573585 acc: 0.912500\n",
      "Epoch  904/2000 train_loss: 1.573558 acc: 0.912700\n",
      "Epoch  905/2000 train_loss: 1.573532 acc: 0.912600\n",
      "Epoch  906/2000 train_loss: 1.573506 acc: 0.912700\n",
      "Epoch  907/2000 train_loss: 1.573480 acc: 0.912600\n",
      "Epoch  908/2000 train_loss: 1.573455 acc: 0.912700\n",
      "Epoch  909/2000 train_loss: 1.573428 acc: 0.912700\n",
      "Epoch  910/2000 train_loss: 1.573403 acc: 0.912700\n",
      "Epoch  911/2000 train_loss: 1.573376 acc: 0.912700\n",
      "Epoch  912/2000 train_loss: 1.573351 acc: 0.912700\n",
      "Epoch  913/2000 train_loss: 1.573325 acc: 0.912800\n",
      "Epoch  914/2000 train_loss: 1.573300 acc: 0.912700\n",
      "Epoch  915/2000 train_loss: 1.573274 acc: 0.912700\n",
      "Epoch  916/2000 train_loss: 1.573249 acc: 0.912800\n",
      "Epoch  917/2000 train_loss: 1.573223 acc: 0.912800\n",
      "Epoch  918/2000 train_loss: 1.573198 acc: 0.912800\n",
      "Epoch  919/2000 train_loss: 1.573172 acc: 0.913000\n",
      "Epoch  920/2000 train_loss: 1.573147 acc: 0.913100\n",
      "Epoch  921/2000 train_loss: 1.573122 acc: 0.913000\n",
      "Epoch  922/2000 train_loss: 1.573096 acc: 0.912900\n",
      "Epoch  923/2000 train_loss: 1.573071 acc: 0.913100\n",
      "Epoch  924/2000 train_loss: 1.573045 acc: 0.913100\n",
      "Epoch  925/2000 train_loss: 1.573020 acc: 0.913000\n",
      "Epoch  926/2000 train_loss: 1.572996 acc: 0.913100\n",
      "Epoch  927/2000 train_loss: 1.572970 acc: 0.913200\n",
      "Epoch  928/2000 train_loss: 1.572945 acc: 0.913100\n",
      "Epoch  929/2000 train_loss: 1.572920 acc: 0.913100\n",
      "Epoch  930/2000 train_loss: 1.572895 acc: 0.913100\n",
      "Epoch  931/2000 train_loss: 1.572870 acc: 0.913100\n",
      "Epoch  932/2000 train_loss: 1.572845 acc: 0.913200\n",
      "Epoch  933/2000 train_loss: 1.572820 acc: 0.913100\n",
      "Epoch  934/2000 train_loss: 1.572796 acc: 0.913300\n",
      "Epoch  935/2000 train_loss: 1.572771 acc: 0.913200\n",
      "Epoch  936/2000 train_loss: 1.572746 acc: 0.913200\n",
      "Epoch  937/2000 train_loss: 1.572721 acc: 0.913200\n",
      "Epoch  938/2000 train_loss: 1.572697 acc: 0.913200\n",
      "Epoch  939/2000 train_loss: 1.572673 acc: 0.913200\n",
      "Epoch  940/2000 train_loss: 1.572648 acc: 0.913200\n",
      "Epoch  941/2000 train_loss: 1.572623 acc: 0.913200\n",
      "Epoch  942/2000 train_loss: 1.572599 acc: 0.913200\n",
      "Epoch  943/2000 train_loss: 1.572574 acc: 0.913200\n",
      "Epoch  944/2000 train_loss: 1.572550 acc: 0.913200\n",
      "Epoch  945/2000 train_loss: 1.572526 acc: 0.913300\n",
      "Epoch  946/2000 train_loss: 1.572501 acc: 0.913300\n",
      "Epoch  947/2000 train_loss: 1.572478 acc: 0.913300\n",
      "Epoch  948/2000 train_loss: 1.572453 acc: 0.913400\n",
      "Epoch  949/2000 train_loss: 1.572430 acc: 0.913200\n",
      "Epoch  950/2000 train_loss: 1.572405 acc: 0.913300\n",
      "Epoch  951/2000 train_loss: 1.572381 acc: 0.913400\n",
      "Epoch  952/2000 train_loss: 1.572356 acc: 0.913400\n",
      "Epoch  953/2000 train_loss: 1.572333 acc: 0.913400\n",
      "Epoch  954/2000 train_loss: 1.572309 acc: 0.913300\n",
      "Epoch  955/2000 train_loss: 1.572285 acc: 0.913400\n",
      "Epoch  956/2000 train_loss: 1.572261 acc: 0.913400\n",
      "Epoch  957/2000 train_loss: 1.572237 acc: 0.913500\n",
      "Epoch  958/2000 train_loss: 1.572213 acc: 0.913400\n",
      "Epoch  959/2000 train_loss: 1.572189 acc: 0.913400\n",
      "Epoch  960/2000 train_loss: 1.572165 acc: 0.913300\n",
      "Epoch  961/2000 train_loss: 1.572142 acc: 0.913400\n",
      "Epoch  962/2000 train_loss: 1.572118 acc: 0.913600\n",
      "Epoch  963/2000 train_loss: 1.572095 acc: 0.913600\n",
      "Epoch  964/2000 train_loss: 1.572071 acc: 0.913500\n",
      "Epoch  965/2000 train_loss: 1.572048 acc: 0.913400\n",
      "Epoch  966/2000 train_loss: 1.572024 acc: 0.913200\n",
      "Epoch  967/2000 train_loss: 1.572000 acc: 0.913400\n",
      "Epoch  968/2000 train_loss: 1.571977 acc: 0.913500\n",
      "Epoch  969/2000 train_loss: 1.571954 acc: 0.913600\n",
      "Epoch  970/2000 train_loss: 1.571931 acc: 0.913600\n",
      "Epoch  971/2000 train_loss: 1.571907 acc: 0.913600\n",
      "Epoch  972/2000 train_loss: 1.571884 acc: 0.913600\n",
      "Epoch  973/2000 train_loss: 1.571860 acc: 0.913600\n",
      "Epoch  974/2000 train_loss: 1.571837 acc: 0.913600\n",
      "Epoch  975/2000 train_loss: 1.571815 acc: 0.913600\n",
      "Epoch  976/2000 train_loss: 1.571791 acc: 0.913500\n",
      "Epoch  977/2000 train_loss: 1.571768 acc: 0.913700\n",
      "Epoch  978/2000 train_loss: 1.571745 acc: 0.913600\n",
      "Epoch  979/2000 train_loss: 1.571721 acc: 0.913500\n",
      "Epoch  980/2000 train_loss: 1.571699 acc: 0.913500\n",
      "Epoch  981/2000 train_loss: 1.571676 acc: 0.913600\n",
      "Epoch  982/2000 train_loss: 1.571652 acc: 0.913600\n",
      "Epoch  983/2000 train_loss: 1.571630 acc: 0.913600\n",
      "Epoch  984/2000 train_loss: 1.571607 acc: 0.913600\n",
      "Epoch  985/2000 train_loss: 1.571584 acc: 0.913700\n",
      "Epoch  986/2000 train_loss: 1.571562 acc: 0.913600\n",
      "Epoch  987/2000 train_loss: 1.571539 acc: 0.913500\n",
      "Epoch  988/2000 train_loss: 1.571516 acc: 0.913600\n",
      "Epoch  989/2000 train_loss: 1.571493 acc: 0.913700\n",
      "Epoch  990/2000 train_loss: 1.571471 acc: 0.913600\n",
      "Epoch  991/2000 train_loss: 1.571449 acc: 0.913600\n",
      "Epoch  992/2000 train_loss: 1.571426 acc: 0.913600\n",
      "Epoch  993/2000 train_loss: 1.571404 acc: 0.913600\n",
      "Epoch  994/2000 train_loss: 1.571381 acc: 0.913500\n",
      "Epoch  995/2000 train_loss: 1.571358 acc: 0.913500\n",
      "Epoch  996/2000 train_loss: 1.571336 acc: 0.913500\n",
      "Epoch  997/2000 train_loss: 1.571314 acc: 0.913500\n",
      "Epoch  998/2000 train_loss: 1.571291 acc: 0.913600\n",
      "Epoch  999/2000 train_loss: 1.571269 acc: 0.913500\n",
      "Epoch 1000/2000 train_loss: 1.571247 acc: 0.913600\n",
      "Epoch 1001/2000 train_loss: 1.571224 acc: 0.913700\n",
      "Epoch 1002/2000 train_loss: 1.571202 acc: 0.913700\n",
      "Epoch 1003/2000 train_loss: 1.571180 acc: 0.913900\n",
      "Epoch 1004/2000 train_loss: 1.571158 acc: 0.913800\n",
      "Epoch 1005/2000 train_loss: 1.571136 acc: 0.913700\n",
      "Epoch 1006/2000 train_loss: 1.571114 acc: 0.913900\n",
      "Epoch 1007/2000 train_loss: 1.571092 acc: 0.913900\n",
      "Epoch 1008/2000 train_loss: 1.571070 acc: 0.913900\n",
      "Epoch 1009/2000 train_loss: 1.571047 acc: 0.913800\n",
      "Epoch 1010/2000 train_loss: 1.571025 acc: 0.914000\n",
      "Epoch 1011/2000 train_loss: 1.571004 acc: 0.914000\n",
      "Epoch 1012/2000 train_loss: 1.570982 acc: 0.914000\n",
      "Epoch 1013/2000 train_loss: 1.570960 acc: 0.914000\n",
      "Epoch 1014/2000 train_loss: 1.570938 acc: 0.914000\n",
      "Epoch 1015/2000 train_loss: 1.570917 acc: 0.914000\n",
      "Epoch 1016/2000 train_loss: 1.570895 acc: 0.914000\n",
      "Epoch 1017/2000 train_loss: 1.570873 acc: 0.914000\n",
      "Epoch 1018/2000 train_loss: 1.570851 acc: 0.914000\n",
      "Epoch 1019/2000 train_loss: 1.570830 acc: 0.914000\n",
      "Epoch 1020/2000 train_loss: 1.570808 acc: 0.914000\n",
      "Epoch 1021/2000 train_loss: 1.570787 acc: 0.914100\n",
      "Epoch 1022/2000 train_loss: 1.570765 acc: 0.914000\n",
      "Epoch 1023/2000 train_loss: 1.570744 acc: 0.914100\n",
      "Epoch 1024/2000 train_loss: 1.570722 acc: 0.914000\n",
      "Epoch 1025/2000 train_loss: 1.570701 acc: 0.914000\n",
      "Epoch 1026/2000 train_loss: 1.570679 acc: 0.914100\n",
      "Epoch 1027/2000 train_loss: 1.570658 acc: 0.914200\n",
      "Epoch 1028/2000 train_loss: 1.570636 acc: 0.914200\n",
      "Epoch 1029/2000 train_loss: 1.570615 acc: 0.914200\n",
      "Epoch 1030/2000 train_loss: 1.570594 acc: 0.914300\n",
      "Epoch 1031/2000 train_loss: 1.570573 acc: 0.914100\n",
      "Epoch 1032/2000 train_loss: 1.570551 acc: 0.914100\n",
      "Epoch 1033/2000 train_loss: 1.570530 acc: 0.914100\n",
      "Epoch 1034/2000 train_loss: 1.570509 acc: 0.914300\n",
      "Epoch 1035/2000 train_loss: 1.570487 acc: 0.914300\n",
      "Epoch 1036/2000 train_loss: 1.570467 acc: 0.914400\n",
      "Epoch 1037/2000 train_loss: 1.570446 acc: 0.914400\n",
      "Epoch 1038/2000 train_loss: 1.570425 acc: 0.914400\n",
      "Epoch 1039/2000 train_loss: 1.570404 acc: 0.914500\n",
      "Epoch 1040/2000 train_loss: 1.570383 acc: 0.914500\n",
      "Epoch 1041/2000 train_loss: 1.570362 acc: 0.914400\n",
      "Epoch 1042/2000 train_loss: 1.570341 acc: 0.914600\n",
      "Epoch 1043/2000 train_loss: 1.570320 acc: 0.914400\n",
      "Epoch 1044/2000 train_loss: 1.570299 acc: 0.914300\n",
      "Epoch 1045/2000 train_loss: 1.570279 acc: 0.914400\n",
      "Epoch 1046/2000 train_loss: 1.570258 acc: 0.914600\n",
      "Epoch 1047/2000 train_loss: 1.570237 acc: 0.914700\n",
      "Epoch 1048/2000 train_loss: 1.570216 acc: 0.914600\n",
      "Epoch 1049/2000 train_loss: 1.570196 acc: 0.914700\n",
      "Epoch 1050/2000 train_loss: 1.570175 acc: 0.914600\n",
      "Epoch 1051/2000 train_loss: 1.570155 acc: 0.914800\n",
      "Epoch 1052/2000 train_loss: 1.570134 acc: 0.914800\n",
      "Epoch 1053/2000 train_loss: 1.570113 acc: 0.914800\n",
      "Epoch 1054/2000 train_loss: 1.570093 acc: 0.915000\n",
      "Epoch 1055/2000 train_loss: 1.570072 acc: 0.915100\n",
      "Epoch 1056/2000 train_loss: 1.570052 acc: 0.915100\n",
      "Epoch 1057/2000 train_loss: 1.570031 acc: 0.915000\n",
      "Epoch 1058/2000 train_loss: 1.570011 acc: 0.915200\n",
      "Epoch 1059/2000 train_loss: 1.569991 acc: 0.915000\n",
      "Epoch 1060/2000 train_loss: 1.569970 acc: 0.915100\n",
      "Epoch 1061/2000 train_loss: 1.569950 acc: 0.915200\n",
      "Epoch 1062/2000 train_loss: 1.569929 acc: 0.915100\n",
      "Epoch 1063/2000 train_loss: 1.569909 acc: 0.915400\n",
      "Epoch 1064/2000 train_loss: 1.569889 acc: 0.915200\n",
      "Epoch 1065/2000 train_loss: 1.569869 acc: 0.915200\n",
      "Epoch 1066/2000 train_loss: 1.569849 acc: 0.915400\n",
      "Epoch 1067/2000 train_loss: 1.569829 acc: 0.915300\n",
      "Epoch 1068/2000 train_loss: 1.569808 acc: 0.915300\n",
      "Epoch 1069/2000 train_loss: 1.569788 acc: 0.915400\n",
      "Epoch 1070/2000 train_loss: 1.569768 acc: 0.915200\n",
      "Epoch 1071/2000 train_loss: 1.569748 acc: 0.915400\n",
      "Epoch 1072/2000 train_loss: 1.569728 acc: 0.915400\n",
      "Epoch 1073/2000 train_loss: 1.569708 acc: 0.915300\n",
      "Epoch 1074/2000 train_loss: 1.569688 acc: 0.915300\n",
      "Epoch 1075/2000 train_loss: 1.569668 acc: 0.915600\n",
      "Epoch 1076/2000 train_loss: 1.569648 acc: 0.915500\n",
      "Epoch 1077/2000 train_loss: 1.569629 acc: 0.915600\n",
      "Epoch 1078/2000 train_loss: 1.569609 acc: 0.915500\n",
      "Epoch 1079/2000 train_loss: 1.569589 acc: 0.915400\n",
      "Epoch 1080/2000 train_loss: 1.569570 acc: 0.915500\n",
      "Epoch 1081/2000 train_loss: 1.569549 acc: 0.915500\n",
      "Epoch 1082/2000 train_loss: 1.569530 acc: 0.915400\n",
      "Epoch 1083/2000 train_loss: 1.569510 acc: 0.915400\n",
      "Epoch 1084/2000 train_loss: 1.569490 acc: 0.915400\n",
      "Epoch 1085/2000 train_loss: 1.569470 acc: 0.915500\n",
      "Epoch 1086/2000 train_loss: 1.569451 acc: 0.915500\n",
      "Epoch 1087/2000 train_loss: 1.569431 acc: 0.915400\n",
      "Epoch 1088/2000 train_loss: 1.569411 acc: 0.915500\n",
      "Epoch 1089/2000 train_loss: 1.569392 acc: 0.915600\n",
      "Epoch 1090/2000 train_loss: 1.569373 acc: 0.915500\n",
      "Epoch 1091/2000 train_loss: 1.569353 acc: 0.915400\n",
      "Epoch 1092/2000 train_loss: 1.569334 acc: 0.915500\n",
      "Epoch 1093/2000 train_loss: 1.569314 acc: 0.915500\n",
      "Epoch 1094/2000 train_loss: 1.569295 acc: 0.915500\n",
      "Epoch 1095/2000 train_loss: 1.569275 acc: 0.915500\n",
      "Epoch 1096/2000 train_loss: 1.569256 acc: 0.915600\n",
      "Epoch 1097/2000 train_loss: 1.569237 acc: 0.915500\n",
      "Epoch 1098/2000 train_loss: 1.569218 acc: 0.915500\n",
      "Epoch 1099/2000 train_loss: 1.569198 acc: 0.915600\n",
      "Epoch 1100/2000 train_loss: 1.569179 acc: 0.915700\n",
      "Epoch 1101/2000 train_loss: 1.569160 acc: 0.915700\n",
      "Epoch 1102/2000 train_loss: 1.569141 acc: 0.915700\n",
      "Epoch 1103/2000 train_loss: 1.569121 acc: 0.915700\n",
      "Epoch 1104/2000 train_loss: 1.569102 acc: 0.915700\n",
      "Epoch 1105/2000 train_loss: 1.569083 acc: 0.915600\n",
      "Epoch 1106/2000 train_loss: 1.569065 acc: 0.915700\n",
      "Epoch 1107/2000 train_loss: 1.569045 acc: 0.915600\n",
      "Epoch 1108/2000 train_loss: 1.569026 acc: 0.915700\n",
      "Epoch 1109/2000 train_loss: 1.569007 acc: 0.915600\n",
      "Epoch 1110/2000 train_loss: 1.568988 acc: 0.915700\n",
      "Epoch 1111/2000 train_loss: 1.568969 acc: 0.915600\n",
      "Epoch 1112/2000 train_loss: 1.568950 acc: 0.915600\n",
      "Epoch 1113/2000 train_loss: 1.568931 acc: 0.915600\n",
      "Epoch 1114/2000 train_loss: 1.568912 acc: 0.915600\n",
      "Epoch 1115/2000 train_loss: 1.568894 acc: 0.915600\n",
      "Epoch 1116/2000 train_loss: 1.568874 acc: 0.915500\n",
      "Epoch 1117/2000 train_loss: 1.568856 acc: 0.915600\n",
      "Epoch 1118/2000 train_loss: 1.568837 acc: 0.915600\n",
      "Epoch 1119/2000 train_loss: 1.568819 acc: 0.915600\n",
      "Epoch 1120/2000 train_loss: 1.568800 acc: 0.915500\n",
      "Epoch 1121/2000 train_loss: 1.568781 acc: 0.915500\n",
      "Epoch 1122/2000 train_loss: 1.568762 acc: 0.915700\n",
      "Epoch 1123/2000 train_loss: 1.568744 acc: 0.915600\n",
      "Epoch 1124/2000 train_loss: 1.568725 acc: 0.915500\n",
      "Epoch 1125/2000 train_loss: 1.568706 acc: 0.915700\n",
      "Epoch 1126/2000 train_loss: 1.568688 acc: 0.915600\n",
      "Epoch 1127/2000 train_loss: 1.568670 acc: 0.915800\n",
      "Epoch 1128/2000 train_loss: 1.568651 acc: 0.915800\n",
      "Epoch 1129/2000 train_loss: 1.568633 acc: 0.915700\n",
      "Epoch 1130/2000 train_loss: 1.568614 acc: 0.915800\n",
      "Epoch 1131/2000 train_loss: 1.568595 acc: 0.915800\n",
      "Epoch 1132/2000 train_loss: 1.568578 acc: 0.915900\n",
      "Epoch 1133/2000 train_loss: 1.568558 acc: 0.915800\n",
      "Epoch 1134/2000 train_loss: 1.568540 acc: 0.915800\n",
      "Epoch 1135/2000 train_loss: 1.568522 acc: 0.915800\n",
      "Epoch 1136/2000 train_loss: 1.568503 acc: 0.915800\n",
      "Epoch 1137/2000 train_loss: 1.568485 acc: 0.915800\n",
      "Epoch 1138/2000 train_loss: 1.568467 acc: 0.915800\n",
      "Epoch 1139/2000 train_loss: 1.568449 acc: 0.915800\n",
      "Epoch 1140/2000 train_loss: 1.568431 acc: 0.915900\n",
      "Epoch 1141/2000 train_loss: 1.568412 acc: 0.915800\n",
      "Epoch 1142/2000 train_loss: 1.568394 acc: 0.915800\n",
      "Epoch 1143/2000 train_loss: 1.568376 acc: 0.915900\n",
      "Epoch 1144/2000 train_loss: 1.568358 acc: 0.915900\n",
      "Epoch 1145/2000 train_loss: 1.568340 acc: 0.915800\n",
      "Epoch 1146/2000 train_loss: 1.568322 acc: 0.915800\n",
      "Epoch 1147/2000 train_loss: 1.568304 acc: 0.915900\n",
      "Epoch 1148/2000 train_loss: 1.568286 acc: 0.916000\n",
      "Epoch 1149/2000 train_loss: 1.568268 acc: 0.916000\n",
      "Epoch 1150/2000 train_loss: 1.568249 acc: 0.916100\n",
      "Epoch 1151/2000 train_loss: 1.568232 acc: 0.916100\n",
      "Epoch 1152/2000 train_loss: 1.568214 acc: 0.916100\n",
      "Epoch 1153/2000 train_loss: 1.568196 acc: 0.916100\n",
      "Epoch 1154/2000 train_loss: 1.568178 acc: 0.916000\n",
      "Epoch 1155/2000 train_loss: 1.568160 acc: 0.916100\n",
      "Epoch 1156/2000 train_loss: 1.568142 acc: 0.916100\n",
      "Epoch 1157/2000 train_loss: 1.568125 acc: 0.916100\n",
      "Epoch 1158/2000 train_loss: 1.568107 acc: 0.916100\n",
      "Epoch 1159/2000 train_loss: 1.568089 acc: 0.916100\n",
      "Epoch 1160/2000 train_loss: 1.568071 acc: 0.916100\n",
      "Epoch 1161/2000 train_loss: 1.568054 acc: 0.916100\n",
      "Epoch 1162/2000 train_loss: 1.568036 acc: 0.916100\n",
      "Epoch 1163/2000 train_loss: 1.568018 acc: 0.916100\n",
      "Epoch 1164/2000 train_loss: 1.568001 acc: 0.916100\n",
      "Epoch 1165/2000 train_loss: 1.567983 acc: 0.916000\n",
      "Epoch 1166/2000 train_loss: 1.567966 acc: 0.916100\n",
      "Epoch 1167/2000 train_loss: 1.567948 acc: 0.916100\n",
      "Epoch 1168/2000 train_loss: 1.567930 acc: 0.916000\n",
      "Epoch 1169/2000 train_loss: 1.567913 acc: 0.916100\n",
      "Epoch 1170/2000 train_loss: 1.567894 acc: 0.916000\n",
      "Epoch 1171/2000 train_loss: 1.567878 acc: 0.916100\n",
      "Epoch 1172/2000 train_loss: 1.567860 acc: 0.916000\n",
      "Epoch 1173/2000 train_loss: 1.567843 acc: 0.916000\n",
      "Epoch 1174/2000 train_loss: 1.567825 acc: 0.916100\n",
      "Epoch 1175/2000 train_loss: 1.567807 acc: 0.916100\n",
      "Epoch 1176/2000 train_loss: 1.567790 acc: 0.916100\n",
      "Epoch 1177/2000 train_loss: 1.567773 acc: 0.916100\n",
      "Epoch 1178/2000 train_loss: 1.567756 acc: 0.916100\n",
      "Epoch 1179/2000 train_loss: 1.567738 acc: 0.916000\n",
      "Epoch 1180/2000 train_loss: 1.567721 acc: 0.916000\n",
      "Epoch 1181/2000 train_loss: 1.567704 acc: 0.916000\n",
      "Epoch 1182/2000 train_loss: 1.567686 acc: 0.916100\n",
      "Epoch 1183/2000 train_loss: 1.567669 acc: 0.916100\n",
      "Epoch 1184/2000 train_loss: 1.567652 acc: 0.916000\n",
      "Epoch 1185/2000 train_loss: 1.567635 acc: 0.916000\n",
      "Epoch 1186/2000 train_loss: 1.567618 acc: 0.916000\n",
      "Epoch 1187/2000 train_loss: 1.567601 acc: 0.916000\n",
      "Epoch 1188/2000 train_loss: 1.567583 acc: 0.916000\n",
      "Epoch 1189/2000 train_loss: 1.567567 acc: 0.916100\n",
      "Epoch 1190/2000 train_loss: 1.567549 acc: 0.916000\n",
      "Epoch 1191/2000 train_loss: 1.567533 acc: 0.916100\n",
      "Epoch 1192/2000 train_loss: 1.567515 acc: 0.916000\n",
      "Epoch 1193/2000 train_loss: 1.567499 acc: 0.916100\n",
      "Epoch 1194/2000 train_loss: 1.567482 acc: 0.916100\n",
      "Epoch 1195/2000 train_loss: 1.567464 acc: 0.916100\n",
      "Epoch 1196/2000 train_loss: 1.567448 acc: 0.916100\n",
      "Epoch 1197/2000 train_loss: 1.567431 acc: 0.916100\n",
      "Epoch 1198/2000 train_loss: 1.567414 acc: 0.916100\n",
      "Epoch 1199/2000 train_loss: 1.567396 acc: 0.916100\n",
      "Epoch 1200/2000 train_loss: 1.567379 acc: 0.916100\n",
      "Epoch 1201/2000 train_loss: 1.567363 acc: 0.916000\n",
      "Epoch 1202/2000 train_loss: 1.567346 acc: 0.916100\n",
      "Epoch 1203/2000 train_loss: 1.567329 acc: 0.916100\n",
      "Epoch 1204/2000 train_loss: 1.567313 acc: 0.916100\n",
      "Epoch 1205/2000 train_loss: 1.567296 acc: 0.916100\n",
      "Epoch 1206/2000 train_loss: 1.567279 acc: 0.916000\n",
      "Epoch 1207/2000 train_loss: 1.567262 acc: 0.916100\n",
      "Epoch 1208/2000 train_loss: 1.567246 acc: 0.916200\n",
      "Epoch 1209/2000 train_loss: 1.567229 acc: 0.916100\n",
      "Epoch 1210/2000 train_loss: 1.567212 acc: 0.916100\n",
      "Epoch 1211/2000 train_loss: 1.567196 acc: 0.916100\n",
      "Epoch 1212/2000 train_loss: 1.567178 acc: 0.916100\n",
      "Epoch 1213/2000 train_loss: 1.567163 acc: 0.916100\n",
      "Epoch 1214/2000 train_loss: 1.567146 acc: 0.916100\n",
      "Epoch 1215/2000 train_loss: 1.567129 acc: 0.916100\n",
      "Epoch 1216/2000 train_loss: 1.567113 acc: 0.916100\n",
      "Epoch 1217/2000 train_loss: 1.567097 acc: 0.916100\n",
      "Epoch 1218/2000 train_loss: 1.567079 acc: 0.916100\n",
      "Epoch 1219/2000 train_loss: 1.567063 acc: 0.916100\n",
      "Epoch 1220/2000 train_loss: 1.567047 acc: 0.916200\n",
      "Epoch 1221/2000 train_loss: 1.567031 acc: 0.916300\n",
      "Epoch 1222/2000 train_loss: 1.567014 acc: 0.916200\n",
      "Epoch 1223/2000 train_loss: 1.566998 acc: 0.916200\n",
      "Epoch 1224/2000 train_loss: 1.566981 acc: 0.916200\n",
      "Epoch 1225/2000 train_loss: 1.566965 acc: 0.916100\n",
      "Epoch 1226/2000 train_loss: 1.566948 acc: 0.916200\n",
      "Epoch 1227/2000 train_loss: 1.566933 acc: 0.916200\n",
      "Epoch 1228/2000 train_loss: 1.566916 acc: 0.916200\n",
      "Epoch 1229/2000 train_loss: 1.566900 acc: 0.916400\n",
      "Epoch 1230/2000 train_loss: 1.566884 acc: 0.916300\n",
      "Epoch 1231/2000 train_loss: 1.566867 acc: 0.916400\n",
      "Epoch 1232/2000 train_loss: 1.566851 acc: 0.916400\n",
      "Epoch 1233/2000 train_loss: 1.566835 acc: 0.916400\n",
      "Epoch 1234/2000 train_loss: 1.566818 acc: 0.916400\n",
      "Epoch 1235/2000 train_loss: 1.566802 acc: 0.916500\n",
      "Epoch 1236/2000 train_loss: 1.566786 acc: 0.916400\n",
      "Epoch 1237/2000 train_loss: 1.566771 acc: 0.916500\n",
      "Epoch 1238/2000 train_loss: 1.566754 acc: 0.916400\n",
      "Epoch 1239/2000 train_loss: 1.566738 acc: 0.916500\n",
      "Epoch 1240/2000 train_loss: 1.566722 acc: 0.916500\n",
      "Epoch 1241/2000 train_loss: 1.566707 acc: 0.916500\n",
      "Epoch 1242/2000 train_loss: 1.566691 acc: 0.916500\n",
      "Epoch 1243/2000 train_loss: 1.566674 acc: 0.916500\n",
      "Epoch 1244/2000 train_loss: 1.566658 acc: 0.916500\n",
      "Epoch 1245/2000 train_loss: 1.566642 acc: 0.916500\n",
      "Epoch 1246/2000 train_loss: 1.566626 acc: 0.916500\n",
      "Epoch 1247/2000 train_loss: 1.566610 acc: 0.916500\n",
      "Epoch 1248/2000 train_loss: 1.566594 acc: 0.916500\n",
      "Epoch 1249/2000 train_loss: 1.566579 acc: 0.916500\n",
      "Epoch 1250/2000 train_loss: 1.566562 acc: 0.916500\n",
      "Epoch 1251/2000 train_loss: 1.566547 acc: 0.916400\n",
      "Epoch 1252/2000 train_loss: 1.566531 acc: 0.916500\n",
      "Epoch 1253/2000 train_loss: 1.566515 acc: 0.916500\n",
      "Epoch 1254/2000 train_loss: 1.566499 acc: 0.916500\n",
      "Epoch 1255/2000 train_loss: 1.566483 acc: 0.916400\n",
      "Epoch 1256/2000 train_loss: 1.566468 acc: 0.916500\n",
      "Epoch 1257/2000 train_loss: 1.566452 acc: 0.916500\n",
      "Epoch 1258/2000 train_loss: 1.566436 acc: 0.916500\n",
      "Epoch 1259/2000 train_loss: 1.566420 acc: 0.916500\n",
      "Epoch 1260/2000 train_loss: 1.566405 acc: 0.916500\n",
      "Epoch 1261/2000 train_loss: 1.566389 acc: 0.916500\n",
      "Epoch 1262/2000 train_loss: 1.566374 acc: 0.916500\n",
      "Epoch 1263/2000 train_loss: 1.566358 acc: 0.916500\n",
      "Epoch 1264/2000 train_loss: 1.566343 acc: 0.916500\n",
      "Epoch 1265/2000 train_loss: 1.566327 acc: 0.916500\n",
      "Epoch 1266/2000 train_loss: 1.566311 acc: 0.916500\n",
      "Epoch 1267/2000 train_loss: 1.566296 acc: 0.916500\n",
      "Epoch 1268/2000 train_loss: 1.566280 acc: 0.916600\n",
      "Epoch 1269/2000 train_loss: 1.566265 acc: 0.916600\n",
      "Epoch 1270/2000 train_loss: 1.566249 acc: 0.916400\n",
      "Epoch 1271/2000 train_loss: 1.566233 acc: 0.916500\n",
      "Epoch 1272/2000 train_loss: 1.566218 acc: 0.916600\n",
      "Epoch 1273/2000 train_loss: 1.566203 acc: 0.916600\n",
      "Epoch 1274/2000 train_loss: 1.566188 acc: 0.916500\n",
      "Epoch 1275/2000 train_loss: 1.566172 acc: 0.916600\n",
      "Epoch 1276/2000 train_loss: 1.566156 acc: 0.916600\n",
      "Epoch 1277/2000 train_loss: 1.566141 acc: 0.916700\n",
      "Epoch 1278/2000 train_loss: 1.566126 acc: 0.916600\n",
      "Epoch 1279/2000 train_loss: 1.566111 acc: 0.916700\n",
      "Epoch 1280/2000 train_loss: 1.566096 acc: 0.916700\n",
      "Epoch 1281/2000 train_loss: 1.566080 acc: 0.916600\n",
      "Epoch 1282/2000 train_loss: 1.566065 acc: 0.916600\n",
      "Epoch 1283/2000 train_loss: 1.566049 acc: 0.916600\n",
      "Epoch 1284/2000 train_loss: 1.566034 acc: 0.916600\n",
      "Epoch 1285/2000 train_loss: 1.566019 acc: 0.916600\n",
      "Epoch 1286/2000 train_loss: 1.566003 acc: 0.916700\n",
      "Epoch 1287/2000 train_loss: 1.565988 acc: 0.916600\n",
      "Epoch 1288/2000 train_loss: 1.565973 acc: 0.916500\n",
      "Epoch 1289/2000 train_loss: 1.565958 acc: 0.916500\n",
      "Epoch 1290/2000 train_loss: 1.565943 acc: 0.916500\n",
      "Epoch 1291/2000 train_loss: 1.565928 acc: 0.916600\n",
      "Epoch 1292/2000 train_loss: 1.565913 acc: 0.916500\n",
      "Epoch 1293/2000 train_loss: 1.565898 acc: 0.916500\n",
      "Epoch 1294/2000 train_loss: 1.565883 acc: 0.916500\n",
      "Epoch 1295/2000 train_loss: 1.565868 acc: 0.916500\n",
      "Epoch 1296/2000 train_loss: 1.565852 acc: 0.916600\n",
      "Epoch 1297/2000 train_loss: 1.565838 acc: 0.916500\n",
      "Epoch 1298/2000 train_loss: 1.565823 acc: 0.916500\n",
      "Epoch 1299/2000 train_loss: 1.565808 acc: 0.916500\n",
      "Epoch 1300/2000 train_loss: 1.565793 acc: 0.916500\n",
      "Epoch 1301/2000 train_loss: 1.565777 acc: 0.916500\n",
      "Epoch 1302/2000 train_loss: 1.565763 acc: 0.916500\n",
      "Epoch 1303/2000 train_loss: 1.565748 acc: 0.916600\n",
      "Epoch 1304/2000 train_loss: 1.565733 acc: 0.916600\n",
      "Epoch 1305/2000 train_loss: 1.565719 acc: 0.916600\n",
      "Epoch 1306/2000 train_loss: 1.565703 acc: 0.916600\n",
      "Epoch 1307/2000 train_loss: 1.565688 acc: 0.916600\n",
      "Epoch 1308/2000 train_loss: 1.565674 acc: 0.916700\n",
      "Epoch 1309/2000 train_loss: 1.565659 acc: 0.916800\n",
      "Epoch 1310/2000 train_loss: 1.565644 acc: 0.916700\n",
      "Epoch 1311/2000 train_loss: 1.565629 acc: 0.916800\n",
      "Epoch 1312/2000 train_loss: 1.565614 acc: 0.916700\n",
      "Epoch 1313/2000 train_loss: 1.565600 acc: 0.916700\n",
      "Epoch 1314/2000 train_loss: 1.565585 acc: 0.916700\n",
      "Epoch 1315/2000 train_loss: 1.565571 acc: 0.916700\n",
      "Epoch 1316/2000 train_loss: 1.565555 acc: 0.916800\n",
      "Epoch 1317/2000 train_loss: 1.565541 acc: 0.916700\n",
      "Epoch 1318/2000 train_loss: 1.565526 acc: 0.916800\n",
      "Epoch 1319/2000 train_loss: 1.565512 acc: 0.916700\n",
      "Epoch 1320/2000 train_loss: 1.565497 acc: 0.916800\n",
      "Epoch 1321/2000 train_loss: 1.565482 acc: 0.916800\n",
      "Epoch 1322/2000 train_loss: 1.565468 acc: 0.916800\n",
      "Epoch 1323/2000 train_loss: 1.565453 acc: 0.916800\n",
      "Epoch 1324/2000 train_loss: 1.565438 acc: 0.916800\n",
      "Epoch 1325/2000 train_loss: 1.565424 acc: 0.916800\n",
      "Epoch 1326/2000 train_loss: 1.565409 acc: 0.916800\n",
      "Epoch 1327/2000 train_loss: 1.565395 acc: 0.916800\n",
      "Epoch 1328/2000 train_loss: 1.565380 acc: 0.916800\n",
      "Epoch 1329/2000 train_loss: 1.565366 acc: 0.916800\n",
      "Epoch 1330/2000 train_loss: 1.565352 acc: 0.916800\n",
      "Epoch 1331/2000 train_loss: 1.565337 acc: 0.916800\n",
      "Epoch 1332/2000 train_loss: 1.565322 acc: 0.916700\n",
      "Epoch 1333/2000 train_loss: 1.565308 acc: 0.916800\n",
      "Epoch 1334/2000 train_loss: 1.565294 acc: 0.916900\n",
      "Epoch 1335/2000 train_loss: 1.565279 acc: 0.916800\n",
      "Epoch 1336/2000 train_loss: 1.565265 acc: 0.916800\n",
      "Epoch 1337/2000 train_loss: 1.565251 acc: 0.916800\n",
      "Epoch 1338/2000 train_loss: 1.565236 acc: 0.916800\n",
      "Epoch 1339/2000 train_loss: 1.565222 acc: 0.916800\n",
      "Epoch 1340/2000 train_loss: 1.565207 acc: 0.916900\n",
      "Epoch 1341/2000 train_loss: 1.565194 acc: 0.916700\n",
      "Epoch 1342/2000 train_loss: 1.565179 acc: 0.916800\n",
      "Epoch 1343/2000 train_loss: 1.565165 acc: 0.916700\n",
      "Epoch 1344/2000 train_loss: 1.565151 acc: 0.916600\n",
      "Epoch 1345/2000 train_loss: 1.565136 acc: 0.916900\n",
      "Epoch 1346/2000 train_loss: 1.565122 acc: 0.916800\n",
      "Epoch 1347/2000 train_loss: 1.565108 acc: 0.916900\n",
      "Epoch 1348/2000 train_loss: 1.565094 acc: 0.917000\n",
      "Epoch 1349/2000 train_loss: 1.565079 acc: 0.917000\n",
      "Epoch 1350/2000 train_loss: 1.565066 acc: 0.916900\n",
      "Epoch 1351/2000 train_loss: 1.565051 acc: 0.917100\n",
      "Epoch 1352/2000 train_loss: 1.565037 acc: 0.916900\n",
      "Epoch 1353/2000 train_loss: 1.565023 acc: 0.916800\n",
      "Epoch 1354/2000 train_loss: 1.565009 acc: 0.916900\n",
      "Epoch 1355/2000 train_loss: 1.564995 acc: 0.916900\n",
      "Epoch 1356/2000 train_loss: 1.564981 acc: 0.917000\n",
      "Epoch 1357/2000 train_loss: 1.564967 acc: 0.916800\n",
      "Epoch 1358/2000 train_loss: 1.564952 acc: 0.917000\n",
      "Epoch 1359/2000 train_loss: 1.564939 acc: 0.916900\n",
      "Epoch 1360/2000 train_loss: 1.564924 acc: 0.917100\n",
      "Epoch 1361/2000 train_loss: 1.564910 acc: 0.917000\n",
      "Epoch 1362/2000 train_loss: 1.564897 acc: 0.917000\n",
      "Epoch 1363/2000 train_loss: 1.564883 acc: 0.917200\n",
      "Epoch 1364/2000 train_loss: 1.564869 acc: 0.917100\n",
      "Epoch 1365/2000 train_loss: 1.564855 acc: 0.917000\n",
      "Epoch 1366/2000 train_loss: 1.564841 acc: 0.917000\n",
      "Epoch 1367/2000 train_loss: 1.564827 acc: 0.917300\n",
      "Epoch 1368/2000 train_loss: 1.564813 acc: 0.917200\n",
      "Epoch 1369/2000 train_loss: 1.564799 acc: 0.917000\n",
      "Epoch 1370/2000 train_loss: 1.564786 acc: 0.917200\n",
      "Epoch 1371/2000 train_loss: 1.564772 acc: 0.917300\n",
      "Epoch 1372/2000 train_loss: 1.564758 acc: 0.917400\n",
      "Epoch 1373/2000 train_loss: 1.564744 acc: 0.917100\n",
      "Epoch 1374/2000 train_loss: 1.564731 acc: 0.917300\n",
      "Epoch 1375/2000 train_loss: 1.564717 acc: 0.917200\n",
      "Epoch 1376/2000 train_loss: 1.564703 acc: 0.917400\n",
      "Epoch 1377/2000 train_loss: 1.564689 acc: 0.917400\n",
      "Epoch 1378/2000 train_loss: 1.564676 acc: 0.917400\n",
      "Epoch 1379/2000 train_loss: 1.564662 acc: 0.917300\n",
      "Epoch 1380/2000 train_loss: 1.564648 acc: 0.917400\n",
      "Epoch 1381/2000 train_loss: 1.564634 acc: 0.917400\n",
      "Epoch 1382/2000 train_loss: 1.564620 acc: 0.917400\n",
      "Epoch 1383/2000 train_loss: 1.564607 acc: 0.917400\n",
      "Epoch 1384/2000 train_loss: 1.564593 acc: 0.917400\n",
      "Epoch 1385/2000 train_loss: 1.564579 acc: 0.917400\n",
      "Epoch 1386/2000 train_loss: 1.564566 acc: 0.917300\n",
      "Epoch 1387/2000 train_loss: 1.564552 acc: 0.917300\n",
      "Epoch 1388/2000 train_loss: 1.564539 acc: 0.917400\n",
      "Epoch 1389/2000 train_loss: 1.564525 acc: 0.917400\n",
      "Epoch 1390/2000 train_loss: 1.564511 acc: 0.917400\n",
      "Epoch 1391/2000 train_loss: 1.564498 acc: 0.917500\n",
      "Epoch 1392/2000 train_loss: 1.564484 acc: 0.917400\n",
      "Epoch 1393/2000 train_loss: 1.564471 acc: 0.917500\n",
      "Epoch 1394/2000 train_loss: 1.564458 acc: 0.917400\n",
      "Epoch 1395/2000 train_loss: 1.564444 acc: 0.917400\n",
      "Epoch 1396/2000 train_loss: 1.564430 acc: 0.917500\n",
      "Epoch 1397/2000 train_loss: 1.564418 acc: 0.917400\n",
      "Epoch 1398/2000 train_loss: 1.564404 acc: 0.917400\n",
      "Epoch 1399/2000 train_loss: 1.564391 acc: 0.917500\n",
      "Epoch 1400/2000 train_loss: 1.564377 acc: 0.917500\n",
      "Epoch 1401/2000 train_loss: 1.564363 acc: 0.917500\n",
      "Epoch 1402/2000 train_loss: 1.564350 acc: 0.917400\n",
      "Epoch 1403/2000 train_loss: 1.564337 acc: 0.917500\n",
      "Epoch 1404/2000 train_loss: 1.564323 acc: 0.917500\n",
      "Epoch 1405/2000 train_loss: 1.564310 acc: 0.917600\n",
      "Epoch 1406/2000 train_loss: 1.564297 acc: 0.917400\n",
      "Epoch 1407/2000 train_loss: 1.564283 acc: 0.917500\n",
      "Epoch 1408/2000 train_loss: 1.564270 acc: 0.917400\n",
      "Epoch 1409/2000 train_loss: 1.564256 acc: 0.917400\n",
      "Epoch 1410/2000 train_loss: 1.564243 acc: 0.917700\n",
      "Epoch 1411/2000 train_loss: 1.564230 acc: 0.917500\n",
      "Epoch 1412/2000 train_loss: 1.564217 acc: 0.917600\n",
      "Epoch 1413/2000 train_loss: 1.564204 acc: 0.917400\n",
      "Epoch 1414/2000 train_loss: 1.564191 acc: 0.917500\n",
      "Epoch 1415/2000 train_loss: 1.564177 acc: 0.917600\n",
      "Epoch 1416/2000 train_loss: 1.564164 acc: 0.917600\n",
      "Epoch 1417/2000 train_loss: 1.564151 acc: 0.917700\n",
      "Epoch 1418/2000 train_loss: 1.564137 acc: 0.917700\n",
      "Epoch 1419/2000 train_loss: 1.564124 acc: 0.917600\n",
      "Epoch 1420/2000 train_loss: 1.564111 acc: 0.917600\n",
      "Epoch 1421/2000 train_loss: 1.564098 acc: 0.917600\n",
      "Epoch 1422/2000 train_loss: 1.564085 acc: 0.917800\n",
      "Epoch 1423/2000 train_loss: 1.564072 acc: 0.917700\n",
      "Epoch 1424/2000 train_loss: 1.564059 acc: 0.917800\n",
      "Epoch 1425/2000 train_loss: 1.564046 acc: 0.917700\n",
      "Epoch 1426/2000 train_loss: 1.564032 acc: 0.917700\n",
      "Epoch 1427/2000 train_loss: 1.564020 acc: 0.917700\n",
      "Epoch 1428/2000 train_loss: 1.564007 acc: 0.917600\n",
      "Epoch 1429/2000 train_loss: 1.563994 acc: 0.917800\n",
      "Epoch 1430/2000 train_loss: 1.563981 acc: 0.917700\n",
      "Epoch 1431/2000 train_loss: 1.563968 acc: 0.917700\n",
      "Epoch 1432/2000 train_loss: 1.563954 acc: 0.917600\n",
      "Epoch 1433/2000 train_loss: 1.563942 acc: 0.917700\n",
      "Epoch 1434/2000 train_loss: 1.563929 acc: 0.917600\n",
      "Epoch 1435/2000 train_loss: 1.563916 acc: 0.917600\n",
      "Epoch 1436/2000 train_loss: 1.563902 acc: 0.917800\n",
      "Epoch 1437/2000 train_loss: 1.563890 acc: 0.917800\n",
      "Epoch 1438/2000 train_loss: 1.563877 acc: 0.917700\n",
      "Epoch 1439/2000 train_loss: 1.563864 acc: 0.917800\n",
      "Epoch 1440/2000 train_loss: 1.563852 acc: 0.917800\n",
      "Epoch 1441/2000 train_loss: 1.563839 acc: 0.917800\n",
      "Epoch 1442/2000 train_loss: 1.563826 acc: 0.917700\n",
      "Epoch 1443/2000 train_loss: 1.563813 acc: 0.917800\n",
      "Epoch 1444/2000 train_loss: 1.563800 acc: 0.917800\n",
      "Epoch 1445/2000 train_loss: 1.563787 acc: 0.917900\n",
      "Epoch 1446/2000 train_loss: 1.563774 acc: 0.917800\n",
      "Epoch 1447/2000 train_loss: 1.563761 acc: 0.917700\n",
      "Epoch 1448/2000 train_loss: 1.563749 acc: 0.917800\n",
      "Epoch 1449/2000 train_loss: 1.563736 acc: 0.918000\n",
      "Epoch 1450/2000 train_loss: 1.563723 acc: 0.917800\n",
      "Epoch 1451/2000 train_loss: 1.563711 acc: 0.918000\n",
      "Epoch 1452/2000 train_loss: 1.563698 acc: 0.918000\n",
      "Epoch 1453/2000 train_loss: 1.563685 acc: 0.918000\n",
      "Epoch 1454/2000 train_loss: 1.563673 acc: 0.917800\n",
      "Epoch 1455/2000 train_loss: 1.563660 acc: 0.918000\n",
      "Epoch 1456/2000 train_loss: 1.563647 acc: 0.918100\n",
      "Epoch 1457/2000 train_loss: 1.563634 acc: 0.917800\n",
      "Epoch 1458/2000 train_loss: 1.563622 acc: 0.918000\n",
      "Epoch 1459/2000 train_loss: 1.563610 acc: 0.918100\n",
      "Epoch 1460/2000 train_loss: 1.563597 acc: 0.918000\n",
      "Epoch 1461/2000 train_loss: 1.563584 acc: 0.918000\n",
      "Epoch 1462/2000 train_loss: 1.563571 acc: 0.918000\n",
      "Epoch 1463/2000 train_loss: 1.563558 acc: 0.918000\n",
      "Epoch 1464/2000 train_loss: 1.563546 acc: 0.918200\n",
      "Epoch 1465/2000 train_loss: 1.563533 acc: 0.918100\n",
      "Epoch 1466/2000 train_loss: 1.563521 acc: 0.918100\n",
      "Epoch 1467/2000 train_loss: 1.563508 acc: 0.918100\n",
      "Epoch 1468/2000 train_loss: 1.563496 acc: 0.918200\n",
      "Epoch 1469/2000 train_loss: 1.563484 acc: 0.918100\n",
      "Epoch 1470/2000 train_loss: 1.563471 acc: 0.918200\n",
      "Epoch 1471/2000 train_loss: 1.563458 acc: 0.918200\n",
      "Epoch 1472/2000 train_loss: 1.563446 acc: 0.918100\n",
      "Epoch 1473/2000 train_loss: 1.563433 acc: 0.918200\n",
      "Epoch 1474/2000 train_loss: 1.563421 acc: 0.918100\n",
      "Epoch 1475/2000 train_loss: 1.563409 acc: 0.918100\n",
      "Epoch 1476/2000 train_loss: 1.563397 acc: 0.918100\n",
      "Epoch 1477/2000 train_loss: 1.563384 acc: 0.918200\n",
      "Epoch 1478/2000 train_loss: 1.563371 acc: 0.918100\n",
      "Epoch 1479/2000 train_loss: 1.563359 acc: 0.918100\n",
      "Epoch 1480/2000 train_loss: 1.563347 acc: 0.918100\n",
      "Epoch 1481/2000 train_loss: 1.563335 acc: 0.918200\n",
      "Epoch 1482/2000 train_loss: 1.563322 acc: 0.918100\n",
      "Epoch 1483/2000 train_loss: 1.563309 acc: 0.918200\n",
      "Epoch 1484/2000 train_loss: 1.563297 acc: 0.918200\n",
      "Epoch 1485/2000 train_loss: 1.563285 acc: 0.918100\n",
      "Epoch 1486/2000 train_loss: 1.563272 acc: 0.918100\n",
      "Epoch 1487/2000 train_loss: 1.563261 acc: 0.918100\n",
      "Epoch 1488/2000 train_loss: 1.563249 acc: 0.918100\n",
      "Epoch 1489/2000 train_loss: 1.563236 acc: 0.918300\n",
      "Epoch 1490/2000 train_loss: 1.563224 acc: 0.918100\n",
      "Epoch 1491/2000 train_loss: 1.563212 acc: 0.918400\n",
      "Epoch 1492/2000 train_loss: 1.563199 acc: 0.918200\n",
      "Epoch 1493/2000 train_loss: 1.563187 acc: 0.918100\n",
      "Epoch 1494/2000 train_loss: 1.563175 acc: 0.918200\n",
      "Epoch 1495/2000 train_loss: 1.563163 acc: 0.918300\n",
      "Epoch 1496/2000 train_loss: 1.563151 acc: 0.918100\n",
      "Epoch 1497/2000 train_loss: 1.563139 acc: 0.918200\n",
      "Epoch 1498/2000 train_loss: 1.563126 acc: 0.918200\n",
      "Epoch 1499/2000 train_loss: 1.563114 acc: 0.918200\n",
      "Epoch 1500/2000 train_loss: 1.563102 acc: 0.918200\n",
      "Epoch 1501/2000 train_loss: 1.563090 acc: 0.918200\n",
      "Epoch 1502/2000 train_loss: 1.563077 acc: 0.918200\n",
      "Epoch 1503/2000 train_loss: 1.563066 acc: 0.918300\n",
      "Epoch 1504/2000 train_loss: 1.563053 acc: 0.918200\n",
      "Epoch 1505/2000 train_loss: 1.563041 acc: 0.918200\n",
      "Epoch 1506/2000 train_loss: 1.563029 acc: 0.918400\n",
      "Epoch 1507/2000 train_loss: 1.563017 acc: 0.918400\n",
      "Epoch 1508/2000 train_loss: 1.563005 acc: 0.918400\n",
      "Epoch 1509/2000 train_loss: 1.562993 acc: 0.918500\n",
      "Epoch 1510/2000 train_loss: 1.562981 acc: 0.918500\n",
      "Epoch 1511/2000 train_loss: 1.562970 acc: 0.918500\n",
      "Epoch 1512/2000 train_loss: 1.562957 acc: 0.918600\n",
      "Epoch 1513/2000 train_loss: 1.562945 acc: 0.918500\n",
      "Epoch 1514/2000 train_loss: 1.562933 acc: 0.918400\n",
      "Epoch 1515/2000 train_loss: 1.562921 acc: 0.918500\n",
      "Epoch 1516/2000 train_loss: 1.562909 acc: 0.918500\n",
      "Epoch 1517/2000 train_loss: 1.562897 acc: 0.918600\n",
      "Epoch 1518/2000 train_loss: 1.562885 acc: 0.918400\n",
      "Epoch 1519/2000 train_loss: 1.562873 acc: 0.918600\n",
      "Epoch 1520/2000 train_loss: 1.562861 acc: 0.918600\n",
      "Epoch 1521/2000 train_loss: 1.562850 acc: 0.918700\n",
      "Epoch 1522/2000 train_loss: 1.562838 acc: 0.918600\n",
      "Epoch 1523/2000 train_loss: 1.562826 acc: 0.918600\n",
      "Epoch 1524/2000 train_loss: 1.562813 acc: 0.918600\n",
      "Epoch 1525/2000 train_loss: 1.562802 acc: 0.918700\n",
      "Epoch 1526/2000 train_loss: 1.562790 acc: 0.918600\n",
      "Epoch 1527/2000 train_loss: 1.562778 acc: 0.918600\n",
      "Epoch 1528/2000 train_loss: 1.562767 acc: 0.918600\n",
      "Epoch 1529/2000 train_loss: 1.562755 acc: 0.918600\n",
      "Epoch 1530/2000 train_loss: 1.562743 acc: 0.918700\n",
      "Epoch 1531/2000 train_loss: 1.562731 acc: 0.918700\n",
      "Epoch 1532/2000 train_loss: 1.562720 acc: 0.918600\n",
      "Epoch 1533/2000 train_loss: 1.562707 acc: 0.918600\n",
      "Epoch 1534/2000 train_loss: 1.562696 acc: 0.918600\n",
      "Epoch 1535/2000 train_loss: 1.562684 acc: 0.918600\n",
      "Epoch 1536/2000 train_loss: 1.562673 acc: 0.918600\n",
      "Epoch 1537/2000 train_loss: 1.562661 acc: 0.918600\n",
      "Epoch 1538/2000 train_loss: 1.562649 acc: 0.918600\n",
      "Epoch 1539/2000 train_loss: 1.562638 acc: 0.918600\n",
      "Epoch 1540/2000 train_loss: 1.562625 acc: 0.918600\n",
      "Epoch 1541/2000 train_loss: 1.562614 acc: 0.918700\n",
      "Epoch 1542/2000 train_loss: 1.562603 acc: 0.918600\n",
      "Epoch 1543/2000 train_loss: 1.562591 acc: 0.918600\n",
      "Epoch 1544/2000 train_loss: 1.562579 acc: 0.918600\n",
      "Epoch 1545/2000 train_loss: 1.562568 acc: 0.918700\n",
      "Epoch 1546/2000 train_loss: 1.562556 acc: 0.918700\n",
      "Epoch 1547/2000 train_loss: 1.562544 acc: 0.918700\n",
      "Epoch 1548/2000 train_loss: 1.562533 acc: 0.918700\n",
      "Epoch 1549/2000 train_loss: 1.562521 acc: 0.918700\n",
      "Epoch 1550/2000 train_loss: 1.562510 acc: 0.918600\n",
      "Epoch 1551/2000 train_loss: 1.562498 acc: 0.918700\n",
      "Epoch 1552/2000 train_loss: 1.562486 acc: 0.918700\n",
      "Epoch 1553/2000 train_loss: 1.562475 acc: 0.918700\n",
      "Epoch 1554/2000 train_loss: 1.562464 acc: 0.918700\n",
      "Epoch 1555/2000 train_loss: 1.562452 acc: 0.918600\n",
      "Epoch 1556/2000 train_loss: 1.562440 acc: 0.918600\n",
      "Epoch 1557/2000 train_loss: 1.562429 acc: 0.918700\n",
      "Epoch 1558/2000 train_loss: 1.562417 acc: 0.918700\n",
      "Epoch 1559/2000 train_loss: 1.562406 acc: 0.918800\n",
      "Epoch 1560/2000 train_loss: 1.562394 acc: 0.918700\n",
      "Epoch 1561/2000 train_loss: 1.562383 acc: 0.918700\n",
      "Epoch 1562/2000 train_loss: 1.562371 acc: 0.918700\n",
      "Epoch 1563/2000 train_loss: 1.562360 acc: 0.918700\n",
      "Epoch 1564/2000 train_loss: 1.562348 acc: 0.918700\n",
      "Epoch 1565/2000 train_loss: 1.562337 acc: 0.918800\n",
      "Epoch 1566/2000 train_loss: 1.562325 acc: 0.918700\n",
      "Epoch 1567/2000 train_loss: 1.562314 acc: 0.918800\n",
      "Epoch 1568/2000 train_loss: 1.562303 acc: 0.918700\n",
      "Epoch 1569/2000 train_loss: 1.562291 acc: 0.918700\n",
      "Epoch 1570/2000 train_loss: 1.562280 acc: 0.918900\n",
      "Epoch 1571/2000 train_loss: 1.562269 acc: 0.918700\n",
      "Epoch 1572/2000 train_loss: 1.562257 acc: 0.918800\n",
      "Epoch 1573/2000 train_loss: 1.562246 acc: 0.918600\n",
      "Epoch 1574/2000 train_loss: 1.562235 acc: 0.918700\n",
      "Epoch 1575/2000 train_loss: 1.562223 acc: 0.918700\n",
      "Epoch 1576/2000 train_loss: 1.562212 acc: 0.918600\n",
      "Epoch 1577/2000 train_loss: 1.562201 acc: 0.918600\n",
      "Epoch 1578/2000 train_loss: 1.562190 acc: 0.918600\n",
      "Epoch 1579/2000 train_loss: 1.562178 acc: 0.918600\n",
      "Epoch 1580/2000 train_loss: 1.562168 acc: 0.918600\n",
      "Epoch 1581/2000 train_loss: 1.562156 acc: 0.918600\n",
      "Epoch 1582/2000 train_loss: 1.562145 acc: 0.918600\n",
      "Epoch 1583/2000 train_loss: 1.562133 acc: 0.918700\n",
      "Epoch 1584/2000 train_loss: 1.562122 acc: 0.918600\n",
      "Epoch 1585/2000 train_loss: 1.562111 acc: 0.918900\n",
      "Epoch 1586/2000 train_loss: 1.562100 acc: 0.918800\n",
      "Epoch 1587/2000 train_loss: 1.562089 acc: 0.918900\n",
      "Epoch 1588/2000 train_loss: 1.562077 acc: 0.918800\n",
      "Epoch 1589/2000 train_loss: 1.562066 acc: 0.918800\n",
      "Epoch 1590/2000 train_loss: 1.562055 acc: 0.918800\n",
      "Epoch 1591/2000 train_loss: 1.562044 acc: 0.918800\n",
      "Epoch 1592/2000 train_loss: 1.562033 acc: 0.918900\n",
      "Epoch 1593/2000 train_loss: 1.562022 acc: 0.918700\n",
      "Epoch 1594/2000 train_loss: 1.562011 acc: 0.919000\n",
      "Epoch 1595/2000 train_loss: 1.562000 acc: 0.918900\n",
      "Epoch 1596/2000 train_loss: 1.561988 acc: 0.918800\n",
      "Epoch 1597/2000 train_loss: 1.561977 acc: 0.919000\n",
      "Epoch 1598/2000 train_loss: 1.561966 acc: 0.918900\n",
      "Epoch 1599/2000 train_loss: 1.561955 acc: 0.918900\n",
      "Epoch 1600/2000 train_loss: 1.561944 acc: 0.918900\n",
      "Epoch 1601/2000 train_loss: 1.561933 acc: 0.918800\n",
      "Epoch 1602/2000 train_loss: 1.561922 acc: 0.918900\n",
      "Epoch 1603/2000 train_loss: 1.561911 acc: 0.918900\n",
      "Epoch 1604/2000 train_loss: 1.561900 acc: 0.918800\n",
      "Epoch 1605/2000 train_loss: 1.561889 acc: 0.919000\n",
      "Epoch 1606/2000 train_loss: 1.561878 acc: 0.918900\n",
      "Epoch 1607/2000 train_loss: 1.561867 acc: 0.918900\n",
      "Epoch 1608/2000 train_loss: 1.561856 acc: 0.918900\n",
      "Epoch 1609/2000 train_loss: 1.561845 acc: 0.918900\n",
      "Epoch 1610/2000 train_loss: 1.561834 acc: 0.918900\n",
      "Epoch 1611/2000 train_loss: 1.561823 acc: 0.919000\n",
      "Epoch 1612/2000 train_loss: 1.561812 acc: 0.918900\n",
      "Epoch 1613/2000 train_loss: 1.561801 acc: 0.918900\n",
      "Epoch 1614/2000 train_loss: 1.561790 acc: 0.919000\n",
      "Epoch 1615/2000 train_loss: 1.561779 acc: 0.918800\n",
      "Epoch 1616/2000 train_loss: 1.561769 acc: 0.919000\n",
      "Epoch 1617/2000 train_loss: 1.561757 acc: 0.918900\n",
      "Epoch 1618/2000 train_loss: 1.561747 acc: 0.918900\n",
      "Epoch 1619/2000 train_loss: 1.561736 acc: 0.918800\n",
      "Epoch 1620/2000 train_loss: 1.561725 acc: 0.919000\n",
      "Epoch 1621/2000 train_loss: 1.561714 acc: 0.918900\n",
      "Epoch 1622/2000 train_loss: 1.561703 acc: 0.918900\n",
      "Epoch 1623/2000 train_loss: 1.561693 acc: 0.919100\n",
      "Epoch 1624/2000 train_loss: 1.561682 acc: 0.918900\n",
      "Epoch 1625/2000 train_loss: 1.561671 acc: 0.919000\n",
      "Epoch 1626/2000 train_loss: 1.561661 acc: 0.918900\n",
      "Epoch 1627/2000 train_loss: 1.561649 acc: 0.918700\n",
      "Epoch 1628/2000 train_loss: 1.561638 acc: 0.918700\n",
      "Epoch 1629/2000 train_loss: 1.561628 acc: 0.919000\n",
      "Epoch 1630/2000 train_loss: 1.561617 acc: 0.918600\n",
      "Epoch 1631/2000 train_loss: 1.561606 acc: 0.918900\n",
      "Epoch 1632/2000 train_loss: 1.561595 acc: 0.918800\n",
      "Epoch 1633/2000 train_loss: 1.561584 acc: 0.918900\n",
      "Epoch 1634/2000 train_loss: 1.561574 acc: 0.918900\n",
      "Epoch 1635/2000 train_loss: 1.561563 acc: 0.918800\n",
      "Epoch 1636/2000 train_loss: 1.561553 acc: 0.918900\n",
      "Epoch 1637/2000 train_loss: 1.561542 acc: 0.918800\n",
      "Epoch 1638/2000 train_loss: 1.561531 acc: 0.918900\n",
      "Epoch 1639/2000 train_loss: 1.561520 acc: 0.918800\n",
      "Epoch 1640/2000 train_loss: 1.561510 acc: 0.918900\n",
      "Epoch 1641/2000 train_loss: 1.561499 acc: 0.918900\n",
      "Epoch 1642/2000 train_loss: 1.561488 acc: 0.918900\n",
      "Epoch 1643/2000 train_loss: 1.561478 acc: 0.918900\n",
      "Epoch 1644/2000 train_loss: 1.561467 acc: 0.918800\n",
      "Epoch 1645/2000 train_loss: 1.561456 acc: 0.918800\n",
      "Epoch 1646/2000 train_loss: 1.561445 acc: 0.918700\n",
      "Epoch 1647/2000 train_loss: 1.561435 acc: 0.918700\n",
      "Epoch 1648/2000 train_loss: 1.561425 acc: 0.918900\n",
      "Epoch 1649/2000 train_loss: 1.561414 acc: 0.918800\n",
      "Epoch 1650/2000 train_loss: 1.561404 acc: 0.918800\n",
      "Epoch 1651/2000 train_loss: 1.561393 acc: 0.918700\n",
      "Epoch 1652/2000 train_loss: 1.561382 acc: 0.918800\n",
      "Epoch 1653/2000 train_loss: 1.561372 acc: 0.918900\n",
      "Epoch 1654/2000 train_loss: 1.561361 acc: 0.919100\n",
      "Epoch 1655/2000 train_loss: 1.561351 acc: 0.918800\n",
      "Epoch 1656/2000 train_loss: 1.561340 acc: 0.918900\n",
      "Epoch 1657/2000 train_loss: 1.561330 acc: 0.918800\n",
      "Epoch 1658/2000 train_loss: 1.561319 acc: 0.919000\n",
      "Epoch 1659/2000 train_loss: 1.561309 acc: 0.918900\n",
      "Epoch 1660/2000 train_loss: 1.561298 acc: 0.919000\n",
      "Epoch 1661/2000 train_loss: 1.561288 acc: 0.918900\n",
      "Epoch 1662/2000 train_loss: 1.561277 acc: 0.919200\n",
      "Epoch 1663/2000 train_loss: 1.561267 acc: 0.919100\n",
      "Epoch 1664/2000 train_loss: 1.561256 acc: 0.918900\n",
      "Epoch 1665/2000 train_loss: 1.561245 acc: 0.918900\n",
      "Epoch 1666/2000 train_loss: 1.561236 acc: 0.919000\n",
      "Epoch 1667/2000 train_loss: 1.561225 acc: 0.919100\n",
      "Epoch 1668/2000 train_loss: 1.561215 acc: 0.919100\n",
      "Epoch 1669/2000 train_loss: 1.561204 acc: 0.919100\n",
      "Epoch 1670/2000 train_loss: 1.561194 acc: 0.919100\n",
      "Epoch 1671/2000 train_loss: 1.561183 acc: 0.919100\n",
      "Epoch 1672/2000 train_loss: 1.561173 acc: 0.919000\n",
      "Epoch 1673/2000 train_loss: 1.561163 acc: 0.919100\n",
      "Epoch 1674/2000 train_loss: 1.561152 acc: 0.919100\n",
      "Epoch 1675/2000 train_loss: 1.561142 acc: 0.919100\n",
      "Epoch 1676/2000 train_loss: 1.561131 acc: 0.919200\n",
      "Epoch 1677/2000 train_loss: 1.561121 acc: 0.919100\n",
      "Epoch 1678/2000 train_loss: 1.561110 acc: 0.919200\n",
      "Epoch 1679/2000 train_loss: 1.561100 acc: 0.919100\n",
      "Epoch 1680/2000 train_loss: 1.561090 acc: 0.919300\n",
      "Epoch 1681/2000 train_loss: 1.561080 acc: 0.919100\n",
      "Epoch 1682/2000 train_loss: 1.561069 acc: 0.919200\n",
      "Epoch 1683/2000 train_loss: 1.561059 acc: 0.919200\n",
      "Epoch 1684/2000 train_loss: 1.561049 acc: 0.919200\n",
      "Epoch 1685/2000 train_loss: 1.561039 acc: 0.919200\n",
      "Epoch 1686/2000 train_loss: 1.561029 acc: 0.919200\n",
      "Epoch 1687/2000 train_loss: 1.561018 acc: 0.919300\n",
      "Epoch 1688/2000 train_loss: 1.561008 acc: 0.919200\n",
      "Epoch 1689/2000 train_loss: 1.560998 acc: 0.919300\n",
      "Epoch 1690/2000 train_loss: 1.560987 acc: 0.919100\n",
      "Epoch 1691/2000 train_loss: 1.560977 acc: 0.919200\n",
      "Epoch 1692/2000 train_loss: 1.560967 acc: 0.919200\n",
      "Epoch 1693/2000 train_loss: 1.560957 acc: 0.919500\n",
      "Epoch 1694/2000 train_loss: 1.560947 acc: 0.919300\n",
      "Epoch 1695/2000 train_loss: 1.560936 acc: 0.919400\n",
      "Epoch 1696/2000 train_loss: 1.560927 acc: 0.919300\n",
      "Epoch 1697/2000 train_loss: 1.560916 acc: 0.919400\n",
      "Epoch 1698/2000 train_loss: 1.560906 acc: 0.919300\n",
      "Epoch 1699/2000 train_loss: 1.560896 acc: 0.919200\n",
      "Epoch 1700/2000 train_loss: 1.560886 acc: 0.919300\n",
      "Epoch 1701/2000 train_loss: 1.560876 acc: 0.919500\n",
      "Epoch 1702/2000 train_loss: 1.560866 acc: 0.919400\n",
      "Epoch 1703/2000 train_loss: 1.560855 acc: 0.919400\n",
      "Epoch 1704/2000 train_loss: 1.560845 acc: 0.919400\n",
      "Epoch 1705/2000 train_loss: 1.560835 acc: 0.919400\n",
      "Epoch 1706/2000 train_loss: 1.560825 acc: 0.919400\n",
      "Epoch 1707/2000 train_loss: 1.560815 acc: 0.919300\n",
      "Epoch 1708/2000 train_loss: 1.560804 acc: 0.919400\n",
      "Epoch 1709/2000 train_loss: 1.560795 acc: 0.919300\n",
      "Epoch 1710/2000 train_loss: 1.560784 acc: 0.919400\n",
      "Epoch 1711/2000 train_loss: 1.560774 acc: 0.919200\n",
      "Epoch 1712/2000 train_loss: 1.560765 acc: 0.919300\n",
      "Epoch 1713/2000 train_loss: 1.560755 acc: 0.919200\n",
      "Epoch 1714/2000 train_loss: 1.560745 acc: 0.919400\n",
      "Epoch 1715/2000 train_loss: 1.560734 acc: 0.919300\n",
      "Epoch 1716/2000 train_loss: 1.560724 acc: 0.919400\n",
      "Epoch 1717/2000 train_loss: 1.560715 acc: 0.919400\n",
      "Epoch 1718/2000 train_loss: 1.560704 acc: 0.919300\n",
      "Epoch 1719/2000 train_loss: 1.560695 acc: 0.919300\n",
      "Epoch 1720/2000 train_loss: 1.560685 acc: 0.919200\n",
      "Epoch 1721/2000 train_loss: 1.560675 acc: 0.919300\n",
      "Epoch 1722/2000 train_loss: 1.560665 acc: 0.919300\n",
      "Epoch 1723/2000 train_loss: 1.560655 acc: 0.919300\n",
      "Epoch 1724/2000 train_loss: 1.560645 acc: 0.919300\n",
      "Epoch 1725/2000 train_loss: 1.560635 acc: 0.919200\n",
      "Epoch 1726/2000 train_loss: 1.560624 acc: 0.919200\n",
      "Epoch 1727/2000 train_loss: 1.560615 acc: 0.919200\n",
      "Epoch 1728/2000 train_loss: 1.560605 acc: 0.919300\n",
      "Epoch 1729/2000 train_loss: 1.560595 acc: 0.919300\n",
      "Epoch 1730/2000 train_loss: 1.560585 acc: 0.919200\n",
      "Epoch 1731/2000 train_loss: 1.560576 acc: 0.919300\n",
      "Epoch 1732/2000 train_loss: 1.560565 acc: 0.919200\n",
      "Epoch 1733/2000 train_loss: 1.560556 acc: 0.919200\n",
      "Epoch 1734/2000 train_loss: 1.560546 acc: 0.919300\n",
      "Epoch 1735/2000 train_loss: 1.560535 acc: 0.919200\n",
      "Epoch 1736/2000 train_loss: 1.560526 acc: 0.919200\n",
      "Epoch 1737/2000 train_loss: 1.560516 acc: 0.919200\n",
      "Epoch 1738/2000 train_loss: 1.560506 acc: 0.919200\n",
      "Epoch 1739/2000 train_loss: 1.560496 acc: 0.919200\n",
      "Epoch 1740/2000 train_loss: 1.560486 acc: 0.919200\n",
      "Epoch 1741/2000 train_loss: 1.560477 acc: 0.919200\n",
      "Epoch 1742/2000 train_loss: 1.560467 acc: 0.919300\n",
      "Epoch 1743/2000 train_loss: 1.560457 acc: 0.919300\n",
      "Epoch 1744/2000 train_loss: 1.560448 acc: 0.919200\n",
      "Epoch 1745/2000 train_loss: 1.560438 acc: 0.919300\n",
      "Epoch 1746/2000 train_loss: 1.560428 acc: 0.919300\n",
      "Epoch 1747/2000 train_loss: 1.560418 acc: 0.919300\n",
      "Epoch 1748/2000 train_loss: 1.560408 acc: 0.919300\n",
      "Epoch 1749/2000 train_loss: 1.560399 acc: 0.919300\n",
      "Epoch 1750/2000 train_loss: 1.560389 acc: 0.919400\n",
      "Epoch 1751/2000 train_loss: 1.560379 acc: 0.919300\n",
      "Epoch 1752/2000 train_loss: 1.560369 acc: 0.919300\n",
      "Epoch 1753/2000 train_loss: 1.560360 acc: 0.919300\n",
      "Epoch 1754/2000 train_loss: 1.560350 acc: 0.919400\n",
      "Epoch 1755/2000 train_loss: 1.560340 acc: 0.919400\n",
      "Epoch 1756/2000 train_loss: 1.560331 acc: 0.919300\n",
      "Epoch 1757/2000 train_loss: 1.560322 acc: 0.919600\n",
      "Epoch 1758/2000 train_loss: 1.560311 acc: 0.919400\n",
      "Epoch 1759/2000 train_loss: 1.560302 acc: 0.919500\n",
      "Epoch 1760/2000 train_loss: 1.560292 acc: 0.919500\n",
      "Epoch 1761/2000 train_loss: 1.560283 acc: 0.919600\n",
      "Epoch 1762/2000 train_loss: 1.560273 acc: 0.919500\n",
      "Epoch 1763/2000 train_loss: 1.560263 acc: 0.919400\n",
      "Epoch 1764/2000 train_loss: 1.560254 acc: 0.919400\n",
      "Epoch 1765/2000 train_loss: 1.560244 acc: 0.919500\n",
      "Epoch 1766/2000 train_loss: 1.560234 acc: 0.919600\n",
      "Epoch 1767/2000 train_loss: 1.560225 acc: 0.919500\n",
      "Epoch 1768/2000 train_loss: 1.560215 acc: 0.919500\n",
      "Epoch 1769/2000 train_loss: 1.560206 acc: 0.919600\n",
      "Epoch 1770/2000 train_loss: 1.560196 acc: 0.919600\n",
      "Epoch 1771/2000 train_loss: 1.560187 acc: 0.919600\n",
      "Epoch 1772/2000 train_loss: 1.560177 acc: 0.919600\n",
      "Epoch 1773/2000 train_loss: 1.560167 acc: 0.919600\n",
      "Epoch 1774/2000 train_loss: 1.560158 acc: 0.919600\n",
      "Epoch 1775/2000 train_loss: 1.560148 acc: 0.919500\n",
      "Epoch 1776/2000 train_loss: 1.560139 acc: 0.919500\n",
      "Epoch 1777/2000 train_loss: 1.560129 acc: 0.919600\n",
      "Epoch 1778/2000 train_loss: 1.560120 acc: 0.919500\n",
      "Epoch 1779/2000 train_loss: 1.560110 acc: 0.919600\n",
      "Epoch 1780/2000 train_loss: 1.560100 acc: 0.919600\n",
      "Epoch 1781/2000 train_loss: 1.560091 acc: 0.919600\n",
      "Epoch 1782/2000 train_loss: 1.560081 acc: 0.919700\n",
      "Epoch 1783/2000 train_loss: 1.560072 acc: 0.919600\n",
      "Epoch 1784/2000 train_loss: 1.560062 acc: 0.919600\n",
      "Epoch 1785/2000 train_loss: 1.560053 acc: 0.919700\n",
      "Epoch 1786/2000 train_loss: 1.560044 acc: 0.919700\n",
      "Epoch 1787/2000 train_loss: 1.560034 acc: 0.919700\n",
      "Epoch 1788/2000 train_loss: 1.560025 acc: 0.919700\n",
      "Epoch 1789/2000 train_loss: 1.560015 acc: 0.919700\n",
      "Epoch 1790/2000 train_loss: 1.560006 acc: 0.919600\n",
      "Epoch 1791/2000 train_loss: 1.559997 acc: 0.919700\n",
      "Epoch 1792/2000 train_loss: 1.559988 acc: 0.919600\n",
      "Epoch 1793/2000 train_loss: 1.559978 acc: 0.919700\n",
      "Epoch 1794/2000 train_loss: 1.559968 acc: 0.919700\n",
      "Epoch 1795/2000 train_loss: 1.559959 acc: 0.919600\n",
      "Epoch 1796/2000 train_loss: 1.559949 acc: 0.919700\n",
      "Epoch 1797/2000 train_loss: 1.559940 acc: 0.919800\n",
      "Epoch 1798/2000 train_loss: 1.559931 acc: 0.919700\n",
      "Epoch 1799/2000 train_loss: 1.559921 acc: 0.919700\n",
      "Epoch 1800/2000 train_loss: 1.559912 acc: 0.919600\n",
      "Epoch 1801/2000 train_loss: 1.559903 acc: 0.919600\n",
      "Epoch 1802/2000 train_loss: 1.559893 acc: 0.919600\n",
      "Epoch 1803/2000 train_loss: 1.559884 acc: 0.919700\n",
      "Epoch 1804/2000 train_loss: 1.559874 acc: 0.919800\n",
      "Epoch 1805/2000 train_loss: 1.559866 acc: 0.919700\n",
      "Epoch 1806/2000 train_loss: 1.559856 acc: 0.919600\n",
      "Epoch 1807/2000 train_loss: 1.559847 acc: 0.919700\n",
      "Epoch 1808/2000 train_loss: 1.559837 acc: 0.919700\n",
      "Epoch 1809/2000 train_loss: 1.559828 acc: 0.919800\n",
      "Epoch 1810/2000 train_loss: 1.559819 acc: 0.919600\n",
      "Epoch 1811/2000 train_loss: 1.559810 acc: 0.919600\n",
      "Epoch 1812/2000 train_loss: 1.559800 acc: 0.919600\n",
      "Epoch 1813/2000 train_loss: 1.559791 acc: 0.919600\n",
      "Epoch 1814/2000 train_loss: 1.559782 acc: 0.919600\n",
      "Epoch 1815/2000 train_loss: 1.559773 acc: 0.919700\n",
      "Epoch 1816/2000 train_loss: 1.559763 acc: 0.919700\n",
      "Epoch 1817/2000 train_loss: 1.559754 acc: 0.919700\n",
      "Epoch 1818/2000 train_loss: 1.559745 acc: 0.919600\n",
      "Epoch 1819/2000 train_loss: 1.559736 acc: 0.919800\n",
      "Epoch 1820/2000 train_loss: 1.559727 acc: 0.919800\n",
      "Epoch 1821/2000 train_loss: 1.559717 acc: 0.919700\n",
      "Epoch 1822/2000 train_loss: 1.559708 acc: 0.919800\n",
      "Epoch 1823/2000 train_loss: 1.559699 acc: 0.919900\n",
      "Epoch 1824/2000 train_loss: 1.559690 acc: 0.919700\n",
      "Epoch 1825/2000 train_loss: 1.559680 acc: 0.919800\n",
      "Epoch 1826/2000 train_loss: 1.559672 acc: 0.919800\n",
      "Epoch 1827/2000 train_loss: 1.559662 acc: 0.919800\n",
      "Epoch 1828/2000 train_loss: 1.559653 acc: 0.919800\n",
      "Epoch 1829/2000 train_loss: 1.559644 acc: 0.919800\n",
      "Epoch 1830/2000 train_loss: 1.559635 acc: 0.919900\n",
      "Epoch 1831/2000 train_loss: 1.559626 acc: 0.919900\n",
      "Epoch 1832/2000 train_loss: 1.559616 acc: 0.919900\n",
      "Epoch 1833/2000 train_loss: 1.559607 acc: 0.919900\n",
      "Epoch 1834/2000 train_loss: 1.559598 acc: 0.919800\n",
      "Epoch 1835/2000 train_loss: 1.559589 acc: 0.919800\n",
      "Epoch 1836/2000 train_loss: 1.559580 acc: 0.919800\n",
      "Epoch 1837/2000 train_loss: 1.559571 acc: 0.919800\n",
      "Epoch 1838/2000 train_loss: 1.559562 acc: 0.919800\n",
      "Epoch 1839/2000 train_loss: 1.559553 acc: 0.919800\n",
      "Epoch 1840/2000 train_loss: 1.559543 acc: 0.919900\n",
      "Epoch 1841/2000 train_loss: 1.559535 acc: 0.919800\n",
      "Epoch 1842/2000 train_loss: 1.559526 acc: 0.919800\n",
      "Epoch 1843/2000 train_loss: 1.559516 acc: 0.919800\n",
      "Epoch 1844/2000 train_loss: 1.559508 acc: 0.919900\n",
      "Epoch 1845/2000 train_loss: 1.559498 acc: 0.920000\n",
      "Epoch 1846/2000 train_loss: 1.559489 acc: 0.919900\n",
      "Epoch 1847/2000 train_loss: 1.559480 acc: 0.919900\n",
      "Epoch 1848/2000 train_loss: 1.559472 acc: 0.919800\n",
      "Epoch 1849/2000 train_loss: 1.559463 acc: 0.919800\n",
      "Epoch 1850/2000 train_loss: 1.559453 acc: 0.919900\n",
      "Epoch 1851/2000 train_loss: 1.559445 acc: 0.919900\n",
      "Epoch 1852/2000 train_loss: 1.559435 acc: 0.919800\n",
      "Epoch 1853/2000 train_loss: 1.559426 acc: 0.919900\n",
      "Epoch 1854/2000 train_loss: 1.559417 acc: 0.919900\n",
      "Epoch 1855/2000 train_loss: 1.559408 acc: 0.919900\n",
      "Epoch 1856/2000 train_loss: 1.559399 acc: 0.920000\n",
      "Epoch 1857/2000 train_loss: 1.559391 acc: 0.920000\n",
      "Epoch 1858/2000 train_loss: 1.559381 acc: 0.919900\n",
      "Epoch 1859/2000 train_loss: 1.559373 acc: 0.920000\n",
      "Epoch 1860/2000 train_loss: 1.559364 acc: 0.920100\n",
      "Epoch 1861/2000 train_loss: 1.559355 acc: 0.920000\n",
      "Epoch 1862/2000 train_loss: 1.559346 acc: 0.919900\n",
      "Epoch 1863/2000 train_loss: 1.559337 acc: 0.919900\n",
      "Epoch 1864/2000 train_loss: 1.559328 acc: 0.919900\n",
      "Epoch 1865/2000 train_loss: 1.559319 acc: 0.920000\n",
      "Epoch 1866/2000 train_loss: 1.559310 acc: 0.920000\n",
      "Epoch 1867/2000 train_loss: 1.559301 acc: 0.920000\n",
      "Epoch 1868/2000 train_loss: 1.559292 acc: 0.920000\n",
      "Epoch 1869/2000 train_loss: 1.559283 acc: 0.919900\n",
      "Epoch 1870/2000 train_loss: 1.559275 acc: 0.920100\n",
      "Epoch 1871/2000 train_loss: 1.559265 acc: 0.919900\n",
      "Epoch 1872/2000 train_loss: 1.559257 acc: 0.920000\n",
      "Epoch 1873/2000 train_loss: 1.559248 acc: 0.920100\n",
      "Epoch 1874/2000 train_loss: 1.559239 acc: 0.920000\n",
      "Epoch 1875/2000 train_loss: 1.559230 acc: 0.920100\n",
      "Epoch 1876/2000 train_loss: 1.559222 acc: 0.920000\n",
      "Epoch 1877/2000 train_loss: 1.559213 acc: 0.920000\n",
      "Epoch 1878/2000 train_loss: 1.559204 acc: 0.919900\n",
      "Epoch 1879/2000 train_loss: 1.559195 acc: 0.920000\n",
      "Epoch 1880/2000 train_loss: 1.559186 acc: 0.920000\n",
      "Epoch 1881/2000 train_loss: 1.559177 acc: 0.920100\n",
      "Epoch 1882/2000 train_loss: 1.559169 acc: 0.920000\n",
      "Epoch 1883/2000 train_loss: 1.559160 acc: 0.920000\n",
      "Epoch 1884/2000 train_loss: 1.559151 acc: 0.920000\n",
      "Epoch 1885/2000 train_loss: 1.559142 acc: 0.920100\n",
      "Epoch 1886/2000 train_loss: 1.559134 acc: 0.920000\n",
      "Epoch 1887/2000 train_loss: 1.559124 acc: 0.920200\n",
      "Epoch 1888/2000 train_loss: 1.559116 acc: 0.920100\n",
      "Epoch 1889/2000 train_loss: 1.559108 acc: 0.920100\n",
      "Epoch 1890/2000 train_loss: 1.559099 acc: 0.920000\n",
      "Epoch 1891/2000 train_loss: 1.559090 acc: 0.920100\n",
      "Epoch 1892/2000 train_loss: 1.559081 acc: 0.920100\n",
      "Epoch 1893/2000 train_loss: 1.559073 acc: 0.920000\n",
      "Epoch 1894/2000 train_loss: 1.559064 acc: 0.920100\n",
      "Epoch 1895/2000 train_loss: 1.559055 acc: 0.920100\n",
      "Epoch 1896/2000 train_loss: 1.559047 acc: 0.920000\n",
      "Epoch 1897/2000 train_loss: 1.559038 acc: 0.920000\n",
      "Epoch 1898/2000 train_loss: 1.559029 acc: 0.920200\n",
      "Epoch 1899/2000 train_loss: 1.559020 acc: 0.920100\n",
      "Epoch 1900/2000 train_loss: 1.559012 acc: 0.920200\n",
      "Epoch 1901/2000 train_loss: 1.559003 acc: 0.920100\n",
      "Epoch 1902/2000 train_loss: 1.558994 acc: 0.920400\n",
      "Epoch 1903/2000 train_loss: 1.558985 acc: 0.920200\n",
      "Epoch 1904/2000 train_loss: 1.558977 acc: 0.920300\n",
      "Epoch 1905/2000 train_loss: 1.558968 acc: 0.920100\n",
      "Epoch 1906/2000 train_loss: 1.558959 acc: 0.920100\n",
      "Epoch 1907/2000 train_loss: 1.558951 acc: 0.920100\n",
      "Epoch 1908/2000 train_loss: 1.558943 acc: 0.920400\n",
      "Epoch 1909/2000 train_loss: 1.558934 acc: 0.920100\n",
      "Epoch 1910/2000 train_loss: 1.558925 acc: 0.920300\n",
      "Epoch 1911/2000 train_loss: 1.558916 acc: 0.920200\n",
      "Epoch 1912/2000 train_loss: 1.558908 acc: 0.920200\n",
      "Epoch 1913/2000 train_loss: 1.558900 acc: 0.920200\n",
      "Epoch 1914/2000 train_loss: 1.558891 acc: 0.920200\n",
      "Epoch 1915/2000 train_loss: 1.558882 acc: 0.920100\n",
      "Epoch 1916/2000 train_loss: 1.558874 acc: 0.920300\n",
      "Epoch 1917/2000 train_loss: 1.558865 acc: 0.920300\n",
      "Epoch 1918/2000 train_loss: 1.558857 acc: 0.920300\n",
      "Epoch 1919/2000 train_loss: 1.558848 acc: 0.920200\n",
      "Epoch 1920/2000 train_loss: 1.558840 acc: 0.920100\n",
      "Epoch 1921/2000 train_loss: 1.558830 acc: 0.920100\n",
      "Epoch 1922/2000 train_loss: 1.558822 acc: 0.920100\n",
      "Epoch 1923/2000 train_loss: 1.558814 acc: 0.920300\n",
      "Epoch 1924/2000 train_loss: 1.558805 acc: 0.920100\n",
      "Epoch 1925/2000 train_loss: 1.558797 acc: 0.920100\n",
      "Epoch 1926/2000 train_loss: 1.558788 acc: 0.920100\n",
      "Epoch 1927/2000 train_loss: 1.558779 acc: 0.920300\n",
      "Epoch 1928/2000 train_loss: 1.558771 acc: 0.920300\n",
      "Epoch 1929/2000 train_loss: 1.558763 acc: 0.920200\n",
      "Epoch 1930/2000 train_loss: 1.558755 acc: 0.920300\n",
      "Epoch 1931/2000 train_loss: 1.558745 acc: 0.920100\n",
      "Epoch 1932/2000 train_loss: 1.558738 acc: 0.920400\n",
      "Epoch 1933/2000 train_loss: 1.558729 acc: 0.920200\n",
      "Epoch 1934/2000 train_loss: 1.558720 acc: 0.920100\n",
      "Epoch 1935/2000 train_loss: 1.558712 acc: 0.920300\n",
      "Epoch 1936/2000 train_loss: 1.558704 acc: 0.920300\n",
      "Epoch 1937/2000 train_loss: 1.558695 acc: 0.920200\n",
      "Epoch 1938/2000 train_loss: 1.558686 acc: 0.920300\n",
      "Epoch 1939/2000 train_loss: 1.558678 acc: 0.920300\n",
      "Epoch 1940/2000 train_loss: 1.558670 acc: 0.920300\n",
      "Epoch 1941/2000 train_loss: 1.558661 acc: 0.920200\n",
      "Epoch 1942/2000 train_loss: 1.558653 acc: 0.920300\n",
      "Epoch 1943/2000 train_loss: 1.558645 acc: 0.920400\n",
      "Epoch 1944/2000 train_loss: 1.558636 acc: 0.920400\n",
      "Epoch 1945/2000 train_loss: 1.558627 acc: 0.920300\n",
      "Epoch 1946/2000 train_loss: 1.558619 acc: 0.920200\n",
      "Epoch 1947/2000 train_loss: 1.558611 acc: 0.920400\n",
      "Epoch 1948/2000 train_loss: 1.558603 acc: 0.920400\n",
      "Epoch 1949/2000 train_loss: 1.558594 acc: 0.920300\n",
      "Epoch 1950/2000 train_loss: 1.558586 acc: 0.920200\n",
      "Epoch 1951/2000 train_loss: 1.558578 acc: 0.920300\n",
      "Epoch 1952/2000 train_loss: 1.558569 acc: 0.920400\n",
      "Epoch 1953/2000 train_loss: 1.558561 acc: 0.920300\n",
      "Epoch 1954/2000 train_loss: 1.558552 acc: 0.920500\n",
      "Epoch 1955/2000 train_loss: 1.558544 acc: 0.920300\n",
      "Epoch 1956/2000 train_loss: 1.558535 acc: 0.920400\n",
      "Epoch 1957/2000 train_loss: 1.558527 acc: 0.920400\n",
      "Epoch 1958/2000 train_loss: 1.558519 acc: 0.920400\n",
      "Epoch 1959/2000 train_loss: 1.558510 acc: 0.920400\n",
      "Epoch 1960/2000 train_loss: 1.558502 acc: 0.920300\n",
      "Epoch 1961/2000 train_loss: 1.558494 acc: 0.920400\n",
      "Epoch 1962/2000 train_loss: 1.558486 acc: 0.920400\n",
      "Epoch 1963/2000 train_loss: 1.558477 acc: 0.920400\n",
      "Epoch 1964/2000 train_loss: 1.558469 acc: 0.920400\n",
      "Epoch 1965/2000 train_loss: 1.558461 acc: 0.920300\n",
      "Epoch 1966/2000 train_loss: 1.558453 acc: 0.920300\n",
      "Epoch 1967/2000 train_loss: 1.558445 acc: 0.920300\n",
      "Epoch 1968/2000 train_loss: 1.558436 acc: 0.920400\n",
      "Epoch 1969/2000 train_loss: 1.558428 acc: 0.920400\n",
      "Epoch 1970/2000 train_loss: 1.558420 acc: 0.920500\n",
      "Epoch 1971/2000 train_loss: 1.558411 acc: 0.920500\n",
      "Epoch 1972/2000 train_loss: 1.558403 acc: 0.920400\n",
      "Epoch 1973/2000 train_loss: 1.558395 acc: 0.920500\n",
      "Epoch 1974/2000 train_loss: 1.558387 acc: 0.920600\n",
      "Epoch 1975/2000 train_loss: 1.558378 acc: 0.920600\n",
      "Epoch 1976/2000 train_loss: 1.558370 acc: 0.920300\n",
      "Epoch 1977/2000 train_loss: 1.558362 acc: 0.920700\n",
      "Epoch 1978/2000 train_loss: 1.558354 acc: 0.920500\n",
      "Epoch 1979/2000 train_loss: 1.558346 acc: 0.920600\n",
      "Epoch 1980/2000 train_loss: 1.558337 acc: 0.920600\n",
      "Epoch 1981/2000 train_loss: 1.558329 acc: 0.920400\n",
      "Epoch 1982/2000 train_loss: 1.558321 acc: 0.920600\n",
      "Epoch 1983/2000 train_loss: 1.558313 acc: 0.920700\n",
      "Epoch 1984/2000 train_loss: 1.558305 acc: 0.920600\n",
      "Epoch 1985/2000 train_loss: 1.558297 acc: 0.920600\n",
      "Epoch 1986/2000 train_loss: 1.558288 acc: 0.920700\n",
      "Epoch 1987/2000 train_loss: 1.558280 acc: 0.920600\n",
      "Epoch 1988/2000 train_loss: 1.558273 acc: 0.920700\n",
      "Epoch 1989/2000 train_loss: 1.558264 acc: 0.920600\n",
      "Epoch 1990/2000 train_loss: 1.558256 acc: 0.920700\n",
      "Epoch 1991/2000 train_loss: 1.558248 acc: 0.920600\n",
      "Epoch 1992/2000 train_loss: 1.558240 acc: 0.920600\n",
      "Epoch 1993/2000 train_loss: 1.558232 acc: 0.920600\n",
      "Epoch 1994/2000 train_loss: 1.558223 acc: 0.920700\n",
      "Epoch 1995/2000 train_loss: 1.558215 acc: 0.920600\n",
      "Epoch 1996/2000 train_loss: 1.558207 acc: 0.920700\n",
      "Epoch 1997/2000 train_loss: 1.558199 acc: 0.920600\n",
      "Epoch 1998/2000 train_loss: 1.558191 acc: 0.920800\n",
      "Epoch 1999/2000 train_loss: 1.558183 acc: 0.920700\n",
      "Epoch 2000/2000 train_loss: 1.558175 acc: 0.920700\n",
      "Epoch 2001/2000 train_loss: 1.558167 acc: 0.920800\n"
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "\n",
    "for epoch in range(1,epochs+1):\n",
    "\n",
    "    loss_sum = 0.0\n",
    "\n",
    "    metric = MulticlassAccuracy()\n",
    "\n",
    "    for (train_features,train_labels),(test_features,test_labels) in zip(train_dataloader,test_dataloader) :\n",
    "\n",
    "        epoch_y = net(train_features)\n",
    "        loss = loss_fc(epoch_y.to(torch.float32),train_labels.to(torch.float32))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 100 == 0 :\n",
    "            for m in net.modules():\n",
    "                if isinstance(m,nn.Linear):\n",
    "                    \n",
    "                    img_tensor = m.weight.clone().detach()\n",
    "                    img = resize_tensor(img_tensor.squeeze())\n",
    "                    \n",
    "                    img = img.reshape(10,28,28)\n",
    "                    np_arr = np.array(img,dtype=np.uint8)\n",
    "                                        \n",
    "                    for img_num ,i in zip(np_arr,range(len(np_arr))):\n",
    "                        writer.add_image(f'weight_to_img_{i}',img_num,epoch + 1,dataformats='WH')\n",
    "\n",
    "        loss_sum += loss\n",
    "        \n",
    "        epoch_test_y = net(test_features)\n",
    "        result_set = []\n",
    "        gt_set = []\n",
    "        \n",
    "        for result , gt in zip(epoch_test_y, test_labels):\n",
    "            \n",
    "            pre_max = 0\n",
    "            temp = -1\n",
    "            \n",
    "            for pre_data,i in zip(result,range(len(result))):\n",
    "                \n",
    "                if pre_data > pre_max:\n",
    "                    temp = i\n",
    "                    pre_max = pre_data\n",
    "            \n",
    "            if temp != -1:\n",
    "                result_set.append(temp)\n",
    "            \n",
    "            for gt_data,i in zip(gt,range(len(gt))):\n",
    "                \n",
    "                if gt_data == 1:\n",
    "                    gt_set.append(i)\n",
    "                \n",
    "        gt_set = np.array(gt_set)\n",
    "        gt_set = torch.tensor(gt_set)\n",
    "        \n",
    "        result_set = np.array(result_set)\n",
    "        result_set = torch.tensor(result_set)\n",
    "        metric.update(result_set,gt_set)\n",
    "        \n",
    "    acc = metric.compute()\n",
    "\n",
    "    loss = loss_sum / len(train_dataloader)\n",
    "\n",
    "    print('Epoch {:4d}/{} train_loss: {:.6f} acc: {:.6f}'.format(\n",
    "        epoch+1, epochs, loss, acc\n",
    "    ))\n",
    "\n",
    "    if loss < 0.1 :\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([test_dataset[40][0]])\n",
    "\n",
    "pre_y = net(x)\n",
    "\n",
    "pre_y = pre_y.squeeze()\n",
    "\n",
    "pre_max = 0\n",
    "temp = -1\n",
    "\n",
    "for pre_data,i in zip(pre_y,range(len(pre_y))):\n",
    "    \n",
    "    if pre_data > pre_max:\n",
    "        temp = i\n",
    "        pre_max = pre_data\n",
    "temp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.imshow(x.squeeze().reshape(28,28).numpy(),cmap='gray')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
