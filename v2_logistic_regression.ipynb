{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torcheval.metrics import BinaryAccuracy\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "data_train = datasets.MNIST(root=\"./dataset\",train=True,download=True,transform=transforms.ToTensor())\n",
    "data_test = datasets.MNIST(root=\"./dataset\",train=False,download=True,transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "image, label = data_train[0]\n",
    "\n",
    "plt.imshow(image.squeeze().numpy(),cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "class UnkownNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UnkownNet,self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784,1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,input):\n",
    "\n",
    "        f2 = self.fc1(input)\n",
    "\n",
    "        output = self.activation(f2)\n",
    "\n",
    "        return output\n",
    "    \n",
    "net = UnkownNet()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(),lr=0.01)\n",
    "loss_func = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = []\n",
    "\n",
    "for x , y in data_train:\n",
    "\n",
    "    if y % 2 != 0:\n",
    "        y = np.array([1])\n",
    "    else :\n",
    "        y = np.array([0])\n",
    "\n",
    "    x = x.squeeze().reshape(-1).numpy()\n",
    "\n",
    "    train_dataset.append([x,y])\n",
    "\n",
    "test_dataset = []\n",
    "\n",
    "for x , y in data_test:\n",
    "\n",
    "    if y % 2 != 0:\n",
    "        y = np.array([1])\n",
    "    else :\n",
    "        y = np.array([0])\n",
    "\n",
    "    x = x.squeeze().reshape(-1).numpy()\n",
    "\n",
    "    test_dataset.append([x,y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import dataset, DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=600,shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=100,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataloader))\n",
    "print(len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_tensor(tensor):\n",
    "    \n",
    "    v_min, v_max = tensor.min(), tensor.max()\n",
    "    \n",
    "    new_min,new_max = 0,255\n",
    "    \n",
    "    v_p = (tensor - v_min)/(v_max - v_min)*(new_max - new_min) + new_min\n",
    "    \n",
    "    return v_p\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/2000 train_loss: 0.572301 acc: 0.773500\n",
      "Epoch    2/2000 train_loss: 0.464542 acc: 0.822300\n",
      "Epoch    3/2000 train_loss: 0.421023 acc: 0.833300\n",
      "Epoch    4/2000 train_loss: 0.396956 acc: 0.839300\n",
      "Epoch    5/2000 train_loss: 0.381363 acc: 0.843900\n",
      "Epoch    6/2000 train_loss: 0.370320 acc: 0.847900\n",
      "Epoch    7/2000 train_loss: 0.362026 acc: 0.850800\n",
      "Epoch    8/2000 train_loss: 0.355464 acc: 0.852800\n",
      "Epoch    9/2000 train_loss: 0.350148 acc: 0.855900\n",
      "Epoch   10/2000 train_loss: 0.345675 acc: 0.857900\n",
      "Epoch   11/2000 train_loss: 0.341861 acc: 0.860500\n",
      "Epoch   12/2000 train_loss: 0.338552 acc: 0.861400\n",
      "Epoch   13/2000 train_loss: 0.335652 acc: 0.862500\n",
      "Epoch   14/2000 train_loss: 0.333063 acc: 0.862900\n",
      "Epoch   15/2000 train_loss: 0.330709 acc: 0.864200\n",
      "Epoch   16/2000 train_loss: 0.328586 acc: 0.865700\n",
      "Epoch   17/2000 train_loss: 0.326655 acc: 0.866500\n",
      "Epoch   18/2000 train_loss: 0.324850 acc: 0.867800\n",
      "Epoch   19/2000 train_loss: 0.323236 acc: 0.868100\n",
      "Epoch   20/2000 train_loss: 0.321688 acc: 0.869000\n",
      "Epoch   21/2000 train_loss: 0.320272 acc: 0.870600\n",
      "Epoch   22/2000 train_loss: 0.318927 acc: 0.871000\n",
      "Epoch   23/2000 train_loss: 0.317667 acc: 0.872700\n",
      "Epoch   24/2000 train_loss: 0.316474 acc: 0.873200\n",
      "Epoch   25/2000 train_loss: 0.315361 acc: 0.873500\n",
      "Epoch   26/2000 train_loss: 0.314281 acc: 0.873500\n",
      "Epoch   27/2000 train_loss: 0.313276 acc: 0.873700\n",
      "Epoch   28/2000 train_loss: 0.312320 acc: 0.875300\n",
      "Epoch   29/2000 train_loss: 0.311378 acc: 0.874400\n",
      "Epoch   30/2000 train_loss: 0.310508 acc: 0.875500\n",
      "Epoch   31/2000 train_loss: 0.309664 acc: 0.875900\n",
      "Epoch   32/2000 train_loss: 0.308856 acc: 0.876500\n",
      "Epoch   33/2000 train_loss: 0.308080 acc: 0.876800\n",
      "Epoch   34/2000 train_loss: 0.307302 acc: 0.877200\n",
      "Epoch   35/2000 train_loss: 0.306591 acc: 0.876800\n",
      "Epoch   36/2000 train_loss: 0.305907 acc: 0.878100\n",
      "Epoch   37/2000 train_loss: 0.305233 acc: 0.878100\n",
      "Epoch   38/2000 train_loss: 0.304574 acc: 0.877900\n",
      "Epoch   39/2000 train_loss: 0.303951 acc: 0.878500\n",
      "Epoch   40/2000 train_loss: 0.303346 acc: 0.878600\n",
      "Epoch   41/2000 train_loss: 0.302740 acc: 0.878600\n",
      "Epoch   42/2000 train_loss: 0.302164 acc: 0.879000\n",
      "Epoch   43/2000 train_loss: 0.301619 acc: 0.879600\n",
      "Epoch   44/2000 train_loss: 0.301068 acc: 0.879500\n",
      "Epoch   45/2000 train_loss: 0.300547 acc: 0.879600\n",
      "Epoch   46/2000 train_loss: 0.300022 acc: 0.880100\n",
      "Epoch   47/2000 train_loss: 0.299531 acc: 0.880000\n",
      "Epoch   48/2000 train_loss: 0.299023 acc: 0.880500\n",
      "Epoch   49/2000 train_loss: 0.298565 acc: 0.880300\n",
      "Epoch   50/2000 train_loss: 0.298100 acc: 0.881000\n",
      "Epoch   51/2000 train_loss: 0.297644 acc: 0.881100\n",
      "Epoch   52/2000 train_loss: 0.297210 acc: 0.881600\n",
      "Epoch   53/2000 train_loss: 0.296781 acc: 0.881400\n",
      "Epoch   54/2000 train_loss: 0.296357 acc: 0.882100\n",
      "Epoch   55/2000 train_loss: 0.295929 acc: 0.882400\n",
      "Epoch   56/2000 train_loss: 0.295533 acc: 0.882500\n",
      "Epoch   57/2000 train_loss: 0.295143 acc: 0.883000\n",
      "Epoch   58/2000 train_loss: 0.294748 acc: 0.883100\n",
      "Epoch   59/2000 train_loss: 0.294377 acc: 0.883200\n",
      "Epoch   60/2000 train_loss: 0.294001 acc: 0.883400\n",
      "Epoch   61/2000 train_loss: 0.293634 acc: 0.883900\n",
      "Epoch   62/2000 train_loss: 0.293279 acc: 0.883800\n",
      "Epoch   63/2000 train_loss: 0.292932 acc: 0.884900\n",
      "Epoch   64/2000 train_loss: 0.292594 acc: 0.884700\n",
      "Epoch   65/2000 train_loss: 0.292254 acc: 0.885000\n",
      "Epoch   66/2000 train_loss: 0.291916 acc: 0.885000\n",
      "Epoch   67/2000 train_loss: 0.291589 acc: 0.885700\n",
      "Epoch   68/2000 train_loss: 0.291267 acc: 0.885700\n",
      "Epoch   69/2000 train_loss: 0.290959 acc: 0.885300\n",
      "Epoch   70/2000 train_loss: 0.290651 acc: 0.885800\n",
      "Epoch   71/2000 train_loss: 0.290345 acc: 0.885200\n",
      "Epoch   72/2000 train_loss: 0.290043 acc: 0.886100\n",
      "Epoch   73/2000 train_loss: 0.289754 acc: 0.886300\n",
      "Epoch   74/2000 train_loss: 0.289470 acc: 0.885800\n",
      "Epoch   75/2000 train_loss: 0.289188 acc: 0.886400\n",
      "Epoch   76/2000 train_loss: 0.288894 acc: 0.886200\n",
      "Epoch   77/2000 train_loss: 0.288627 acc: 0.886100\n",
      "Epoch   78/2000 train_loss: 0.288365 acc: 0.886200\n",
      "Epoch   79/2000 train_loss: 0.288108 acc: 0.886100\n",
      "Epoch   80/2000 train_loss: 0.287826 acc: 0.886200\n",
      "Epoch   81/2000 train_loss: 0.287581 acc: 0.886700\n",
      "Epoch   82/2000 train_loss: 0.287332 acc: 0.886400\n",
      "Epoch   83/2000 train_loss: 0.287068 acc: 0.887000\n",
      "Epoch   84/2000 train_loss: 0.286836 acc: 0.886700\n",
      "Epoch   85/2000 train_loss: 0.286580 acc: 0.887000\n",
      "Epoch   86/2000 train_loss: 0.286357 acc: 0.887500\n",
      "Epoch   87/2000 train_loss: 0.286121 acc: 0.887400\n",
      "Epoch   88/2000 train_loss: 0.285880 acc: 0.887000\n",
      "Epoch   89/2000 train_loss: 0.285654 acc: 0.886800\n",
      "Epoch   90/2000 train_loss: 0.285426 acc: 0.886900\n",
      "Epoch   91/2000 train_loss: 0.285211 acc: 0.887300\n",
      "Epoch   92/2000 train_loss: 0.284985 acc: 0.887200\n",
      "Epoch   93/2000 train_loss: 0.284771 acc: 0.887200\n",
      "Epoch   94/2000 train_loss: 0.284544 acc: 0.887500\n",
      "Epoch   95/2000 train_loss: 0.284355 acc: 0.887600\n",
      "Epoch   96/2000 train_loss: 0.284151 acc: 0.887400\n",
      "Epoch   97/2000 train_loss: 0.283935 acc: 0.887900\n",
      "Epoch   98/2000 train_loss: 0.283726 acc: 0.887400\n",
      "Epoch   99/2000 train_loss: 0.283540 acc: 0.888200\n",
      "Epoch  100/2000 train_loss: 0.283328 acc: 0.888100\n",
      "Epoch  101/2000 train_loss: 0.283151 acc: 0.888300\n",
      "Epoch  102/2000 train_loss: 0.282965 acc: 0.888200\n",
      "Epoch  103/2000 train_loss: 0.282759 acc: 0.888100\n",
      "Epoch  104/2000 train_loss: 0.282591 acc: 0.888000\n",
      "Epoch  105/2000 train_loss: 0.282394 acc: 0.888200\n",
      "Epoch  106/2000 train_loss: 0.282223 acc: 0.888500\n",
      "Epoch  107/2000 train_loss: 0.282029 acc: 0.888000\n",
      "Epoch  108/2000 train_loss: 0.281858 acc: 0.888700\n",
      "Epoch  109/2000 train_loss: 0.281681 acc: 0.888400\n",
      "Epoch  110/2000 train_loss: 0.281495 acc: 0.889100\n",
      "Epoch  111/2000 train_loss: 0.281326 acc: 0.888100\n",
      "Epoch  112/2000 train_loss: 0.281151 acc: 0.888000\n",
      "Epoch  113/2000 train_loss: 0.281012 acc: 0.888700\n",
      "Epoch  114/2000 train_loss: 0.280838 acc: 0.889700\n",
      "Epoch  115/2000 train_loss: 0.280676 acc: 0.889400\n",
      "Epoch  116/2000 train_loss: 0.280523 acc: 0.889100\n",
      "Epoch  117/2000 train_loss: 0.280358 acc: 0.889500\n",
      "Epoch  118/2000 train_loss: 0.280197 acc: 0.889000\n",
      "Epoch  119/2000 train_loss: 0.280039 acc: 0.889500\n",
      "Epoch  120/2000 train_loss: 0.279889 acc: 0.889400\n",
      "Epoch  121/2000 train_loss: 0.279732 acc: 0.889600\n",
      "Epoch  122/2000 train_loss: 0.279588 acc: 0.889500\n",
      "Epoch  123/2000 train_loss: 0.279440 acc: 0.889200\n",
      "Epoch  124/2000 train_loss: 0.279287 acc: 0.889500\n",
      "Epoch  125/2000 train_loss: 0.279143 acc: 0.889600\n",
      "Epoch  126/2000 train_loss: 0.279001 acc: 0.889600\n",
      "Epoch  127/2000 train_loss: 0.278856 acc: 0.890100\n",
      "Epoch  128/2000 train_loss: 0.278718 acc: 0.889400\n",
      "Epoch  129/2000 train_loss: 0.278585 acc: 0.889700\n",
      "Epoch  130/2000 train_loss: 0.278452 acc: 0.889900\n",
      "Epoch  131/2000 train_loss: 0.278305 acc: 0.889800\n",
      "Epoch  132/2000 train_loss: 0.278176 acc: 0.889300\n",
      "Epoch  133/2000 train_loss: 0.278040 acc: 0.889800\n",
      "Epoch  134/2000 train_loss: 0.277909 acc: 0.889700\n",
      "Epoch  135/2000 train_loss: 0.277763 acc: 0.890100\n",
      "Epoch  136/2000 train_loss: 0.277649 acc: 0.889700\n",
      "Epoch  137/2000 train_loss: 0.277511 acc: 0.890100\n",
      "Epoch  138/2000 train_loss: 0.277394 acc: 0.889700\n",
      "Epoch  139/2000 train_loss: 0.277248 acc: 0.891100\n",
      "Epoch  140/2000 train_loss: 0.277154 acc: 0.890100\n",
      "Epoch  141/2000 train_loss: 0.277007 acc: 0.889800\n",
      "Epoch  142/2000 train_loss: 0.276905 acc: 0.890700\n",
      "Epoch  143/2000 train_loss: 0.276779 acc: 0.890100\n",
      "Epoch  144/2000 train_loss: 0.276667 acc: 0.890800\n",
      "Epoch  145/2000 train_loss: 0.276542 acc: 0.890600\n",
      "Epoch  146/2000 train_loss: 0.276432 acc: 0.891000\n",
      "Epoch  147/2000 train_loss: 0.276310 acc: 0.890900\n",
      "Epoch  148/2000 train_loss: 0.276204 acc: 0.890600\n",
      "Epoch  149/2000 train_loss: 0.276091 acc: 0.890600\n",
      "Epoch  150/2000 train_loss: 0.275978 acc: 0.890700\n",
      "Epoch  151/2000 train_loss: 0.275856 acc: 0.890600\n",
      "Epoch  152/2000 train_loss: 0.275760 acc: 0.890700\n",
      "Epoch  153/2000 train_loss: 0.275636 acc: 0.890900\n",
      "Epoch  154/2000 train_loss: 0.275534 acc: 0.890600\n",
      "Epoch  155/2000 train_loss: 0.275439 acc: 0.890700\n",
      "Epoch  156/2000 train_loss: 0.275321 acc: 0.891000\n",
      "Epoch  157/2000 train_loss: 0.275226 acc: 0.890900\n",
      "Epoch  158/2000 train_loss: 0.275124 acc: 0.890900\n",
      "Epoch  159/2000 train_loss: 0.275018 acc: 0.891400\n",
      "Epoch  160/2000 train_loss: 0.274912 acc: 0.891300\n",
      "Epoch  161/2000 train_loss: 0.274817 acc: 0.891100\n",
      "Epoch  162/2000 train_loss: 0.274706 acc: 0.891500\n",
      "Epoch  163/2000 train_loss: 0.274618 acc: 0.891600\n",
      "Epoch  164/2000 train_loss: 0.274506 acc: 0.891200\n",
      "Epoch  165/2000 train_loss: 0.274420 acc: 0.891400\n",
      "Epoch  166/2000 train_loss: 0.274328 acc: 0.891400\n",
      "Epoch  167/2000 train_loss: 0.274219 acc: 0.891200\n",
      "Epoch  168/2000 train_loss: 0.274136 acc: 0.891600\n",
      "Epoch  169/2000 train_loss: 0.274041 acc: 0.891500\n",
      "Epoch  170/2000 train_loss: 0.273928 acc: 0.891600\n",
      "Epoch  171/2000 train_loss: 0.273869 acc: 0.891700\n",
      "Epoch  172/2000 train_loss: 0.273749 acc: 0.891500\n",
      "Epoch  173/2000 train_loss: 0.273664 acc: 0.891900\n",
      "Epoch  174/2000 train_loss: 0.273558 acc: 0.891900\n",
      "Epoch  175/2000 train_loss: 0.273493 acc: 0.891800\n",
      "Epoch  176/2000 train_loss: 0.273404 acc: 0.891700\n",
      "Epoch  177/2000 train_loss: 0.273316 acc: 0.892200\n",
      "Epoch  178/2000 train_loss: 0.273235 acc: 0.891900\n",
      "Epoch  179/2000 train_loss: 0.273151 acc: 0.892200\n",
      "Epoch  180/2000 train_loss: 0.273065 acc: 0.892100\n",
      "Epoch  181/2000 train_loss: 0.272975 acc: 0.892100\n",
      "Epoch  182/2000 train_loss: 0.272877 acc: 0.891800\n",
      "Epoch  183/2000 train_loss: 0.272795 acc: 0.892000\n",
      "Epoch  184/2000 train_loss: 0.272729 acc: 0.892200\n",
      "Epoch  185/2000 train_loss: 0.272633 acc: 0.891700\n",
      "Epoch  186/2000 train_loss: 0.272552 acc: 0.891700\n",
      "Epoch  187/2000 train_loss: 0.272470 acc: 0.892200\n",
      "Epoch  188/2000 train_loss: 0.272388 acc: 0.892500\n",
      "Epoch  189/2000 train_loss: 0.272323 acc: 0.892200\n",
      "Epoch  190/2000 train_loss: 0.272240 acc: 0.892100\n",
      "Epoch  191/2000 train_loss: 0.272159 acc: 0.892300\n",
      "Epoch  192/2000 train_loss: 0.272089 acc: 0.891900\n",
      "Epoch  193/2000 train_loss: 0.272019 acc: 0.892000\n",
      "Epoch  194/2000 train_loss: 0.271913 acc: 0.892100\n",
      "Epoch  195/2000 train_loss: 0.271863 acc: 0.892000\n",
      "Epoch  196/2000 train_loss: 0.271788 acc: 0.891500\n",
      "Epoch  197/2000 train_loss: 0.271721 acc: 0.892100\n",
      "Epoch  198/2000 train_loss: 0.271628 acc: 0.892000\n",
      "Epoch  199/2000 train_loss: 0.271565 acc: 0.892000\n",
      "Epoch  200/2000 train_loss: 0.271492 acc: 0.892100\n",
      "Epoch  201/2000 train_loss: 0.271416 acc: 0.892000\n",
      "Epoch  202/2000 train_loss: 0.271346 acc: 0.891900\n",
      "Epoch  203/2000 train_loss: 0.271284 acc: 0.891600\n",
      "Epoch  204/2000 train_loss: 0.271216 acc: 0.892300\n",
      "Epoch  205/2000 train_loss: 0.271142 acc: 0.892000\n",
      "Epoch  206/2000 train_loss: 0.271046 acc: 0.892600\n",
      "Epoch  207/2000 train_loss: 0.271001 acc: 0.892200\n",
      "Epoch  208/2000 train_loss: 0.270929 acc: 0.892400\n",
      "Epoch  209/2000 train_loss: 0.270855 acc: 0.891800\n",
      "Epoch  210/2000 train_loss: 0.270801 acc: 0.892100\n",
      "Epoch  211/2000 train_loss: 0.270724 acc: 0.892300\n",
      "Epoch  212/2000 train_loss: 0.270670 acc: 0.892600\n",
      "Epoch  213/2000 train_loss: 0.270600 acc: 0.892100\n",
      "Epoch  214/2000 train_loss: 0.270534 acc: 0.892200\n",
      "Epoch  215/2000 train_loss: 0.270466 acc: 0.892200\n",
      "Epoch  216/2000 train_loss: 0.270411 acc: 0.892200\n",
      "Epoch  217/2000 train_loss: 0.270344 acc: 0.892800\n",
      "Epoch  218/2000 train_loss: 0.270260 acc: 0.892700\n",
      "Epoch  219/2000 train_loss: 0.270216 acc: 0.892800\n",
      "Epoch  220/2000 train_loss: 0.270149 acc: 0.892600\n",
      "Epoch  221/2000 train_loss: 0.270103 acc: 0.892600\n",
      "Epoch  222/2000 train_loss: 0.270032 acc: 0.892700\n",
      "Epoch  223/2000 train_loss: 0.269967 acc: 0.892400\n",
      "Epoch  224/2000 train_loss: 0.269912 acc: 0.892900\n",
      "Epoch  225/2000 train_loss: 0.269852 acc: 0.892900\n",
      "Epoch  226/2000 train_loss: 0.269788 acc: 0.892800\n",
      "Epoch  227/2000 train_loss: 0.269731 acc: 0.892800\n",
      "Epoch  228/2000 train_loss: 0.269670 acc: 0.892800\n",
      "Epoch  229/2000 train_loss: 0.269611 acc: 0.892600\n",
      "Epoch  230/2000 train_loss: 0.269538 acc: 0.892600\n",
      "Epoch  231/2000 train_loss: 0.269489 acc: 0.892800\n",
      "Epoch  232/2000 train_loss: 0.269435 acc: 0.892500\n",
      "Epoch  233/2000 train_loss: 0.269376 acc: 0.892900\n",
      "Epoch  234/2000 train_loss: 0.269328 acc: 0.892700\n",
      "Epoch  235/2000 train_loss: 0.269263 acc: 0.893300\n",
      "Epoch  236/2000 train_loss: 0.269213 acc: 0.893200\n",
      "Epoch  237/2000 train_loss: 0.269151 acc: 0.893100\n",
      "Epoch  238/2000 train_loss: 0.269102 acc: 0.893200\n",
      "Epoch  239/2000 train_loss: 0.269038 acc: 0.893500\n",
      "Epoch  240/2000 train_loss: 0.268990 acc: 0.893100\n",
      "Epoch  241/2000 train_loss: 0.268931 acc: 0.893300\n",
      "Epoch  242/2000 train_loss: 0.268880 acc: 0.893200\n",
      "Epoch  243/2000 train_loss: 0.268828 acc: 0.893900\n",
      "Epoch  244/2000 train_loss: 0.268780 acc: 0.893000\n",
      "Epoch  245/2000 train_loss: 0.268719 acc: 0.893200\n",
      "Epoch  246/2000 train_loss: 0.268652 acc: 0.893300\n",
      "Epoch  247/2000 train_loss: 0.268623 acc: 0.893400\n",
      "Epoch  248/2000 train_loss: 0.268559 acc: 0.893000\n",
      "Epoch  249/2000 train_loss: 0.268516 acc: 0.893300\n",
      "Epoch  250/2000 train_loss: 0.268462 acc: 0.893500\n",
      "Epoch  251/2000 train_loss: 0.268410 acc: 0.893800\n",
      "Epoch  252/2000 train_loss: 0.268363 acc: 0.893400\n",
      "Epoch  253/2000 train_loss: 0.268315 acc: 0.893500\n",
      "Epoch  254/2000 train_loss: 0.268266 acc: 0.893700\n",
      "Epoch  255/2000 train_loss: 0.268207 acc: 0.893500\n",
      "Epoch  256/2000 train_loss: 0.268163 acc: 0.893400\n",
      "Epoch  257/2000 train_loss: 0.268109 acc: 0.893600\n",
      "Epoch  258/2000 train_loss: 0.268072 acc: 0.893600\n",
      "Epoch  259/2000 train_loss: 0.268014 acc: 0.893400\n",
      "Epoch  260/2000 train_loss: 0.267965 acc: 0.893900\n",
      "Epoch  261/2000 train_loss: 0.267918 acc: 0.893500\n",
      "Epoch  262/2000 train_loss: 0.267881 acc: 0.893400\n",
      "Epoch  263/2000 train_loss: 0.267817 acc: 0.893500\n",
      "Epoch  264/2000 train_loss: 0.267779 acc: 0.893900\n",
      "Epoch  265/2000 train_loss: 0.267741 acc: 0.893400\n",
      "Epoch  266/2000 train_loss: 0.267687 acc: 0.893600\n",
      "Epoch  267/2000 train_loss: 0.267641 acc: 0.893900\n",
      "Epoch  268/2000 train_loss: 0.267587 acc: 0.893700\n",
      "Epoch  269/2000 train_loss: 0.267544 acc: 0.893700\n",
      "Epoch  270/2000 train_loss: 0.267502 acc: 0.893700\n",
      "Epoch  271/2000 train_loss: 0.267458 acc: 0.893500\n",
      "Epoch  272/2000 train_loss: 0.267419 acc: 0.893900\n",
      "Epoch  273/2000 train_loss: 0.267376 acc: 0.893500\n",
      "Epoch  274/2000 train_loss: 0.267333 acc: 0.893600\n",
      "Epoch  275/2000 train_loss: 0.267284 acc: 0.894000\n",
      "Epoch  276/2000 train_loss: 0.267243 acc: 0.893400\n",
      "Epoch  277/2000 train_loss: 0.267192 acc: 0.893400\n",
      "Epoch  278/2000 train_loss: 0.267153 acc: 0.893700\n",
      "Epoch  279/2000 train_loss: 0.267113 acc: 0.893700\n",
      "Epoch  280/2000 train_loss: 0.267075 acc: 0.893600\n",
      "Epoch  281/2000 train_loss: 0.267015 acc: 0.893900\n",
      "Epoch  282/2000 train_loss: 0.266995 acc: 0.893700\n",
      "Epoch  283/2000 train_loss: 0.266943 acc: 0.894200\n",
      "Epoch  284/2000 train_loss: 0.266902 acc: 0.894100\n",
      "Epoch  285/2000 train_loss: 0.266860 acc: 0.893800\n",
      "Epoch  286/2000 train_loss: 0.266817 acc: 0.893600\n",
      "Epoch  287/2000 train_loss: 0.266777 acc: 0.894100\n",
      "Epoch  288/2000 train_loss: 0.266725 acc: 0.894200\n",
      "Epoch  289/2000 train_loss: 0.266692 acc: 0.893900\n",
      "Epoch  290/2000 train_loss: 0.266658 acc: 0.894200\n",
      "Epoch  291/2000 train_loss: 0.266621 acc: 0.894300\n",
      "Epoch  292/2000 train_loss: 0.266574 acc: 0.894200\n",
      "Epoch  293/2000 train_loss: 0.266545 acc: 0.894000\n",
      "Epoch  294/2000 train_loss: 0.266502 acc: 0.894100\n",
      "Epoch  295/2000 train_loss: 0.266448 acc: 0.893900\n",
      "Epoch  296/2000 train_loss: 0.266419 acc: 0.894100\n",
      "Epoch  297/2000 train_loss: 0.266382 acc: 0.894100\n",
      "Epoch  298/2000 train_loss: 0.266339 acc: 0.894100\n",
      "Epoch  299/2000 train_loss: 0.266290 acc: 0.893800\n",
      "Epoch  300/2000 train_loss: 0.266273 acc: 0.894200\n",
      "Epoch  301/2000 train_loss: 0.266228 acc: 0.894400\n",
      "Epoch  302/2000 train_loss: 0.266190 acc: 0.894700\n",
      "Epoch  303/2000 train_loss: 0.266153 acc: 0.894100\n",
      "Epoch  304/2000 train_loss: 0.266113 acc: 0.894300\n",
      "Epoch  305/2000 train_loss: 0.266075 acc: 0.894100\n",
      "Epoch  306/2000 train_loss: 0.266044 acc: 0.894400\n",
      "Epoch  307/2000 train_loss: 0.265994 acc: 0.894600\n",
      "Epoch  308/2000 train_loss: 0.265967 acc: 0.894500\n",
      "Epoch  309/2000 train_loss: 0.265929 acc: 0.894400\n",
      "Epoch  310/2000 train_loss: 0.265895 acc: 0.894400\n",
      "Epoch  311/2000 train_loss: 0.265861 acc: 0.894400\n",
      "Epoch  312/2000 train_loss: 0.265806 acc: 0.894600\n",
      "Epoch  313/2000 train_loss: 0.265797 acc: 0.894400\n",
      "Epoch  314/2000 train_loss: 0.265752 acc: 0.894700\n",
      "Epoch  315/2000 train_loss: 0.265723 acc: 0.895000\n",
      "Epoch  316/2000 train_loss: 0.265676 acc: 0.894400\n",
      "Epoch  317/2000 train_loss: 0.265641 acc: 0.894900\n",
      "Epoch  318/2000 train_loss: 0.265623 acc: 0.894600\n",
      "Epoch  319/2000 train_loss: 0.265578 acc: 0.894500\n",
      "Epoch  320/2000 train_loss: 0.265541 acc: 0.895500\n",
      "Epoch  321/2000 train_loss: 0.265512 acc: 0.894700\n",
      "Epoch  322/2000 train_loss: 0.265476 acc: 0.895500\n",
      "Epoch  323/2000 train_loss: 0.265419 acc: 0.894600\n",
      "Epoch  324/2000 train_loss: 0.265412 acc: 0.894500\n",
      "Epoch  325/2000 train_loss: 0.265373 acc: 0.894700\n",
      "Epoch  326/2000 train_loss: 0.265328 acc: 0.894800\n",
      "Epoch  327/2000 train_loss: 0.265320 acc: 0.894700\n",
      "Epoch  328/2000 train_loss: 0.265266 acc: 0.894800\n",
      "Epoch  329/2000 train_loss: 0.265241 acc: 0.895500\n",
      "Epoch  330/2000 train_loss: 0.265213 acc: 0.894600\n",
      "Epoch  331/2000 train_loss: 0.265180 acc: 0.895000\n",
      "Epoch  332/2000 train_loss: 0.265149 acc: 0.895200\n",
      "Epoch  333/2000 train_loss: 0.265119 acc: 0.895000\n",
      "Epoch  334/2000 train_loss: 0.265085 acc: 0.895300\n",
      "Epoch  335/2000 train_loss: 0.265050 acc: 0.895100\n",
      "Epoch  336/2000 train_loss: 0.264999 acc: 0.895300\n",
      "Epoch  337/2000 train_loss: 0.264976 acc: 0.895400\n",
      "Epoch  338/2000 train_loss: 0.264957 acc: 0.895500\n",
      "Epoch  339/2000 train_loss: 0.264922 acc: 0.895400\n",
      "Epoch  340/2000 train_loss: 0.264883 acc: 0.895500\n",
      "Epoch  341/2000 train_loss: 0.264868 acc: 0.895300\n",
      "Epoch  342/2000 train_loss: 0.264835 acc: 0.895400\n",
      "Epoch  343/2000 train_loss: 0.264800 acc: 0.895400\n",
      "Epoch  344/2000 train_loss: 0.264772 acc: 0.895900\n",
      "Epoch  345/2000 train_loss: 0.264743 acc: 0.895400\n",
      "Epoch  346/2000 train_loss: 0.264717 acc: 0.895900\n",
      "Epoch  347/2000 train_loss: 0.264682 acc: 0.895600\n",
      "Epoch  348/2000 train_loss: 0.264652 acc: 0.895700\n",
      "Epoch  349/2000 train_loss: 0.264614 acc: 0.896000\n",
      "Epoch  350/2000 train_loss: 0.264577 acc: 0.895200\n",
      "Epoch  351/2000 train_loss: 0.264563 acc: 0.896100\n",
      "Epoch  352/2000 train_loss: 0.264527 acc: 0.895700\n",
      "Epoch  353/2000 train_loss: 0.264500 acc: 0.895700\n",
      "Epoch  354/2000 train_loss: 0.264471 acc: 0.896000\n",
      "Epoch  355/2000 train_loss: 0.264443 acc: 0.896100\n",
      "Epoch  356/2000 train_loss: 0.264411 acc: 0.896100\n",
      "Epoch  357/2000 train_loss: 0.264374 acc: 0.896500\n",
      "Epoch  358/2000 train_loss: 0.264347 acc: 0.896000\n",
      "Epoch  359/2000 train_loss: 0.264326 acc: 0.895500\n",
      "Epoch  360/2000 train_loss: 0.264305 acc: 0.895800\n",
      "Epoch  361/2000 train_loss: 0.264273 acc: 0.896400\n",
      "Epoch  362/2000 train_loss: 0.264234 acc: 0.896400\n",
      "Epoch  363/2000 train_loss: 0.264209 acc: 0.896100\n",
      "Epoch  364/2000 train_loss: 0.264185 acc: 0.896400\n",
      "Epoch  365/2000 train_loss: 0.264160 acc: 0.896400\n",
      "Epoch  366/2000 train_loss: 0.264128 acc: 0.895700\n",
      "Epoch  367/2000 train_loss: 0.264102 acc: 0.896400\n",
      "Epoch  368/2000 train_loss: 0.264072 acc: 0.896100\n",
      "Epoch  369/2000 train_loss: 0.264050 acc: 0.896400\n",
      "Epoch  370/2000 train_loss: 0.264009 acc: 0.896300\n",
      "Epoch  371/2000 train_loss: 0.263991 acc: 0.896100\n",
      "Epoch  372/2000 train_loss: 0.263974 acc: 0.895900\n",
      "Epoch  373/2000 train_loss: 0.263937 acc: 0.895800\n",
      "Epoch  374/2000 train_loss: 0.263908 acc: 0.896400\n",
      "Epoch  375/2000 train_loss: 0.263876 acc: 0.896200\n",
      "Epoch  376/2000 train_loss: 0.263861 acc: 0.896200\n",
      "Epoch  377/2000 train_loss: 0.263842 acc: 0.896300\n",
      "Epoch  378/2000 train_loss: 0.263808 acc: 0.896000\n",
      "Epoch  379/2000 train_loss: 0.263785 acc: 0.896100\n",
      "Epoch  380/2000 train_loss: 0.263763 acc: 0.896400\n",
      "Epoch  381/2000 train_loss: 0.263732 acc: 0.896100\n",
      "Epoch  382/2000 train_loss: 0.263701 acc: 0.896600\n",
      "Epoch  383/2000 train_loss: 0.263679 acc: 0.896200\n",
      "Epoch  384/2000 train_loss: 0.263659 acc: 0.896200\n",
      "Epoch  385/2000 train_loss: 0.263615 acc: 0.896300\n",
      "Epoch  386/2000 train_loss: 0.263610 acc: 0.895900\n",
      "Epoch  387/2000 train_loss: 0.263549 acc: 0.895900\n",
      "Epoch  388/2000 train_loss: 0.263551 acc: 0.896200\n",
      "Epoch  389/2000 train_loss: 0.263523 acc: 0.896300\n",
      "Epoch  390/2000 train_loss: 0.263498 acc: 0.896600\n",
      "Epoch  391/2000 train_loss: 0.263479 acc: 0.896300\n",
      "Epoch  392/2000 train_loss: 0.263445 acc: 0.896100\n",
      "Epoch  393/2000 train_loss: 0.263404 acc: 0.896700\n",
      "Epoch  394/2000 train_loss: 0.263415 acc: 0.895900\n",
      "Epoch  395/2000 train_loss: 0.263384 acc: 0.896100\n",
      "Epoch  396/2000 train_loss: 0.263356 acc: 0.896200\n",
      "Epoch  397/2000 train_loss: 0.263323 acc: 0.896000\n",
      "Epoch  398/2000 train_loss: 0.263311 acc: 0.896400\n",
      "Epoch  399/2000 train_loss: 0.263287 acc: 0.896400\n",
      "Epoch  400/2000 train_loss: 0.263257 acc: 0.896300\n",
      "Epoch  401/2000 train_loss: 0.263227 acc: 0.896200\n",
      "Epoch  402/2000 train_loss: 0.263176 acc: 0.896100\n",
      "Epoch  403/2000 train_loss: 0.263197 acc: 0.896300\n",
      "Epoch  404/2000 train_loss: 0.263162 acc: 0.896000\n",
      "Epoch  405/2000 train_loss: 0.263139 acc: 0.896400\n",
      "Epoch  406/2000 train_loss: 0.263111 acc: 0.896500\n",
      "Epoch  407/2000 train_loss: 0.263095 acc: 0.896200\n",
      "Epoch  408/2000 train_loss: 0.263061 acc: 0.896700\n",
      "Epoch  409/2000 train_loss: 0.263043 acc: 0.896300\n",
      "Epoch  410/2000 train_loss: 0.263023 acc: 0.896300\n",
      "Epoch  411/2000 train_loss: 0.263002 acc: 0.896400\n",
      "Epoch  412/2000 train_loss: 0.262979 acc: 0.896300\n",
      "Epoch  413/2000 train_loss: 0.262943 acc: 0.896200\n",
      "Epoch  414/2000 train_loss: 0.262932 acc: 0.896300\n",
      "Epoch  415/2000 train_loss: 0.262906 acc: 0.896300\n",
      "Epoch  416/2000 train_loss: 0.262880 acc: 0.896200\n",
      "Epoch  417/2000 train_loss: 0.262866 acc: 0.896100\n",
      "Epoch  418/2000 train_loss: 0.262841 acc: 0.896700\n",
      "Epoch  419/2000 train_loss: 0.262811 acc: 0.896000\n",
      "Epoch  420/2000 train_loss: 0.262804 acc: 0.896100\n",
      "Epoch  421/2000 train_loss: 0.262774 acc: 0.896300\n",
      "Epoch  422/2000 train_loss: 0.262741 acc: 0.896500\n",
      "Epoch  423/2000 train_loss: 0.262728 acc: 0.896300\n",
      "Epoch  424/2000 train_loss: 0.262709 acc: 0.896200\n",
      "Epoch  425/2000 train_loss: 0.262682 acc: 0.896500\n",
      "Epoch  426/2000 train_loss: 0.262670 acc: 0.895900\n",
      "Epoch  427/2000 train_loss: 0.262641 acc: 0.896000\n",
      "Epoch  428/2000 train_loss: 0.262625 acc: 0.896700\n",
      "Epoch  429/2000 train_loss: 0.262601 acc: 0.896100\n",
      "Epoch  430/2000 train_loss: 0.262576 acc: 0.896400\n",
      "Epoch  431/2000 train_loss: 0.262562 acc: 0.896300\n",
      "Epoch  432/2000 train_loss: 0.262525 acc: 0.896300\n",
      "Epoch  433/2000 train_loss: 0.262520 acc: 0.896300\n",
      "Epoch  434/2000 train_loss: 0.262493 acc: 0.896400\n",
      "Epoch  435/2000 train_loss: 0.262476 acc: 0.896100\n",
      "Epoch  436/2000 train_loss: 0.262446 acc: 0.896000\n",
      "Epoch  437/2000 train_loss: 0.262432 acc: 0.896700\n",
      "Epoch  438/2000 train_loss: 0.262402 acc: 0.896000\n",
      "Epoch  439/2000 train_loss: 0.262395 acc: 0.896500\n",
      "Epoch  440/2000 train_loss: 0.262353 acc: 0.896100\n",
      "Epoch  441/2000 train_loss: 0.262348 acc: 0.896400\n",
      "Epoch  442/2000 train_loss: 0.262330 acc: 0.896100\n",
      "Epoch  443/2000 train_loss: 0.262299 acc: 0.896300\n",
      "Epoch  444/2000 train_loss: 0.262297 acc: 0.896300\n",
      "Epoch  445/2000 train_loss: 0.262265 acc: 0.896100\n",
      "Epoch  446/2000 train_loss: 0.262245 acc: 0.896000\n",
      "Epoch  447/2000 train_loss: 0.262230 acc: 0.896200\n",
      "Epoch  448/2000 train_loss: 0.262206 acc: 0.896200\n",
      "Epoch  449/2000 train_loss: 0.262188 acc: 0.896100\n",
      "Epoch  450/2000 train_loss: 0.262168 acc: 0.896600\n",
      "Epoch  451/2000 train_loss: 0.262137 acc: 0.896200\n",
      "Epoch  452/2000 train_loss: 0.262127 acc: 0.895800\n",
      "Epoch  453/2000 train_loss: 0.262113 acc: 0.896100\n",
      "Epoch  454/2000 train_loss: 0.262084 acc: 0.897100\n",
      "Epoch  455/2000 train_loss: 0.262067 acc: 0.896400\n",
      "Epoch  456/2000 train_loss: 0.262043 acc: 0.896000\n",
      "Epoch  457/2000 train_loss: 0.262041 acc: 0.896600\n",
      "Epoch  458/2000 train_loss: 0.262014 acc: 0.896000\n",
      "Epoch  459/2000 train_loss: 0.261994 acc: 0.896700\n",
      "Epoch  460/2000 train_loss: 0.261976 acc: 0.896100\n",
      "Epoch  461/2000 train_loss: 0.261952 acc: 0.896300\n",
      "Epoch  462/2000 train_loss: 0.261919 acc: 0.896300\n",
      "Epoch  463/2000 train_loss: 0.261916 acc: 0.896400\n",
      "Epoch  464/2000 train_loss: 0.261893 acc: 0.896400\n",
      "Epoch  465/2000 train_loss: 0.261878 acc: 0.895800\n",
      "Epoch  466/2000 train_loss: 0.261853 acc: 0.896500\n",
      "Epoch  467/2000 train_loss: 0.261838 acc: 0.896200\n",
      "Epoch  468/2000 train_loss: 0.261828 acc: 0.896500\n",
      "Epoch  469/2000 train_loss: 0.261799 acc: 0.896100\n",
      "Epoch  470/2000 train_loss: 0.261784 acc: 0.896600\n",
      "Epoch  471/2000 train_loss: 0.261757 acc: 0.896300\n",
      "Epoch  472/2000 train_loss: 0.261749 acc: 0.896400\n",
      "Epoch  473/2000 train_loss: 0.261727 acc: 0.896300\n",
      "Epoch  474/2000 train_loss: 0.261702 acc: 0.896500\n",
      "Epoch  475/2000 train_loss: 0.261684 acc: 0.896600\n",
      "Epoch  476/2000 train_loss: 0.261670 acc: 0.896200\n",
      "Epoch  477/2000 train_loss: 0.261656 acc: 0.896300\n",
      "Epoch  478/2000 train_loss: 0.261633 acc: 0.896600\n",
      "Epoch  479/2000 train_loss: 0.261619 acc: 0.896400\n",
      "Epoch  480/2000 train_loss: 0.261594 acc: 0.896300\n",
      "Epoch  481/2000 train_loss: 0.261577 acc: 0.896600\n",
      "Epoch  482/2000 train_loss: 0.261558 acc: 0.896600\n",
      "Epoch  483/2000 train_loss: 0.261551 acc: 0.896200\n",
      "Epoch  484/2000 train_loss: 0.261532 acc: 0.896700\n",
      "Epoch  485/2000 train_loss: 0.261512 acc: 0.896000\n",
      "Epoch  486/2000 train_loss: 0.261498 acc: 0.896600\n",
      "Epoch  487/2000 train_loss: 0.261474 acc: 0.896600\n",
      "Epoch  488/2000 train_loss: 0.261452 acc: 0.896500\n",
      "Epoch  489/2000 train_loss: 0.261441 acc: 0.896400\n",
      "Epoch  490/2000 train_loss: 0.261416 acc: 0.896700\n",
      "Epoch  491/2000 train_loss: 0.261409 acc: 0.896600\n",
      "Epoch  492/2000 train_loss: 0.261381 acc: 0.896900\n",
      "Epoch  493/2000 train_loss: 0.261371 acc: 0.896500\n",
      "Epoch  494/2000 train_loss: 0.261339 acc: 0.897000\n",
      "Epoch  495/2000 train_loss: 0.261337 acc: 0.897100\n",
      "Epoch  496/2000 train_loss: 0.261315 acc: 0.896700\n",
      "Epoch  497/2000 train_loss: 0.261305 acc: 0.896700\n",
      "Epoch  498/2000 train_loss: 0.261293 acc: 0.896500\n",
      "Epoch  499/2000 train_loss: 0.261280 acc: 0.896600\n",
      "Epoch  500/2000 train_loss: 0.261255 acc: 0.896500\n",
      "Epoch  501/2000 train_loss: 0.261240 acc: 0.896800\n",
      "Epoch  502/2000 train_loss: 0.261210 acc: 0.896500\n",
      "Epoch  503/2000 train_loss: 0.261200 acc: 0.896800\n",
      "Epoch  504/2000 train_loss: 0.261178 acc: 0.896200\n",
      "Epoch  505/2000 train_loss: 0.261164 acc: 0.896900\n",
      "Epoch  506/2000 train_loss: 0.261153 acc: 0.896600\n",
      "Epoch  507/2000 train_loss: 0.261131 acc: 0.896900\n",
      "Epoch  508/2000 train_loss: 0.261120 acc: 0.897100\n",
      "Epoch  509/2000 train_loss: 0.261105 acc: 0.897000\n",
      "Epoch  510/2000 train_loss: 0.261072 acc: 0.896800\n",
      "Epoch  511/2000 train_loss: 0.261066 acc: 0.897100\n",
      "Epoch  512/2000 train_loss: 0.261048 acc: 0.896600\n",
      "Epoch  513/2000 train_loss: 0.261040 acc: 0.897200\n",
      "Epoch  514/2000 train_loss: 0.261020 acc: 0.897400\n",
      "Epoch  515/2000 train_loss: 0.260997 acc: 0.896800\n",
      "Epoch  516/2000 train_loss: 0.260983 acc: 0.897300\n",
      "Epoch  517/2000 train_loss: 0.260956 acc: 0.896800\n",
      "Epoch  518/2000 train_loss: 0.260963 acc: 0.897200\n",
      "Epoch  519/2000 train_loss: 0.260932 acc: 0.897000\n",
      "Epoch  520/2000 train_loss: 0.260921 acc: 0.896800\n",
      "Epoch  521/2000 train_loss: 0.260919 acc: 0.896500\n",
      "Epoch  522/2000 train_loss: 0.260891 acc: 0.897100\n",
      "Epoch  523/2000 train_loss: 0.260868 acc: 0.896900\n",
      "Epoch  524/2000 train_loss: 0.260863 acc: 0.897000\n",
      "Epoch  525/2000 train_loss: 0.260860 acc: 0.897400\n",
      "Epoch  526/2000 train_loss: 0.260836 acc: 0.896900\n",
      "Epoch  527/2000 train_loss: 0.260813 acc: 0.897100\n",
      "Epoch  528/2000 train_loss: 0.260807 acc: 0.896800\n",
      "Epoch  529/2000 train_loss: 0.260773 acc: 0.897300\n",
      "Epoch  530/2000 train_loss: 0.260768 acc: 0.897600\n",
      "Epoch  531/2000 train_loss: 0.260752 acc: 0.896500\n",
      "Epoch  532/2000 train_loss: 0.260739 acc: 0.897100\n",
      "Epoch  533/2000 train_loss: 0.260723 acc: 0.897200\n",
      "Epoch  534/2000 train_loss: 0.260710 acc: 0.897400\n",
      "Epoch  535/2000 train_loss: 0.260690 acc: 0.897000\n",
      "Epoch  536/2000 train_loss: 0.260676 acc: 0.897300\n",
      "Epoch  537/2000 train_loss: 0.260667 acc: 0.897600\n",
      "Epoch  538/2000 train_loss: 0.260650 acc: 0.897400\n",
      "Epoch  539/2000 train_loss: 0.260628 acc: 0.897200\n",
      "Epoch  540/2000 train_loss: 0.260616 acc: 0.897100\n",
      "Epoch  541/2000 train_loss: 0.260606 acc: 0.897300\n",
      "Epoch  542/2000 train_loss: 0.260581 acc: 0.897000\n",
      "Epoch  543/2000 train_loss: 0.260576 acc: 0.898000\n",
      "Epoch  544/2000 train_loss: 0.260556 acc: 0.897000\n",
      "Epoch  545/2000 train_loss: 0.260544 acc: 0.897200\n",
      "Epoch  546/2000 train_loss: 0.260525 acc: 0.897500\n",
      "Epoch  547/2000 train_loss: 0.260512 acc: 0.897400\n",
      "Epoch  548/2000 train_loss: 0.260502 acc: 0.897300\n",
      "Epoch  549/2000 train_loss: 0.260484 acc: 0.897500\n",
      "Epoch  550/2000 train_loss: 0.260471 acc: 0.897600\n",
      "Epoch  551/2000 train_loss: 0.260454 acc: 0.897600\n",
      "Epoch  552/2000 train_loss: 0.260444 acc: 0.897400\n",
      "Epoch  553/2000 train_loss: 0.260429 acc: 0.897200\n",
      "Epoch  554/2000 train_loss: 0.260414 acc: 0.897300\n",
      "Epoch  555/2000 train_loss: 0.260396 acc: 0.897300\n",
      "Epoch  556/2000 train_loss: 0.260385 acc: 0.897600\n",
      "Epoch  557/2000 train_loss: 0.260362 acc: 0.897600\n",
      "Epoch  558/2000 train_loss: 0.260355 acc: 0.897800\n",
      "Epoch  559/2000 train_loss: 0.260337 acc: 0.897600\n",
      "Epoch  560/2000 train_loss: 0.260326 acc: 0.897300\n",
      "Epoch  561/2000 train_loss: 0.260311 acc: 0.897500\n",
      "Epoch  562/2000 train_loss: 0.260300 acc: 0.897800\n",
      "Epoch  563/2000 train_loss: 0.260288 acc: 0.897500\n",
      "Epoch  564/2000 train_loss: 0.260280 acc: 0.897200\n",
      "Epoch  565/2000 train_loss: 0.260253 acc: 0.897200\n",
      "Epoch  566/2000 train_loss: 0.260244 acc: 0.897400\n",
      "Epoch  567/2000 train_loss: 0.260230 acc: 0.898000\n",
      "Epoch  568/2000 train_loss: 0.260213 acc: 0.897300\n",
      "Epoch  569/2000 train_loss: 0.260200 acc: 0.897800\n",
      "Epoch  570/2000 train_loss: 0.260185 acc: 0.897700\n",
      "Epoch  571/2000 train_loss: 0.260174 acc: 0.897600\n",
      "Epoch  572/2000 train_loss: 0.260162 acc: 0.897700\n",
      "Epoch  573/2000 train_loss: 0.260136 acc: 0.897700\n",
      "Epoch  574/2000 train_loss: 0.260138 acc: 0.897800\n",
      "Epoch  575/2000 train_loss: 0.260129 acc: 0.897800\n",
      "Epoch  576/2000 train_loss: 0.260110 acc: 0.897900\n",
      "Epoch  577/2000 train_loss: 0.260091 acc: 0.897800\n",
      "Epoch  578/2000 train_loss: 0.260083 acc: 0.897700\n",
      "Epoch  579/2000 train_loss: 0.260064 acc: 0.897500\n",
      "Epoch  580/2000 train_loss: 0.260052 acc: 0.897400\n",
      "Epoch  581/2000 train_loss: 0.260046 acc: 0.897600\n",
      "Epoch  582/2000 train_loss: 0.260029 acc: 0.897900\n",
      "Epoch  583/2000 train_loss: 0.260013 acc: 0.898100\n",
      "Epoch  584/2000 train_loss: 0.259988 acc: 0.897400\n",
      "Epoch  585/2000 train_loss: 0.259992 acc: 0.897600\n",
      "Epoch  586/2000 train_loss: 0.259977 acc: 0.897600\n",
      "Epoch  587/2000 train_loss: 0.259958 acc: 0.897800\n",
      "Epoch  588/2000 train_loss: 0.259946 acc: 0.897900\n",
      "Epoch  589/2000 train_loss: 0.259940 acc: 0.897600\n",
      "Epoch  590/2000 train_loss: 0.259915 acc: 0.897700\n",
      "Epoch  591/2000 train_loss: 0.259912 acc: 0.897300\n",
      "Epoch  592/2000 train_loss: 0.259892 acc: 0.897300\n",
      "Epoch  593/2000 train_loss: 0.259881 acc: 0.897600\n",
      "Epoch  594/2000 train_loss: 0.259860 acc: 0.897900\n",
      "Epoch  595/2000 train_loss: 0.259852 acc: 0.897900\n",
      "Epoch  596/2000 train_loss: 0.259839 acc: 0.897600\n",
      "Epoch  597/2000 train_loss: 0.259833 acc: 0.897600\n",
      "Epoch  598/2000 train_loss: 0.259820 acc: 0.897500\n",
      "Epoch  599/2000 train_loss: 0.259796 acc: 0.897900\n",
      "Epoch  600/2000 train_loss: 0.259794 acc: 0.897300\n",
      "Epoch  601/2000 train_loss: 0.259778 acc: 0.898100\n",
      "Epoch  602/2000 train_loss: 0.259763 acc: 0.897700\n",
      "Epoch  603/2000 train_loss: 0.259752 acc: 0.897500\n",
      "Epoch  604/2000 train_loss: 0.259736 acc: 0.898000\n",
      "Epoch  605/2000 train_loss: 0.259728 acc: 0.897400\n",
      "Epoch  606/2000 train_loss: 0.259722 acc: 0.897200\n",
      "Epoch  607/2000 train_loss: 0.259704 acc: 0.897400\n",
      "Epoch  608/2000 train_loss: 0.259684 acc: 0.897500\n",
      "Epoch  609/2000 train_loss: 0.259674 acc: 0.897900\n",
      "Epoch  610/2000 train_loss: 0.259666 acc: 0.898200\n",
      "Epoch  611/2000 train_loss: 0.259654 acc: 0.897500\n",
      "Epoch  612/2000 train_loss: 0.259641 acc: 0.897600\n",
      "Epoch  613/2000 train_loss: 0.259626 acc: 0.897600\n",
      "Epoch  614/2000 train_loss: 0.259616 acc: 0.897500\n",
      "Epoch  615/2000 train_loss: 0.259595 acc: 0.897400\n",
      "Epoch  616/2000 train_loss: 0.259595 acc: 0.897700\n",
      "Epoch  617/2000 train_loss: 0.259580 acc: 0.897900\n",
      "Epoch  618/2000 train_loss: 0.259572 acc: 0.897900\n",
      "Epoch  619/2000 train_loss: 0.259560 acc: 0.897500\n",
      "Epoch  620/2000 train_loss: 0.259544 acc: 0.897700\n",
      "Epoch  621/2000 train_loss: 0.259531 acc: 0.897900\n",
      "Epoch  622/2000 train_loss: 0.259518 acc: 0.897500\n",
      "Epoch  623/2000 train_loss: 0.259500 acc: 0.898500\n",
      "Epoch  624/2000 train_loss: 0.259507 acc: 0.897600\n",
      "Epoch  625/2000 train_loss: 0.259486 acc: 0.898100\n",
      "Epoch  626/2000 train_loss: 0.259466 acc: 0.898000\n",
      "Epoch  627/2000 train_loss: 0.259455 acc: 0.897200\n",
      "Epoch  628/2000 train_loss: 0.259449 acc: 0.897800\n",
      "Epoch  629/2000 train_loss: 0.259432 acc: 0.897900\n",
      "Epoch  630/2000 train_loss: 0.259426 acc: 0.897300\n",
      "Epoch  631/2000 train_loss: 0.259401 acc: 0.898200\n",
      "Epoch  632/2000 train_loss: 0.259413 acc: 0.897400\n",
      "Epoch  633/2000 train_loss: 0.259396 acc: 0.897900\n",
      "Epoch  634/2000 train_loss: 0.259379 acc: 0.897700\n",
      "Epoch  635/2000 train_loss: 0.259369 acc: 0.898000\n",
      "Epoch  636/2000 train_loss: 0.259366 acc: 0.897500\n",
      "Epoch  637/2000 train_loss: 0.259344 acc: 0.897900\n",
      "Epoch  638/2000 train_loss: 0.259326 acc: 0.897700\n",
      "Epoch  639/2000 train_loss: 0.259321 acc: 0.897900\n",
      "Epoch  640/2000 train_loss: 0.259312 acc: 0.898000\n",
      "Epoch  641/2000 train_loss: 0.259285 acc: 0.897500\n",
      "Epoch  642/2000 train_loss: 0.259282 acc: 0.897900\n",
      "Epoch  643/2000 train_loss: 0.259272 acc: 0.897900\n",
      "Epoch  644/2000 train_loss: 0.259271 acc: 0.897500\n",
      "Epoch  645/2000 train_loss: 0.259251 acc: 0.897600\n",
      "Epoch  646/2000 train_loss: 0.259237 acc: 0.898300\n",
      "Epoch  647/2000 train_loss: 0.259240 acc: 0.897700\n",
      "Epoch  648/2000 train_loss: 0.259223 acc: 0.897800\n",
      "Epoch  649/2000 train_loss: 0.259203 acc: 0.897900\n",
      "Epoch  650/2000 train_loss: 0.259203 acc: 0.897900\n",
      "Epoch  651/2000 train_loss: 0.259171 acc: 0.898100\n",
      "Epoch  652/2000 train_loss: 0.259181 acc: 0.897300\n",
      "Epoch  653/2000 train_loss: 0.259164 acc: 0.897800\n",
      "Epoch  654/2000 train_loss: 0.259154 acc: 0.897700\n",
      "Epoch  655/2000 train_loss: 0.259140 acc: 0.897900\n",
      "Epoch  656/2000 train_loss: 0.259125 acc: 0.897700\n",
      "Epoch  657/2000 train_loss: 0.259115 acc: 0.898000\n",
      "Epoch  658/2000 train_loss: 0.259102 acc: 0.897500\n",
      "Epoch  659/2000 train_loss: 0.259097 acc: 0.898200\n",
      "Epoch  660/2000 train_loss: 0.259092 acc: 0.897800\n",
      "Epoch  661/2000 train_loss: 0.259070 acc: 0.897900\n",
      "Epoch  662/2000 train_loss: 0.259068 acc: 0.897400\n",
      "Epoch  663/2000 train_loss: 0.259053 acc: 0.898000\n",
      "Epoch  664/2000 train_loss: 0.259035 acc: 0.897800\n",
      "Epoch  665/2000 train_loss: 0.259025 acc: 0.897900\n",
      "Epoch  666/2000 train_loss: 0.259027 acc: 0.898200\n",
      "Epoch  667/2000 train_loss: 0.259012 acc: 0.897500\n",
      "Epoch  668/2000 train_loss: 0.258996 acc: 0.897900\n",
      "Epoch  669/2000 train_loss: 0.258985 acc: 0.898200\n",
      "Epoch  670/2000 train_loss: 0.258976 acc: 0.897700\n",
      "Epoch  671/2000 train_loss: 0.258963 acc: 0.898200\n",
      "Epoch  672/2000 train_loss: 0.258963 acc: 0.898200\n",
      "Epoch  673/2000 train_loss: 0.258943 acc: 0.897800\n",
      "Epoch  674/2000 train_loss: 0.258931 acc: 0.898100\n",
      "Epoch  675/2000 train_loss: 0.258913 acc: 0.897700\n",
      "Epoch  676/2000 train_loss: 0.258912 acc: 0.897900\n",
      "Epoch  677/2000 train_loss: 0.258901 acc: 0.897700\n",
      "Epoch  678/2000 train_loss: 0.258887 acc: 0.897900\n",
      "Epoch  679/2000 train_loss: 0.258877 acc: 0.897500\n",
      "Epoch  680/2000 train_loss: 0.258867 acc: 0.897800\n",
      "Epoch  681/2000 train_loss: 0.258867 acc: 0.897900\n",
      "Epoch  682/2000 train_loss: 0.258837 acc: 0.898300\n",
      "Epoch  683/2000 train_loss: 0.258843 acc: 0.898000\n",
      "Epoch  684/2000 train_loss: 0.258833 acc: 0.898100\n",
      "Epoch  685/2000 train_loss: 0.258819 acc: 0.898100\n",
      "Epoch  686/2000 train_loss: 0.258806 acc: 0.898000\n",
      "Epoch  687/2000 train_loss: 0.258787 acc: 0.898200\n",
      "Epoch  688/2000 train_loss: 0.258780 acc: 0.898300\n",
      "Epoch  689/2000 train_loss: 0.258772 acc: 0.897800\n",
      "Epoch  690/2000 train_loss: 0.258760 acc: 0.897600\n",
      "Epoch  691/2000 train_loss: 0.258764 acc: 0.898000\n",
      "Epoch  692/2000 train_loss: 0.258748 acc: 0.897900\n",
      "Epoch  693/2000 train_loss: 0.258742 acc: 0.898300\n",
      "Epoch  694/2000 train_loss: 0.258731 acc: 0.898000\n",
      "Epoch  695/2000 train_loss: 0.258720 acc: 0.898000\n",
      "Epoch  696/2000 train_loss: 0.258707 acc: 0.898300\n",
      "Epoch  697/2000 train_loss: 0.258701 acc: 0.898000\n",
      "Epoch  698/2000 train_loss: 0.258684 acc: 0.898100\n",
      "Epoch  699/2000 train_loss: 0.258684 acc: 0.898200\n",
      "Epoch  700/2000 train_loss: 0.258665 acc: 0.898100\n",
      "Epoch  701/2000 train_loss: 0.258652 acc: 0.898200\n",
      "Epoch  702/2000 train_loss: 0.258649 acc: 0.898000\n",
      "Epoch  703/2000 train_loss: 0.258633 acc: 0.898000\n",
      "Epoch  704/2000 train_loss: 0.258637 acc: 0.897900\n",
      "Epoch  705/2000 train_loss: 0.258611 acc: 0.898200\n",
      "Epoch  706/2000 train_loss: 0.258601 acc: 0.897700\n",
      "Epoch  707/2000 train_loss: 0.258595 acc: 0.898300\n",
      "Epoch  708/2000 train_loss: 0.258585 acc: 0.897900\n",
      "Epoch  709/2000 train_loss: 0.258585 acc: 0.898000\n",
      "Epoch  710/2000 train_loss: 0.258567 acc: 0.897700\n",
      "Epoch  711/2000 train_loss: 0.258560 acc: 0.897600\n",
      "Epoch  712/2000 train_loss: 0.258557 acc: 0.898200\n",
      "Epoch  713/2000 train_loss: 0.258522 acc: 0.898700\n",
      "Epoch  714/2000 train_loss: 0.258536 acc: 0.897700\n",
      "Epoch  715/2000 train_loss: 0.258512 acc: 0.898200\n",
      "Epoch  716/2000 train_loss: 0.258515 acc: 0.898000\n",
      "Epoch  717/2000 train_loss: 0.258497 acc: 0.898000\n",
      "Epoch  718/2000 train_loss: 0.258492 acc: 0.897700\n",
      "Epoch  719/2000 train_loss: 0.258477 acc: 0.898300\n",
      "Epoch  720/2000 train_loss: 0.258476 acc: 0.898500\n",
      "Epoch  721/2000 train_loss: 0.258467 acc: 0.898100\n",
      "Epoch  722/2000 train_loss: 0.258455 acc: 0.898200\n",
      "Epoch  723/2000 train_loss: 0.258437 acc: 0.898000\n",
      "Epoch  724/2000 train_loss: 0.258427 acc: 0.898200\n",
      "Epoch  725/2000 train_loss: 0.258418 acc: 0.897700\n",
      "Epoch  726/2000 train_loss: 0.258417 acc: 0.898200\n",
      "Epoch  727/2000 train_loss: 0.258404 acc: 0.898100\n",
      "Epoch  728/2000 train_loss: 0.258395 acc: 0.898200\n",
      "Epoch  729/2000 train_loss: 0.258386 acc: 0.898400\n",
      "Epoch  730/2000 train_loss: 0.258373 acc: 0.898100\n",
      "Epoch  731/2000 train_loss: 0.258362 acc: 0.898200\n",
      "Epoch  732/2000 train_loss: 0.258356 acc: 0.898300\n",
      "Epoch  733/2000 train_loss: 0.258347 acc: 0.898000\n",
      "Epoch  734/2000 train_loss: 0.258331 acc: 0.897700\n",
      "Epoch  735/2000 train_loss: 0.258336 acc: 0.898500\n",
      "Epoch  736/2000 train_loss: 0.258319 acc: 0.898100\n",
      "Epoch  737/2000 train_loss: 0.258309 acc: 0.898500\n",
      "Epoch  738/2000 train_loss: 0.258272 acc: 0.898000\n",
      "Epoch  739/2000 train_loss: 0.258284 acc: 0.897900\n",
      "Epoch  740/2000 train_loss: 0.258281 acc: 0.898100\n",
      "Epoch  741/2000 train_loss: 0.258273 acc: 0.898100\n",
      "Epoch  742/2000 train_loss: 0.258273 acc: 0.898400\n",
      "Epoch  743/2000 train_loss: 0.258252 acc: 0.897900\n",
      "Epoch  744/2000 train_loss: 0.258247 acc: 0.898400\n",
      "Epoch  745/2000 train_loss: 0.258227 acc: 0.898100\n",
      "Epoch  746/2000 train_loss: 0.258234 acc: 0.898300\n",
      "Epoch  747/2000 train_loss: 0.258218 acc: 0.897900\n",
      "Epoch  748/2000 train_loss: 0.258211 acc: 0.898000\n",
      "Epoch  749/2000 train_loss: 0.258207 acc: 0.898400\n",
      "Epoch  750/2000 train_loss: 0.258186 acc: 0.898200\n",
      "Epoch  751/2000 train_loss: 0.258176 acc: 0.898300\n",
      "Epoch  752/2000 train_loss: 0.258174 acc: 0.898300\n",
      "Epoch  753/2000 train_loss: 0.258158 acc: 0.898100\n",
      "Epoch  754/2000 train_loss: 0.258152 acc: 0.898300\n",
      "Epoch  755/2000 train_loss: 0.258145 acc: 0.898500\n",
      "Epoch  756/2000 train_loss: 0.258139 acc: 0.898400\n",
      "Epoch  757/2000 train_loss: 0.258116 acc: 0.898600\n",
      "Epoch  758/2000 train_loss: 0.258136 acc: 0.898300\n",
      "Epoch  759/2000 train_loss: 0.258113 acc: 0.898300\n",
      "Epoch  760/2000 train_loss: 0.258103 acc: 0.898600\n",
      "Epoch  761/2000 train_loss: 0.258096 acc: 0.898600\n",
      "Epoch  762/2000 train_loss: 0.258081 acc: 0.898000\n",
      "Epoch  763/2000 train_loss: 0.258077 acc: 0.898600\n",
      "Epoch  764/2000 train_loss: 0.258071 acc: 0.898400\n",
      "Epoch  765/2000 train_loss: 0.258056 acc: 0.898100\n",
      "Epoch  766/2000 train_loss: 0.258044 acc: 0.898500\n",
      "Epoch  767/2000 train_loss: 0.258044 acc: 0.898400\n",
      "Epoch  768/2000 train_loss: 0.258033 acc: 0.897800\n",
      "Epoch  769/2000 train_loss: 0.258029 acc: 0.898100\n",
      "Epoch  770/2000 train_loss: 0.258016 acc: 0.898300\n",
      "Epoch  771/2000 train_loss: 0.258004 acc: 0.898300\n",
      "Epoch  772/2000 train_loss: 0.257998 acc: 0.898000\n",
      "Epoch  773/2000 train_loss: 0.257982 acc: 0.898200\n",
      "Epoch  774/2000 train_loss: 0.257984 acc: 0.898500\n",
      "Epoch  775/2000 train_loss: 0.257976 acc: 0.898000\n",
      "Epoch  776/2000 train_loss: 0.257967 acc: 0.898300\n",
      "Epoch  777/2000 train_loss: 0.257957 acc: 0.898300\n",
      "Epoch  778/2000 train_loss: 0.257949 acc: 0.898000\n",
      "Epoch  779/2000 train_loss: 0.257947 acc: 0.898100\n",
      "Epoch  780/2000 train_loss: 0.257921 acc: 0.898100\n",
      "Epoch  781/2000 train_loss: 0.257924 acc: 0.898600\n",
      "Epoch  782/2000 train_loss: 0.257920 acc: 0.898000\n",
      "Epoch  783/2000 train_loss: 0.257899 acc: 0.898300\n",
      "Epoch  784/2000 train_loss: 0.257887 acc: 0.898500\n",
      "Epoch  785/2000 train_loss: 0.257890 acc: 0.898400\n",
      "Epoch  786/2000 train_loss: 0.257880 acc: 0.898100\n",
      "Epoch  787/2000 train_loss: 0.257866 acc: 0.897900\n",
      "Epoch  788/2000 train_loss: 0.257868 acc: 0.898500\n",
      "Epoch  789/2000 train_loss: 0.257857 acc: 0.898600\n",
      "Epoch  790/2000 train_loss: 0.257841 acc: 0.898200\n",
      "Epoch  791/2000 train_loss: 0.257824 acc: 0.898000\n",
      "Epoch  792/2000 train_loss: 0.257834 acc: 0.897900\n",
      "Epoch  793/2000 train_loss: 0.257813 acc: 0.898700\n",
      "Epoch  794/2000 train_loss: 0.257812 acc: 0.898300\n",
      "Epoch  795/2000 train_loss: 0.257789 acc: 0.898100\n",
      "Epoch  796/2000 train_loss: 0.257794 acc: 0.898300\n",
      "Epoch  797/2000 train_loss: 0.257786 acc: 0.898600\n",
      "Epoch  798/2000 train_loss: 0.257774 acc: 0.898500\n",
      "Epoch  799/2000 train_loss: 0.257769 acc: 0.898500\n",
      "Epoch  800/2000 train_loss: 0.257749 acc: 0.898200\n",
      "Epoch  801/2000 train_loss: 0.257753 acc: 0.898200\n",
      "Epoch  802/2000 train_loss: 0.257738 acc: 0.898200\n",
      "Epoch  803/2000 train_loss: 0.257737 acc: 0.899200\n",
      "Epoch  804/2000 train_loss: 0.257728 acc: 0.898500\n",
      "Epoch  805/2000 train_loss: 0.257724 acc: 0.898300\n",
      "Epoch  806/2000 train_loss: 0.257723 acc: 0.898400\n",
      "Epoch  807/2000 train_loss: 0.257705 acc: 0.898800\n",
      "Epoch  808/2000 train_loss: 0.257700 acc: 0.898600\n",
      "Epoch  809/2000 train_loss: 0.257692 acc: 0.898200\n",
      "Epoch  810/2000 train_loss: 0.257689 acc: 0.898700\n",
      "Epoch  811/2000 train_loss: 0.257669 acc: 0.898300\n",
      "Epoch  812/2000 train_loss: 0.257657 acc: 0.898600\n",
      "Epoch  813/2000 train_loss: 0.257658 acc: 0.898700\n",
      "Epoch  814/2000 train_loss: 0.257648 acc: 0.898400\n",
      "Epoch  815/2000 train_loss: 0.257624 acc: 0.898900\n",
      "Epoch  816/2000 train_loss: 0.257644 acc: 0.898700\n",
      "Epoch  817/2000 train_loss: 0.257622 acc: 0.898800\n",
      "Epoch  818/2000 train_loss: 0.257615 acc: 0.898200\n",
      "Epoch  819/2000 train_loss: 0.257613 acc: 0.898600\n",
      "Epoch  820/2000 train_loss: 0.257600 acc: 0.898600\n",
      "Epoch  821/2000 train_loss: 0.257604 acc: 0.898600\n",
      "Epoch  822/2000 train_loss: 0.257586 acc: 0.898500\n",
      "Epoch  823/2000 train_loss: 0.257576 acc: 0.898700\n",
      "Epoch  824/2000 train_loss: 0.257573 acc: 0.898800\n",
      "Epoch  825/2000 train_loss: 0.257569 acc: 0.898600\n",
      "Epoch  826/2000 train_loss: 0.257549 acc: 0.898700\n",
      "Epoch  827/2000 train_loss: 0.257544 acc: 0.898500\n",
      "Epoch  828/2000 train_loss: 0.257544 acc: 0.898600\n",
      "Epoch  829/2000 train_loss: 0.257537 acc: 0.898500\n",
      "Epoch  830/2000 train_loss: 0.257531 acc: 0.898400\n",
      "Epoch  831/2000 train_loss: 0.257523 acc: 0.898700\n",
      "Epoch  832/2000 train_loss: 0.257501 acc: 0.898400\n",
      "Epoch  833/2000 train_loss: 0.257504 acc: 0.898600\n",
      "Epoch  834/2000 train_loss: 0.257501 acc: 0.898600\n",
      "Epoch  835/2000 train_loss: 0.257484 acc: 0.898800\n",
      "Epoch  836/2000 train_loss: 0.257474 acc: 0.898600\n",
      "Epoch  837/2000 train_loss: 0.257473 acc: 0.898500\n",
      "Epoch  838/2000 train_loss: 0.257459 acc: 0.898500\n",
      "Epoch  839/2000 train_loss: 0.257446 acc: 0.898800\n",
      "Epoch  840/2000 train_loss: 0.257451 acc: 0.898700\n",
      "Epoch  841/2000 train_loss: 0.257440 acc: 0.898500\n",
      "Epoch  842/2000 train_loss: 0.257432 acc: 0.898600\n",
      "Epoch  843/2000 train_loss: 0.257415 acc: 0.898600\n",
      "Epoch  844/2000 train_loss: 0.257422 acc: 0.898700\n",
      "Epoch  845/2000 train_loss: 0.257415 acc: 0.898800\n",
      "Epoch  846/2000 train_loss: 0.257405 acc: 0.898800\n",
      "Epoch  847/2000 train_loss: 0.257395 acc: 0.898500\n",
      "Epoch  848/2000 train_loss: 0.257375 acc: 0.898200\n",
      "Epoch  849/2000 train_loss: 0.257379 acc: 0.898800\n",
      "Epoch  850/2000 train_loss: 0.257377 acc: 0.898700\n",
      "Epoch  851/2000 train_loss: 0.257365 acc: 0.898500\n",
      "Epoch  852/2000 train_loss: 0.257358 acc: 0.898400\n",
      "Epoch  853/2000 train_loss: 0.257346 acc: 0.898500\n",
      "Epoch  854/2000 train_loss: 0.257343 acc: 0.898700\n",
      "Epoch  855/2000 train_loss: 0.257333 acc: 0.899000\n",
      "Epoch  856/2000 train_loss: 0.257329 acc: 0.898500\n",
      "Epoch  857/2000 train_loss: 0.257312 acc: 0.898800\n",
      "Epoch  858/2000 train_loss: 0.257310 acc: 0.898300\n",
      "Epoch  859/2000 train_loss: 0.257301 acc: 0.898600\n",
      "Epoch  860/2000 train_loss: 0.257306 acc: 0.898600\n",
      "Epoch  861/2000 train_loss: 0.257293 acc: 0.898600\n",
      "Epoch  862/2000 train_loss: 0.257280 acc: 0.898700\n",
      "Epoch  863/2000 train_loss: 0.257273 acc: 0.898500\n",
      "Epoch  864/2000 train_loss: 0.257262 acc: 0.898500\n",
      "Epoch  865/2000 train_loss: 0.257262 acc: 0.898500\n",
      "Epoch  866/2000 train_loss: 0.257256 acc: 0.898900\n",
      "Epoch  867/2000 train_loss: 0.257253 acc: 0.898300\n",
      "Epoch  868/2000 train_loss: 0.257239 acc: 0.898500\n",
      "Epoch  869/2000 train_loss: 0.257234 acc: 0.898500\n",
      "Epoch  870/2000 train_loss: 0.257201 acc: 0.899100\n",
      "Epoch  871/2000 train_loss: 0.257225 acc: 0.898900\n",
      "Epoch  872/2000 train_loss: 0.257206 acc: 0.899000\n",
      "Epoch  873/2000 train_loss: 0.257190 acc: 0.898700\n",
      "Epoch  874/2000 train_loss: 0.257202 acc: 0.898500\n",
      "Epoch  875/2000 train_loss: 0.257181 acc: 0.898900\n",
      "Epoch  876/2000 train_loss: 0.257186 acc: 0.898300\n",
      "Epoch  877/2000 train_loss: 0.257181 acc: 0.898500\n",
      "Epoch  878/2000 train_loss: 0.257170 acc: 0.898400\n",
      "Epoch  879/2000 train_loss: 0.257158 acc: 0.898800\n",
      "Epoch  880/2000 train_loss: 0.257155 acc: 0.898200\n",
      "Epoch  881/2000 train_loss: 0.257139 acc: 0.898200\n",
      "Epoch  882/2000 train_loss: 0.257137 acc: 0.899100\n",
      "Epoch  883/2000 train_loss: 0.257129 acc: 0.898500\n",
      "Epoch  884/2000 train_loss: 0.257125 acc: 0.898600\n",
      "Epoch  885/2000 train_loss: 0.257091 acc: 0.899000\n",
      "Epoch  886/2000 train_loss: 0.257124 acc: 0.898600\n",
      "Epoch  887/2000 train_loss: 0.257102 acc: 0.898500\n",
      "Epoch  888/2000 train_loss: 0.257102 acc: 0.898800\n",
      "Epoch  889/2000 train_loss: 0.257087 acc: 0.898500\n",
      "Epoch  890/2000 train_loss: 0.257088 acc: 0.898400\n",
      "Epoch  891/2000 train_loss: 0.257074 acc: 0.898300\n",
      "Epoch  892/2000 train_loss: 0.257069 acc: 0.898400\n",
      "Epoch  893/2000 train_loss: 0.257069 acc: 0.898200\n",
      "Epoch  894/2000 train_loss: 0.257053 acc: 0.898600\n",
      "Epoch  895/2000 train_loss: 0.257052 acc: 0.898600\n",
      "Epoch  896/2000 train_loss: 0.257044 acc: 0.898500\n",
      "Epoch  897/2000 train_loss: 0.257033 acc: 0.898600\n",
      "Epoch  898/2000 train_loss: 0.257025 acc: 0.898700\n",
      "Epoch  899/2000 train_loss: 0.257025 acc: 0.898700\n",
      "Epoch  900/2000 train_loss: 0.257011 acc: 0.898500\n",
      "Epoch  901/2000 train_loss: 0.257003 acc: 0.898300\n",
      "Epoch  902/2000 train_loss: 0.257001 acc: 0.898300\n",
      "Epoch  903/2000 train_loss: 0.256992 acc: 0.898600\n",
      "Epoch  904/2000 train_loss: 0.256990 acc: 0.898200\n",
      "Epoch  905/2000 train_loss: 0.256977 acc: 0.898600\n",
      "Epoch  906/2000 train_loss: 0.256973 acc: 0.898700\n",
      "Epoch  907/2000 train_loss: 0.256960 acc: 0.898700\n",
      "Epoch  908/2000 train_loss: 0.256960 acc: 0.898700\n",
      "Epoch  909/2000 train_loss: 0.256951 acc: 0.898700\n",
      "Epoch  910/2000 train_loss: 0.256952 acc: 0.898500\n",
      "Epoch  911/2000 train_loss: 0.256941 acc: 0.898500\n",
      "Epoch  912/2000 train_loss: 0.256937 acc: 0.898400\n",
      "Epoch  913/2000 train_loss: 0.256931 acc: 0.898500\n",
      "Epoch  914/2000 train_loss: 0.256923 acc: 0.898100\n",
      "Epoch  915/2000 train_loss: 0.256913 acc: 0.898800\n",
      "Epoch  916/2000 train_loss: 0.256909 acc: 0.898600\n",
      "Epoch  917/2000 train_loss: 0.256905 acc: 0.898700\n",
      "Epoch  918/2000 train_loss: 0.256896 acc: 0.898400\n",
      "Epoch  919/2000 train_loss: 0.256884 acc: 0.898700\n",
      "Epoch  920/2000 train_loss: 0.256884 acc: 0.898700\n",
      "Epoch  921/2000 train_loss: 0.256872 acc: 0.898600\n",
      "Epoch  922/2000 train_loss: 0.256864 acc: 0.898700\n",
      "Epoch  923/2000 train_loss: 0.256856 acc: 0.898900\n",
      "Epoch  924/2000 train_loss: 0.256857 acc: 0.898600\n",
      "Epoch  925/2000 train_loss: 0.256856 acc: 0.898400\n",
      "Epoch  926/2000 train_loss: 0.256841 acc: 0.898200\n",
      "Epoch  927/2000 train_loss: 0.256837 acc: 0.898800\n",
      "Epoch  928/2000 train_loss: 0.256828 acc: 0.898400\n",
      "Epoch  929/2000 train_loss: 0.256823 acc: 0.898700\n",
      "Epoch  930/2000 train_loss: 0.256808 acc: 0.898500\n",
      "Epoch  931/2000 train_loss: 0.256808 acc: 0.898400\n",
      "Epoch  932/2000 train_loss: 0.256797 acc: 0.898500\n",
      "Epoch  933/2000 train_loss: 0.256800 acc: 0.898600\n",
      "Epoch  934/2000 train_loss: 0.256791 acc: 0.898800\n",
      "Epoch  935/2000 train_loss: 0.256781 acc: 0.898400\n",
      "Epoch  936/2000 train_loss: 0.256782 acc: 0.898700\n",
      "Epoch  937/2000 train_loss: 0.256769 acc: 0.898900\n",
      "Epoch  938/2000 train_loss: 0.256768 acc: 0.898800\n",
      "Epoch  939/2000 train_loss: 0.256752 acc: 0.898600\n",
      "Epoch  940/2000 train_loss: 0.256749 acc: 0.898600\n",
      "Epoch  941/2000 train_loss: 0.256752 acc: 0.898600\n",
      "Epoch  942/2000 train_loss: 0.256735 acc: 0.899400\n",
      "Epoch  943/2000 train_loss: 0.256729 acc: 0.898900\n",
      "Epoch  944/2000 train_loss: 0.256733 acc: 0.898600\n",
      "Epoch  945/2000 train_loss: 0.256712 acc: 0.898900\n",
      "Epoch  946/2000 train_loss: 0.256720 acc: 0.899000\n",
      "Epoch  947/2000 train_loss: 0.256705 acc: 0.898800\n",
      "Epoch  948/2000 train_loss: 0.256705 acc: 0.898800\n",
      "Epoch  949/2000 train_loss: 0.256701 acc: 0.898800\n",
      "Epoch  950/2000 train_loss: 0.256689 acc: 0.898600\n",
      "Epoch  951/2000 train_loss: 0.256690 acc: 0.898600\n",
      "Epoch  952/2000 train_loss: 0.256679 acc: 0.898700\n",
      "Epoch  953/2000 train_loss: 0.256668 acc: 0.898500\n",
      "Epoch  954/2000 train_loss: 0.256661 acc: 0.898900\n",
      "Epoch  955/2000 train_loss: 0.256661 acc: 0.898800\n",
      "Epoch  956/2000 train_loss: 0.256639 acc: 0.898900\n",
      "Epoch  957/2000 train_loss: 0.256638 acc: 0.899100\n",
      "Epoch  958/2000 train_loss: 0.256641 acc: 0.898900\n",
      "Epoch  959/2000 train_loss: 0.256640 acc: 0.898900\n",
      "Epoch  960/2000 train_loss: 0.256627 acc: 0.899000\n",
      "Epoch  961/2000 train_loss: 0.256626 acc: 0.898700\n",
      "Epoch  962/2000 train_loss: 0.256611 acc: 0.899100\n",
      "Epoch  963/2000 train_loss: 0.256602 acc: 0.898500\n",
      "Epoch  964/2000 train_loss: 0.256597 acc: 0.898900\n",
      "Epoch  965/2000 train_loss: 0.256600 acc: 0.899100\n",
      "Epoch  966/2000 train_loss: 0.256594 acc: 0.898900\n",
      "Epoch  967/2000 train_loss: 0.256590 acc: 0.899200\n",
      "Epoch  968/2000 train_loss: 0.256579 acc: 0.898600\n",
      "Epoch  969/2000 train_loss: 0.256566 acc: 0.898900\n",
      "Epoch  970/2000 train_loss: 0.256557 acc: 0.898800\n",
      "Epoch  971/2000 train_loss: 0.256560 acc: 0.898800\n",
      "Epoch  972/2000 train_loss: 0.256555 acc: 0.898800\n",
      "Epoch  973/2000 train_loss: 0.256534 acc: 0.898800\n",
      "Epoch  974/2000 train_loss: 0.256557 acc: 0.899000\n",
      "Epoch  975/2000 train_loss: 0.256540 acc: 0.898800\n",
      "Epoch  976/2000 train_loss: 0.256533 acc: 0.899100\n",
      "Epoch  977/2000 train_loss: 0.256525 acc: 0.899000\n",
      "Epoch  978/2000 train_loss: 0.256509 acc: 0.898900\n",
      "Epoch  979/2000 train_loss: 0.256512 acc: 0.898800\n",
      "Epoch  980/2000 train_loss: 0.256506 acc: 0.898900\n",
      "Epoch  981/2000 train_loss: 0.256497 acc: 0.898900\n",
      "Epoch  982/2000 train_loss: 0.256489 acc: 0.898800\n",
      "Epoch  983/2000 train_loss: 0.256469 acc: 0.898700\n",
      "Epoch  984/2000 train_loss: 0.256474 acc: 0.899400\n",
      "Epoch  985/2000 train_loss: 0.256476 acc: 0.898800\n",
      "Epoch  986/2000 train_loss: 0.256463 acc: 0.899100\n",
      "Epoch  987/2000 train_loss: 0.256464 acc: 0.899200\n",
      "Epoch  988/2000 train_loss: 0.256457 acc: 0.899400\n",
      "Epoch  989/2000 train_loss: 0.256453 acc: 0.899200\n",
      "Epoch  990/2000 train_loss: 0.256450 acc: 0.898900\n",
      "Epoch  991/2000 train_loss: 0.256439 acc: 0.899300\n",
      "Epoch  992/2000 train_loss: 0.256431 acc: 0.898400\n",
      "Epoch  993/2000 train_loss: 0.256423 acc: 0.898800\n",
      "Epoch  994/2000 train_loss: 0.256421 acc: 0.899300\n",
      "Epoch  995/2000 train_loss: 0.256415 acc: 0.899200\n",
      "Epoch  996/2000 train_loss: 0.256407 acc: 0.899000\n",
      "Epoch  997/2000 train_loss: 0.256403 acc: 0.899100\n",
      "Epoch  998/2000 train_loss: 0.256393 acc: 0.899000\n",
      "Epoch  999/2000 train_loss: 0.256397 acc: 0.898800\n",
      "Epoch 1000/2000 train_loss: 0.256393 acc: 0.898900\n",
      "Epoch 1001/2000 train_loss: 0.256383 acc: 0.899200\n",
      "Epoch 1002/2000 train_loss: 0.256375 acc: 0.899200\n",
      "Epoch 1003/2000 train_loss: 0.256365 acc: 0.898500\n",
      "Epoch 1004/2000 train_loss: 0.256368 acc: 0.898800\n",
      "Epoch 1005/2000 train_loss: 0.256360 acc: 0.899300\n",
      "Epoch 1006/2000 train_loss: 0.256358 acc: 0.899000\n",
      "Epoch 1007/2000 train_loss: 0.256345 acc: 0.899100\n",
      "Epoch 1008/2000 train_loss: 0.256347 acc: 0.898900\n",
      "Epoch 1009/2000 train_loss: 0.256327 acc: 0.898900\n",
      "Epoch 1010/2000 train_loss: 0.256336 acc: 0.899300\n",
      "Epoch 1011/2000 train_loss: 0.256326 acc: 0.899100\n",
      "Epoch 1012/2000 train_loss: 0.256322 acc: 0.899400\n",
      "Epoch 1013/2000 train_loss: 0.256319 acc: 0.899100\n",
      "Epoch 1014/2000 train_loss: 0.256313 acc: 0.899100\n",
      "Epoch 1015/2000 train_loss: 0.256304 acc: 0.899300\n",
      "Epoch 1016/2000 train_loss: 0.256296 acc: 0.899600\n",
      "Epoch 1017/2000 train_loss: 0.256293 acc: 0.899200\n",
      "Epoch 1018/2000 train_loss: 0.256274 acc: 0.898900\n",
      "Epoch 1019/2000 train_loss: 0.256282 acc: 0.899400\n",
      "Epoch 1020/2000 train_loss: 0.256269 acc: 0.898900\n",
      "Epoch 1021/2000 train_loss: 0.256248 acc: 0.899600\n",
      "Epoch 1022/2000 train_loss: 0.256270 acc: 0.899200\n",
      "Epoch 1023/2000 train_loss: 0.256264 acc: 0.899100\n",
      "Epoch 1024/2000 train_loss: 0.256234 acc: 0.899300\n",
      "Epoch 1025/2000 train_loss: 0.256248 acc: 0.899000\n",
      "Epoch 1026/2000 train_loss: 0.256240 acc: 0.899200\n",
      "Epoch 1027/2000 train_loss: 0.256223 acc: 0.899300\n",
      "Epoch 1028/2000 train_loss: 0.256232 acc: 0.899300\n",
      "Epoch 1029/2000 train_loss: 0.256226 acc: 0.899500\n",
      "Epoch 1030/2000 train_loss: 0.256214 acc: 0.899300\n",
      "Epoch 1031/2000 train_loss: 0.256212 acc: 0.899600\n",
      "Epoch 1032/2000 train_loss: 0.256214 acc: 0.899100\n",
      "Epoch 1033/2000 train_loss: 0.256186 acc: 0.899000\n",
      "Epoch 1034/2000 train_loss: 0.256188 acc: 0.899700\n",
      "Epoch 1035/2000 train_loss: 0.256193 acc: 0.899500\n",
      "Epoch 1036/2000 train_loss: 0.256182 acc: 0.899200\n",
      "Epoch 1037/2000 train_loss: 0.256168 acc: 0.899300\n",
      "Epoch 1038/2000 train_loss: 0.256173 acc: 0.899900\n",
      "Epoch 1039/2000 train_loss: 0.256172 acc: 0.899400\n",
      "Epoch 1040/2000 train_loss: 0.256158 acc: 0.899600\n",
      "Epoch 1041/2000 train_loss: 0.256153 acc: 0.899200\n",
      "Epoch 1042/2000 train_loss: 0.256143 acc: 0.899700\n",
      "Epoch 1043/2000 train_loss: 0.256150 acc: 0.899000\n",
      "Epoch 1044/2000 train_loss: 0.256143 acc: 0.899300\n",
      "Epoch 1045/2000 train_loss: 0.256130 acc: 0.899100\n",
      "Epoch 1046/2000 train_loss: 0.256116 acc: 0.899500\n",
      "Epoch 1047/2000 train_loss: 0.256128 acc: 0.899900\n",
      "Epoch 1048/2000 train_loss: 0.256118 acc: 0.899500\n",
      "Epoch 1049/2000 train_loss: 0.256105 acc: 0.899200\n",
      "Epoch 1050/2000 train_loss: 0.256110 acc: 0.899600\n",
      "Epoch 1051/2000 train_loss: 0.256108 acc: 0.899200\n",
      "Epoch 1052/2000 train_loss: 0.256083 acc: 0.899200\n",
      "Epoch 1053/2000 train_loss: 0.256094 acc: 0.899600\n",
      "Epoch 1054/2000 train_loss: 0.256085 acc: 0.899300\n",
      "Epoch 1055/2000 train_loss: 0.256090 acc: 0.899300\n",
      "Epoch 1056/2000 train_loss: 0.256078 acc: 0.899700\n",
      "Epoch 1057/2000 train_loss: 0.256067 acc: 0.899600\n",
      "Epoch 1058/2000 train_loss: 0.256047 acc: 0.899100\n",
      "Epoch 1059/2000 train_loss: 0.256066 acc: 0.899500\n",
      "Epoch 1060/2000 train_loss: 0.256049 acc: 0.899600\n",
      "Epoch 1061/2000 train_loss: 0.256044 acc: 0.899400\n",
      "Epoch 1062/2000 train_loss: 0.256042 acc: 0.899400\n",
      "Epoch 1063/2000 train_loss: 0.256040 acc: 0.899400\n",
      "Epoch 1064/2000 train_loss: 0.256028 acc: 0.899900\n",
      "Epoch 1065/2000 train_loss: 0.256032 acc: 0.899500\n",
      "Epoch 1066/2000 train_loss: 0.256021 acc: 0.899800\n",
      "Epoch 1067/2000 train_loss: 0.256019 acc: 0.899500\n",
      "Epoch 1068/2000 train_loss: 0.256006 acc: 0.899300\n",
      "Epoch 1069/2000 train_loss: 0.256010 acc: 0.899400\n",
      "Epoch 1070/2000 train_loss: 0.256005 acc: 0.899500\n",
      "Epoch 1071/2000 train_loss: 0.255995 acc: 0.899700\n",
      "Epoch 1072/2000 train_loss: 0.255994 acc: 0.899300\n",
      "Epoch 1073/2000 train_loss: 0.255987 acc: 0.899600\n",
      "Epoch 1074/2000 train_loss: 0.255974 acc: 0.899600\n",
      "Epoch 1075/2000 train_loss: 0.255976 acc: 0.899200\n",
      "Epoch 1076/2000 train_loss: 0.255969 acc: 0.899300\n",
      "Epoch 1077/2000 train_loss: 0.255965 acc: 0.899400\n",
      "Epoch 1078/2000 train_loss: 0.255956 acc: 0.899400\n",
      "Epoch 1079/2000 train_loss: 0.255933 acc: 0.899000\n",
      "Epoch 1080/2000 train_loss: 0.255955 acc: 0.899600\n",
      "Epoch 1081/2000 train_loss: 0.255931 acc: 0.899100\n",
      "Epoch 1082/2000 train_loss: 0.255936 acc: 0.899300\n",
      "Epoch 1083/2000 train_loss: 0.255930 acc: 0.899300\n",
      "Epoch 1084/2000 train_loss: 0.255930 acc: 0.899800\n",
      "Epoch 1085/2000 train_loss: 0.255927 acc: 0.899500\n",
      "Epoch 1086/2000 train_loss: 0.255905 acc: 0.900100\n",
      "Epoch 1087/2000 train_loss: 0.255918 acc: 0.899500\n",
      "Epoch 1088/2000 train_loss: 0.255907 acc: 0.899600\n",
      "Epoch 1089/2000 train_loss: 0.255899 acc: 0.899200\n",
      "Epoch 1090/2000 train_loss: 0.255902 acc: 0.899800\n",
      "Epoch 1091/2000 train_loss: 0.255895 acc: 0.899500\n",
      "Epoch 1092/2000 train_loss: 0.255881 acc: 0.899200\n",
      "Epoch 1093/2000 train_loss: 0.255875 acc: 0.899700\n",
      "Epoch 1094/2000 train_loss: 0.255872 acc: 0.900000\n",
      "Epoch 1095/2000 train_loss: 0.255862 acc: 0.899900\n",
      "Epoch 1096/2000 train_loss: 0.255865 acc: 0.899500\n",
      "Epoch 1097/2000 train_loss: 0.255857 acc: 0.899700\n",
      "Epoch 1098/2000 train_loss: 0.255861 acc: 0.899300\n",
      "Epoch 1099/2000 train_loss: 0.255848 acc: 0.899900\n",
      "Epoch 1100/2000 train_loss: 0.255858 acc: 0.899600\n",
      "Epoch 1101/2000 train_loss: 0.255837 acc: 0.899700\n",
      "Epoch 1102/2000 train_loss: 0.255838 acc: 0.900000\n",
      "Epoch 1103/2000 train_loss: 0.255825 acc: 0.899300\n",
      "Epoch 1104/2000 train_loss: 0.255840 acc: 0.899600\n",
      "Epoch 1105/2000 train_loss: 0.255811 acc: 0.899700\n",
      "Epoch 1106/2000 train_loss: 0.255822 acc: 0.899400\n",
      "Epoch 1107/2000 train_loss: 0.255800 acc: 0.899200\n",
      "Epoch 1108/2000 train_loss: 0.255810 acc: 0.900300\n",
      "Epoch 1109/2000 train_loss: 0.255802 acc: 0.899200\n",
      "Epoch 1110/2000 train_loss: 0.255800 acc: 0.899700\n",
      "Epoch 1111/2000 train_loss: 0.255790 acc: 0.899500\n",
      "Epoch 1112/2000 train_loss: 0.255787 acc: 0.899600\n",
      "Epoch 1113/2000 train_loss: 0.255780 acc: 0.899700\n",
      "Epoch 1114/2000 train_loss: 0.255780 acc: 0.899900\n",
      "Epoch 1115/2000 train_loss: 0.255776 acc: 0.899700\n",
      "Epoch 1116/2000 train_loss: 0.255768 acc: 0.899700\n",
      "Epoch 1117/2000 train_loss: 0.255769 acc: 0.899900\n",
      "Epoch 1118/2000 train_loss: 0.255758 acc: 0.900100\n",
      "Epoch 1119/2000 train_loss: 0.255748 acc: 0.899500\n",
      "Epoch 1120/2000 train_loss: 0.255753 acc: 0.899600\n",
      "Epoch 1121/2000 train_loss: 0.255747 acc: 0.899500\n",
      "Epoch 1122/2000 train_loss: 0.255739 acc: 0.899500\n",
      "Epoch 1123/2000 train_loss: 0.255730 acc: 0.899000\n",
      "Epoch 1124/2000 train_loss: 0.255723 acc: 0.900200\n",
      "Epoch 1125/2000 train_loss: 0.255721 acc: 0.899600\n",
      "Epoch 1126/2000 train_loss: 0.255715 acc: 0.899700\n",
      "Epoch 1127/2000 train_loss: 0.255711 acc: 0.899700\n",
      "Epoch 1128/2000 train_loss: 0.255704 acc: 0.899300\n",
      "Epoch 1129/2000 train_loss: 0.255699 acc: 0.899700\n",
      "Epoch 1130/2000 train_loss: 0.255677 acc: 0.899300\n",
      "Epoch 1131/2000 train_loss: 0.255686 acc: 0.899000\n",
      "Epoch 1132/2000 train_loss: 0.255696 acc: 0.899800\n",
      "Epoch 1133/2000 train_loss: 0.255683 acc: 0.900200\n",
      "Epoch 1134/2000 train_loss: 0.255677 acc: 0.899900\n",
      "Epoch 1135/2000 train_loss: 0.255683 acc: 0.899800\n",
      "Epoch 1136/2000 train_loss: 0.255668 acc: 0.899500\n",
      "Epoch 1137/2000 train_loss: 0.255660 acc: 0.899700\n",
      "Epoch 1138/2000 train_loss: 0.255666 acc: 0.899500\n",
      "Epoch 1139/2000 train_loss: 0.255658 acc: 0.899700\n",
      "Epoch 1140/2000 train_loss: 0.255655 acc: 0.899500\n",
      "Epoch 1141/2000 train_loss: 0.255629 acc: 0.899300\n",
      "Epoch 1142/2000 train_loss: 0.255637 acc: 0.899500\n",
      "Epoch 1143/2000 train_loss: 0.255639 acc: 0.899800\n",
      "Epoch 1144/2000 train_loss: 0.255629 acc: 0.899800\n",
      "Epoch 1145/2000 train_loss: 0.255622 acc: 0.899600\n",
      "Epoch 1146/2000 train_loss: 0.255616 acc: 0.899700\n",
      "Epoch 1147/2000 train_loss: 0.255617 acc: 0.899500\n",
      "Epoch 1148/2000 train_loss: 0.255616 acc: 0.899700\n",
      "Epoch 1149/2000 train_loss: 0.255601 acc: 0.899700\n",
      "Epoch 1150/2000 train_loss: 0.255602 acc: 0.899900\n",
      "Epoch 1151/2000 train_loss: 0.255605 acc: 0.899900\n",
      "Epoch 1152/2000 train_loss: 0.255593 acc: 0.899700\n",
      "Epoch 1153/2000 train_loss: 0.255585 acc: 0.899900\n",
      "Epoch 1154/2000 train_loss: 0.255583 acc: 0.899700\n",
      "Epoch 1155/2000 train_loss: 0.255583 acc: 0.899800\n",
      "Epoch 1156/2000 train_loss: 0.255581 acc: 0.899500\n",
      "Epoch 1157/2000 train_loss: 0.255577 acc: 0.899600\n",
      "Epoch 1158/2000 train_loss: 0.255562 acc: 0.899900\n",
      "Epoch 1159/2000 train_loss: 0.255563 acc: 0.899800\n",
      "Epoch 1160/2000 train_loss: 0.255565 acc: 0.899700\n",
      "Epoch 1161/2000 train_loss: 0.255552 acc: 0.899600\n",
      "Epoch 1162/2000 train_loss: 0.255557 acc: 0.899900\n",
      "Epoch 1163/2000 train_loss: 0.255549 acc: 0.899400\n",
      "Epoch 1164/2000 train_loss: 0.255539 acc: 0.899600\n",
      "Epoch 1165/2000 train_loss: 0.255535 acc: 0.899800\n",
      "Epoch 1166/2000 train_loss: 0.255519 acc: 0.899900\n",
      "Epoch 1167/2000 train_loss: 0.255517 acc: 0.899700\n",
      "Epoch 1168/2000 train_loss: 0.255518 acc: 0.899800\n",
      "Epoch 1169/2000 train_loss: 0.255524 acc: 0.899700\n",
      "Epoch 1170/2000 train_loss: 0.255510 acc: 0.899700\n",
      "Epoch 1171/2000 train_loss: 0.255512 acc: 0.899800\n",
      "Epoch 1172/2000 train_loss: 0.255495 acc: 0.899800\n",
      "Epoch 1173/2000 train_loss: 0.255498 acc: 0.899900\n",
      "Epoch 1174/2000 train_loss: 0.255494 acc: 0.899400\n",
      "Epoch 1175/2000 train_loss: 0.255493 acc: 0.900000\n",
      "Epoch 1176/2000 train_loss: 0.255488 acc: 0.899800\n",
      "Epoch 1177/2000 train_loss: 0.255474 acc: 0.900500\n",
      "Epoch 1178/2000 train_loss: 0.255475 acc: 0.900100\n",
      "Epoch 1179/2000 train_loss: 0.255465 acc: 0.899500\n",
      "Epoch 1180/2000 train_loss: 0.255474 acc: 0.899600\n",
      "Epoch 1181/2000 train_loss: 0.255458 acc: 0.899500\n",
      "Epoch 1182/2000 train_loss: 0.255457 acc: 0.899500\n",
      "Epoch 1183/2000 train_loss: 0.255446 acc: 0.899800\n",
      "Epoch 1184/2000 train_loss: 0.255451 acc: 0.899400\n",
      "Epoch 1185/2000 train_loss: 0.255440 acc: 0.899600\n",
      "Epoch 1186/2000 train_loss: 0.255439 acc: 0.899100\n",
      "Epoch 1187/2000 train_loss: 0.255424 acc: 0.900500\n",
      "Epoch 1188/2000 train_loss: 0.255435 acc: 0.899500\n",
      "Epoch 1189/2000 train_loss: 0.255433 acc: 0.899400\n",
      "Epoch 1190/2000 train_loss: 0.255422 acc: 0.899800\n",
      "Epoch 1191/2000 train_loss: 0.255423 acc: 0.899500\n",
      "Epoch 1192/2000 train_loss: 0.255412 acc: 0.899300\n",
      "Epoch 1193/2000 train_loss: 0.255404 acc: 0.899400\n",
      "Epoch 1194/2000 train_loss: 0.255409 acc: 0.899600\n",
      "Epoch 1195/2000 train_loss: 0.255402 acc: 0.900100\n",
      "Epoch 1196/2000 train_loss: 0.255397 acc: 0.899800\n",
      "Epoch 1197/2000 train_loss: 0.255393 acc: 0.899700\n",
      "Epoch 1198/2000 train_loss: 0.255386 acc: 0.899500\n",
      "Epoch 1199/2000 train_loss: 0.255378 acc: 0.899800\n",
      "Epoch 1200/2000 train_loss: 0.255371 acc: 0.899800\n",
      "Epoch 1201/2000 train_loss: 0.255366 acc: 0.899900\n",
      "Epoch 1202/2000 train_loss: 0.255364 acc: 0.899900\n",
      "Epoch 1203/2000 train_loss: 0.255362 acc: 0.900100\n",
      "Epoch 1204/2000 train_loss: 0.255361 acc: 0.899700\n",
      "Epoch 1205/2000 train_loss: 0.255354 acc: 0.899500\n",
      "Epoch 1206/2000 train_loss: 0.255352 acc: 0.899400\n",
      "Epoch 1207/2000 train_loss: 0.255351 acc: 0.899800\n",
      "Epoch 1208/2000 train_loss: 0.255339 acc: 0.899600\n",
      "Epoch 1209/2000 train_loss: 0.255325 acc: 0.899500\n",
      "Epoch 1210/2000 train_loss: 0.255340 acc: 0.899800\n",
      "Epoch 1211/2000 train_loss: 0.255332 acc: 0.899800\n",
      "Epoch 1212/2000 train_loss: 0.255327 acc: 0.900000\n",
      "Epoch 1213/2000 train_loss: 0.255318 acc: 0.899500\n",
      "Epoch 1214/2000 train_loss: 0.255314 acc: 0.899600\n",
      "Epoch 1215/2000 train_loss: 0.255311 acc: 0.899700\n",
      "Epoch 1216/2000 train_loss: 0.255309 acc: 0.899600\n",
      "Epoch 1217/2000 train_loss: 0.255306 acc: 0.899400\n",
      "Epoch 1218/2000 train_loss: 0.255306 acc: 0.899600\n",
      "Epoch 1219/2000 train_loss: 0.255299 acc: 0.899600\n",
      "Epoch 1220/2000 train_loss: 0.255284 acc: 0.899100\n",
      "Epoch 1221/2000 train_loss: 0.255291 acc: 0.899800\n",
      "Epoch 1222/2000 train_loss: 0.255284 acc: 0.899600\n",
      "Epoch 1223/2000 train_loss: 0.255277 acc: 0.899700\n",
      "Epoch 1224/2000 train_loss: 0.255275 acc: 0.900100\n",
      "Epoch 1225/2000 train_loss: 0.255278 acc: 0.899600\n",
      "Epoch 1226/2000 train_loss: 0.255260 acc: 0.899700\n",
      "Epoch 1227/2000 train_loss: 0.255267 acc: 0.899600\n",
      "Epoch 1228/2000 train_loss: 0.255256 acc: 0.899800\n",
      "Epoch 1229/2000 train_loss: 0.255246 acc: 0.899800\n",
      "Epoch 1230/2000 train_loss: 0.255249 acc: 0.899600\n",
      "Epoch 1231/2000 train_loss: 0.255250 acc: 0.899900\n",
      "Epoch 1232/2000 train_loss: 0.255237 acc: 0.900100\n",
      "Epoch 1233/2000 train_loss: 0.255233 acc: 0.899500\n",
      "Epoch 1234/2000 train_loss: 0.255237 acc: 0.899700\n",
      "Epoch 1235/2000 train_loss: 0.255230 acc: 0.900000\n",
      "Epoch 1236/2000 train_loss: 0.255234 acc: 0.899800\n",
      "Epoch 1237/2000 train_loss: 0.255221 acc: 0.899400\n",
      "Epoch 1238/2000 train_loss: 0.255222 acc: 0.899900\n",
      "Epoch 1239/2000 train_loss: 0.255211 acc: 0.899700\n",
      "Epoch 1240/2000 train_loss: 0.255210 acc: 0.900300\n",
      "Epoch 1241/2000 train_loss: 0.255199 acc: 0.899700\n",
      "Epoch 1242/2000 train_loss: 0.255207 acc: 0.899300\n",
      "Epoch 1243/2000 train_loss: 0.255201 acc: 0.899900\n",
      "Epoch 1244/2000 train_loss: 0.255175 acc: 0.899500\n",
      "Epoch 1245/2000 train_loss: 0.255184 acc: 0.899600\n",
      "Epoch 1246/2000 train_loss: 0.255172 acc: 0.899700\n",
      "Epoch 1247/2000 train_loss: 0.255172 acc: 0.899700\n",
      "Epoch 1248/2000 train_loss: 0.255182 acc: 0.899500\n",
      "Epoch 1249/2000 train_loss: 0.255173 acc: 0.899600\n",
      "Epoch 1250/2000 train_loss: 0.255174 acc: 0.899900\n",
      "Epoch 1251/2000 train_loss: 0.255164 acc: 0.899600\n",
      "Epoch 1252/2000 train_loss: 0.255165 acc: 0.899800\n",
      "Epoch 1253/2000 train_loss: 0.255155 acc: 0.899500\n",
      "Epoch 1254/2000 train_loss: 0.255140 acc: 0.899000\n",
      "Epoch 1255/2000 train_loss: 0.255142 acc: 0.899800\n",
      "Epoch 1256/2000 train_loss: 0.255116 acc: 0.899300\n",
      "Epoch 1257/2000 train_loss: 0.255134 acc: 0.899600\n",
      "Epoch 1258/2000 train_loss: 0.255129 acc: 0.899600\n",
      "Epoch 1259/2000 train_loss: 0.255135 acc: 0.899300\n",
      "Epoch 1260/2000 train_loss: 0.255127 acc: 0.899500\n",
      "Epoch 1261/2000 train_loss: 0.255125 acc: 0.899600\n",
      "Epoch 1262/2000 train_loss: 0.255122 acc: 0.899500\n",
      "Epoch 1263/2000 train_loss: 0.255104 acc: 0.899300\n",
      "Epoch 1264/2000 train_loss: 0.255098 acc: 0.900000\n",
      "Epoch 1265/2000 train_loss: 0.255102 acc: 0.899800\n",
      "Epoch 1266/2000 train_loss: 0.255102 acc: 0.899800\n",
      "Epoch 1267/2000 train_loss: 0.255093 acc: 0.899400\n",
      "Epoch 1268/2000 train_loss: 0.255093 acc: 0.899800\n",
      "Epoch 1269/2000 train_loss: 0.255090 acc: 0.899500\n",
      "Epoch 1270/2000 train_loss: 0.255086 acc: 0.899500\n",
      "Epoch 1271/2000 train_loss: 0.255075 acc: 0.899700\n",
      "Epoch 1272/2000 train_loss: 0.255083 acc: 0.899500\n",
      "Epoch 1273/2000 train_loss: 0.255059 acc: 0.900100\n",
      "Epoch 1274/2000 train_loss: 0.255070 acc: 0.900000\n",
      "Epoch 1275/2000 train_loss: 0.255067 acc: 0.899700\n",
      "Epoch 1276/2000 train_loss: 0.255055 acc: 0.899600\n",
      "Epoch 1277/2000 train_loss: 0.255038 acc: 0.899800\n",
      "Epoch 1278/2000 train_loss: 0.255050 acc: 0.899500\n",
      "Epoch 1279/2000 train_loss: 0.255044 acc: 0.899300\n",
      "Epoch 1280/2000 train_loss: 0.255042 acc: 0.899800\n",
      "Epoch 1281/2000 train_loss: 0.255034 acc: 0.899900\n",
      "Epoch 1282/2000 train_loss: 0.255043 acc: 0.899600\n",
      "Epoch 1283/2000 train_loss: 0.255024 acc: 0.899700\n",
      "Epoch 1284/2000 train_loss: 0.255034 acc: 0.899700\n",
      "Epoch 1285/2000 train_loss: 0.255012 acc: 0.899400\n",
      "Epoch 1286/2000 train_loss: 0.255016 acc: 0.899200\n",
      "Epoch 1287/2000 train_loss: 0.255015 acc: 0.899600\n",
      "Epoch 1288/2000 train_loss: 0.255011 acc: 0.899400\n",
      "Epoch 1289/2000 train_loss: 0.255003 acc: 0.899600\n",
      "Epoch 1290/2000 train_loss: 0.255004 acc: 0.899800\n",
      "Epoch 1291/2000 train_loss: 0.255003 acc: 0.899800\n",
      "Epoch 1292/2000 train_loss: 0.254997 acc: 0.899800\n",
      "Epoch 1293/2000 train_loss: 0.254981 acc: 0.899700\n",
      "Epoch 1294/2000 train_loss: 0.254985 acc: 0.899800\n",
      "Epoch 1295/2000 train_loss: 0.254990 acc: 0.899800\n",
      "Epoch 1296/2000 train_loss: 0.254976 acc: 0.899800\n",
      "Epoch 1297/2000 train_loss: 0.254977 acc: 0.899800\n",
      "Epoch 1298/2000 train_loss: 0.254971 acc: 0.899500\n",
      "Epoch 1299/2000 train_loss: 0.254976 acc: 0.900000\n",
      "Epoch 1300/2000 train_loss: 0.254968 acc: 0.899600\n",
      "Epoch 1301/2000 train_loss: 0.254964 acc: 0.899700\n",
      "Epoch 1302/2000 train_loss: 0.254950 acc: 0.899500\n",
      "Epoch 1303/2000 train_loss: 0.254956 acc: 0.899700\n",
      "Epoch 1304/2000 train_loss: 0.254939 acc: 0.899500\n",
      "Epoch 1305/2000 train_loss: 0.254951 acc: 0.899800\n",
      "Epoch 1306/2000 train_loss: 0.254947 acc: 0.899600\n",
      "Epoch 1307/2000 train_loss: 0.254943 acc: 0.899400\n",
      "Epoch 1308/2000 train_loss: 0.254929 acc: 0.899400\n",
      "Epoch 1309/2000 train_loss: 0.254936 acc: 0.899700\n",
      "Epoch 1310/2000 train_loss: 0.254929 acc: 0.899800\n",
      "Epoch 1311/2000 train_loss: 0.254909 acc: 0.899400\n",
      "Epoch 1312/2000 train_loss: 0.254931 acc: 0.899800\n",
      "Epoch 1313/2000 train_loss: 0.254915 acc: 0.899700\n",
      "Epoch 1314/2000 train_loss: 0.254907 acc: 0.899400\n",
      "Epoch 1315/2000 train_loss: 0.254896 acc: 0.899200\n",
      "Epoch 1316/2000 train_loss: 0.254892 acc: 0.900300\n",
      "Epoch 1317/2000 train_loss: 0.254899 acc: 0.899700\n",
      "Epoch 1318/2000 train_loss: 0.254899 acc: 0.899200\n",
      "Epoch 1319/2000 train_loss: 0.254894 acc: 0.899600\n",
      "Epoch 1320/2000 train_loss: 0.254895 acc: 0.899500\n",
      "Epoch 1321/2000 train_loss: 0.254887 acc: 0.899600\n",
      "Epoch 1322/2000 train_loss: 0.254880 acc: 0.899200\n",
      "Epoch 1323/2000 train_loss: 0.254882 acc: 0.899900\n",
      "Epoch 1324/2000 train_loss: 0.254871 acc: 0.899400\n",
      "Epoch 1325/2000 train_loss: 0.254877 acc: 0.899700\n",
      "Epoch 1326/2000 train_loss: 0.254860 acc: 0.900100\n",
      "Epoch 1327/2000 train_loss: 0.254863 acc: 0.899800\n",
      "Epoch 1328/2000 train_loss: 0.254851 acc: 0.899600\n",
      "Epoch 1329/2000 train_loss: 0.254841 acc: 0.899800\n",
      "Epoch 1330/2000 train_loss: 0.254857 acc: 0.899700\n",
      "Epoch 1331/2000 train_loss: 0.254836 acc: 0.899800\n",
      "Epoch 1332/2000 train_loss: 0.254842 acc: 0.899900\n",
      "Epoch 1333/2000 train_loss: 0.254832 acc: 0.899500\n",
      "Epoch 1334/2000 train_loss: 0.254837 acc: 0.899600\n",
      "Epoch 1335/2000 train_loss: 0.254825 acc: 0.899500\n",
      "Epoch 1336/2000 train_loss: 0.254824 acc: 0.899400\n",
      "Epoch 1337/2000 train_loss: 0.254834 acc: 0.900000\n",
      "Epoch 1338/2000 train_loss: 0.254824 acc: 0.899200\n",
      "Epoch 1339/2000 train_loss: 0.254821 acc: 0.899800\n",
      "Epoch 1340/2000 train_loss: 0.254808 acc: 0.899300\n",
      "Epoch 1341/2000 train_loss: 0.254814 acc: 0.899700\n",
      "Epoch 1342/2000 train_loss: 0.254803 acc: 0.899700\n",
      "Epoch 1343/2000 train_loss: 0.254807 acc: 0.899800\n",
      "Epoch 1344/2000 train_loss: 0.254804 acc: 0.899900\n",
      "Epoch 1345/2000 train_loss: 0.254801 acc: 0.899300\n",
      "Epoch 1346/2000 train_loss: 0.254790 acc: 0.900000\n",
      "Epoch 1347/2000 train_loss: 0.254794 acc: 0.899600\n",
      "Epoch 1348/2000 train_loss: 0.254785 acc: 0.899100\n",
      "Epoch 1349/2000 train_loss: 0.254788 acc: 0.899400\n",
      "Epoch 1350/2000 train_loss: 0.254779 acc: 0.899100\n",
      "Epoch 1351/2000 train_loss: 0.254773 acc: 0.899600\n",
      "Epoch 1352/2000 train_loss: 0.254770 acc: 0.899600\n",
      "Epoch 1353/2000 train_loss: 0.254771 acc: 0.899700\n",
      "Epoch 1354/2000 train_loss: 0.254761 acc: 0.899700\n",
      "Epoch 1355/2000 train_loss: 0.254759 acc: 0.899200\n",
      "Epoch 1356/2000 train_loss: 0.254759 acc: 0.899600\n",
      "Epoch 1357/2000 train_loss: 0.254739 acc: 0.899400\n",
      "Epoch 1358/2000 train_loss: 0.254754 acc: 0.899300\n",
      "Epoch 1359/2000 train_loss: 0.254740 acc: 0.899900\n",
      "Epoch 1360/2000 train_loss: 0.254734 acc: 0.900000\n",
      "Epoch 1361/2000 train_loss: 0.254736 acc: 0.899400\n",
      "Epoch 1362/2000 train_loss: 0.254721 acc: 0.899600\n",
      "Epoch 1363/2000 train_loss: 0.254744 acc: 0.899600\n",
      "Epoch 1364/2000 train_loss: 0.254730 acc: 0.899700\n",
      "Epoch 1365/2000 train_loss: 0.254722 acc: 0.899100\n",
      "Epoch 1366/2000 train_loss: 0.254716 acc: 0.899300\n",
      "Epoch 1367/2000 train_loss: 0.254717 acc: 0.899600\n",
      "Epoch 1368/2000 train_loss: 0.254709 acc: 0.899400\n",
      "Epoch 1369/2000 train_loss: 0.254710 acc: 0.899800\n",
      "Epoch 1370/2000 train_loss: 0.254690 acc: 0.899300\n",
      "Epoch 1371/2000 train_loss: 0.254703 acc: 0.899900\n",
      "Epoch 1372/2000 train_loss: 0.254694 acc: 0.899600\n",
      "Epoch 1373/2000 train_loss: 0.254700 acc: 0.899600\n",
      "Epoch 1374/2000 train_loss: 0.254695 acc: 0.899700\n",
      "Epoch 1375/2000 train_loss: 0.254694 acc: 0.899700\n",
      "Epoch 1376/2000 train_loss: 0.254673 acc: 0.899700\n",
      "Epoch 1377/2000 train_loss: 0.254686 acc: 0.899400\n",
      "Epoch 1378/2000 train_loss: 0.254681 acc: 0.899500\n",
      "Epoch 1379/2000 train_loss: 0.254677 acc: 0.899500\n",
      "Epoch 1380/2000 train_loss: 0.254668 acc: 0.899800\n",
      "Epoch 1381/2000 train_loss: 0.254668 acc: 0.899900\n",
      "Epoch 1382/2000 train_loss: 0.254658 acc: 0.899800\n",
      "Epoch 1383/2000 train_loss: 0.254654 acc: 0.899400\n",
      "Epoch 1384/2000 train_loss: 0.254655 acc: 0.899800\n",
      "Epoch 1385/2000 train_loss: 0.254628 acc: 0.899500\n",
      "Epoch 1386/2000 train_loss: 0.254656 acc: 0.899800\n",
      "Epoch 1387/2000 train_loss: 0.254644 acc: 0.899500\n",
      "Epoch 1388/2000 train_loss: 0.254640 acc: 0.899300\n",
      "Epoch 1389/2000 train_loss: 0.254626 acc: 0.899400\n",
      "Epoch 1390/2000 train_loss: 0.254633 acc: 0.899600\n",
      "Epoch 1391/2000 train_loss: 0.254629 acc: 0.899700\n",
      "Epoch 1392/2000 train_loss: 0.254624 acc: 0.899500\n",
      "Epoch 1393/2000 train_loss: 0.254631 acc: 0.899300\n",
      "Epoch 1394/2000 train_loss: 0.254624 acc: 0.899700\n",
      "Epoch 1395/2000 train_loss: 0.254614 acc: 0.899200\n",
      "Epoch 1396/2000 train_loss: 0.254606 acc: 0.899500\n",
      "Epoch 1397/2000 train_loss: 0.254613 acc: 0.899800\n",
      "Epoch 1398/2000 train_loss: 0.254606 acc: 0.899500\n",
      "Epoch 1399/2000 train_loss: 0.254607 acc: 0.899300\n",
      "Epoch 1400/2000 train_loss: 0.254601 acc: 0.899200\n",
      "Epoch 1401/2000 train_loss: 0.254593 acc: 0.899700\n",
      "Epoch 1402/2000 train_loss: 0.254592 acc: 0.899400\n",
      "Epoch 1403/2000 train_loss: 0.254589 acc: 0.899600\n",
      "Epoch 1404/2000 train_loss: 0.254587 acc: 0.899700\n",
      "Epoch 1405/2000 train_loss: 0.254579 acc: 0.899400\n",
      "Epoch 1406/2000 train_loss: 0.254581 acc: 0.899500\n",
      "Epoch 1407/2000 train_loss: 0.254572 acc: 0.899500\n",
      "Epoch 1408/2000 train_loss: 0.254575 acc: 0.899400\n",
      "Epoch 1409/2000 train_loss: 0.254569 acc: 0.899600\n",
      "Epoch 1410/2000 train_loss: 0.254564 acc: 0.899700\n",
      "Epoch 1411/2000 train_loss: 0.254562 acc: 0.899300\n",
      "Epoch 1412/2000 train_loss: 0.254560 acc: 0.899900\n",
      "Epoch 1413/2000 train_loss: 0.254555 acc: 0.899400\n",
      "Epoch 1414/2000 train_loss: 0.254550 acc: 0.899400\n",
      "Epoch 1415/2000 train_loss: 0.254537 acc: 0.899700\n",
      "Epoch 1416/2000 train_loss: 0.254549 acc: 0.899700\n",
      "Epoch 1417/2000 train_loss: 0.254539 acc: 0.899800\n",
      "Epoch 1418/2000 train_loss: 0.254550 acc: 0.899700\n",
      "Epoch 1419/2000 train_loss: 0.254539 acc: 0.899100\n",
      "Epoch 1420/2000 train_loss: 0.254528 acc: 0.899500\n",
      "Epoch 1421/2000 train_loss: 0.254532 acc: 0.899900\n",
      "Epoch 1422/2000 train_loss: 0.254523 acc: 0.899300\n",
      "Epoch 1423/2000 train_loss: 0.254521 acc: 0.899500\n",
      "Epoch 1424/2000 train_loss: 0.254514 acc: 0.899500\n",
      "Epoch 1425/2000 train_loss: 0.254518 acc: 0.899600\n",
      "Epoch 1426/2000 train_loss: 0.254516 acc: 0.899800\n",
      "Epoch 1427/2000 train_loss: 0.254508 acc: 0.899400\n",
      "Epoch 1428/2000 train_loss: 0.254498 acc: 0.899800\n",
      "Epoch 1429/2000 train_loss: 0.254502 acc: 0.899600\n",
      "Epoch 1430/2000 train_loss: 0.254501 acc: 0.899600\n",
      "Epoch 1431/2000 train_loss: 0.254490 acc: 0.899800\n",
      "Epoch 1432/2000 train_loss: 0.254487 acc: 0.899500\n",
      "Epoch 1433/2000 train_loss: 0.254477 acc: 0.899600\n",
      "Epoch 1434/2000 train_loss: 0.254486 acc: 0.899700\n",
      "Epoch 1435/2000 train_loss: 0.254479 acc: 0.899700\n",
      "Epoch 1436/2000 train_loss: 0.254483 acc: 0.899400\n",
      "Epoch 1437/2000 train_loss: 0.254479 acc: 0.899500\n",
      "Epoch 1438/2000 train_loss: 0.254475 acc: 0.899500\n",
      "Epoch 1439/2000 train_loss: 0.254463 acc: 0.899800\n",
      "Epoch 1440/2000 train_loss: 0.254461 acc: 0.899200\n",
      "Epoch 1441/2000 train_loss: 0.254450 acc: 0.899700\n",
      "Epoch 1442/2000 train_loss: 0.254458 acc: 0.899900\n",
      "Epoch 1443/2000 train_loss: 0.254437 acc: 0.899300\n",
      "Epoch 1444/2000 train_loss: 0.254453 acc: 0.899800\n",
      "Epoch 1445/2000 train_loss: 0.254450 acc: 0.899800\n",
      "Epoch 1446/2000 train_loss: 0.254443 acc: 0.899800\n",
      "Epoch 1447/2000 train_loss: 0.254444 acc: 0.899500\n",
      "Epoch 1448/2000 train_loss: 0.254438 acc: 0.899200\n",
      "Epoch 1449/2000 train_loss: 0.254430 acc: 0.899000\n",
      "Epoch 1450/2000 train_loss: 0.254423 acc: 0.899700\n",
      "Epoch 1451/2000 train_loss: 0.254429 acc: 0.899700\n",
      "Epoch 1452/2000 train_loss: 0.254428 acc: 0.899400\n",
      "Epoch 1453/2000 train_loss: 0.254404 acc: 0.899800\n",
      "Epoch 1454/2000 train_loss: 0.254425 acc: 0.899400\n",
      "Epoch 1455/2000 train_loss: 0.254418 acc: 0.899400\n",
      "Epoch 1456/2000 train_loss: 0.254416 acc: 0.899800\n",
      "Epoch 1457/2000 train_loss: 0.254411 acc: 0.899700\n",
      "Epoch 1458/2000 train_loss: 0.254408 acc: 0.899400\n",
      "Epoch 1459/2000 train_loss: 0.254405 acc: 0.899500\n",
      "Epoch 1460/2000 train_loss: 0.254398 acc: 0.899700\n",
      "Epoch 1461/2000 train_loss: 0.254403 acc: 0.899500\n",
      "Epoch 1462/2000 train_loss: 0.254391 acc: 0.900000\n",
      "Epoch 1463/2000 train_loss: 0.254387 acc: 0.899600\n",
      "Epoch 1464/2000 train_loss: 0.254386 acc: 0.899400\n",
      "Epoch 1465/2000 train_loss: 0.254387 acc: 0.899300\n",
      "Epoch 1466/2000 train_loss: 0.254382 acc: 0.899500\n",
      "Epoch 1467/2000 train_loss: 0.254377 acc: 0.899500\n",
      "Epoch 1468/2000 train_loss: 0.254375 acc: 0.899400\n",
      "Epoch 1469/2000 train_loss: 0.254370 acc: 0.899700\n",
      "Epoch 1470/2000 train_loss: 0.254371 acc: 0.899500\n",
      "Epoch 1471/2000 train_loss: 0.254362 acc: 0.899400\n",
      "Epoch 1472/2000 train_loss: 0.254364 acc: 0.899600\n",
      "Epoch 1473/2000 train_loss: 0.254358 acc: 0.899800\n",
      "Epoch 1474/2000 train_loss: 0.254354 acc: 0.899600\n",
      "Epoch 1475/2000 train_loss: 0.254348 acc: 0.899300\n",
      "Epoch 1476/2000 train_loss: 0.254350 acc: 0.899700\n",
      "Epoch 1477/2000 train_loss: 0.254347 acc: 0.899700\n",
      "Epoch 1478/2000 train_loss: 0.254341 acc: 0.899600\n",
      "Epoch 1479/2000 train_loss: 0.254323 acc: 0.899300\n",
      "Epoch 1480/2000 train_loss: 0.254341 acc: 0.899500\n",
      "Epoch 1481/2000 train_loss: 0.254324 acc: 0.899400\n",
      "Epoch 1482/2000 train_loss: 0.254330 acc: 0.899800\n",
      "Epoch 1483/2000 train_loss: 0.254301 acc: 0.899500\n",
      "Epoch 1484/2000 train_loss: 0.254328 acc: 0.899600\n",
      "Epoch 1485/2000 train_loss: 0.254324 acc: 0.899400\n",
      "Epoch 1486/2000 train_loss: 0.254306 acc: 0.899500\n",
      "Epoch 1487/2000 train_loss: 0.254320 acc: 0.899600\n",
      "Epoch 1488/2000 train_loss: 0.254312 acc: 0.899500\n",
      "Epoch 1489/2000 train_loss: 0.254307 acc: 0.899800\n",
      "Epoch 1490/2000 train_loss: 0.254311 acc: 0.899800\n",
      "Epoch 1491/2000 train_loss: 0.254303 acc: 0.899400\n",
      "Epoch 1492/2000 train_loss: 0.254296 acc: 0.899500\n",
      "Epoch 1493/2000 train_loss: 0.254300 acc: 0.899400\n",
      "Epoch 1494/2000 train_loss: 0.254294 acc: 0.899900\n",
      "Epoch 1495/2000 train_loss: 0.254284 acc: 0.899800\n",
      "Epoch 1496/2000 train_loss: 0.254287 acc: 0.899600\n",
      "Epoch 1497/2000 train_loss: 0.254283 acc: 0.899500\n",
      "Epoch 1498/2000 train_loss: 0.254274 acc: 0.899600\n",
      "Epoch 1499/2000 train_loss: 0.254282 acc: 0.899600\n",
      "Epoch 1500/2000 train_loss: 0.254270 acc: 0.899800\n",
      "Epoch 1501/2000 train_loss: 0.254277 acc: 0.899400\n",
      "Epoch 1502/2000 train_loss: 0.254273 acc: 0.899600\n",
      "Epoch 1503/2000 train_loss: 0.254264 acc: 0.899300\n",
      "Epoch 1504/2000 train_loss: 0.254264 acc: 0.899500\n",
      "Epoch 1505/2000 train_loss: 0.254261 acc: 0.899700\n",
      "Epoch 1506/2000 train_loss: 0.254249 acc: 0.899300\n",
      "Epoch 1507/2000 train_loss: 0.254253 acc: 0.899600\n",
      "Epoch 1508/2000 train_loss: 0.254251 acc: 0.899400\n",
      "Epoch 1509/2000 train_loss: 0.254245 acc: 0.899600\n",
      "Epoch 1510/2000 train_loss: 0.254242 acc: 0.899400\n",
      "Epoch 1511/2000 train_loss: 0.254239 acc: 0.899700\n",
      "Epoch 1512/2000 train_loss: 0.254236 acc: 0.899900\n",
      "Epoch 1513/2000 train_loss: 0.254235 acc: 0.899600\n",
      "Epoch 1514/2000 train_loss: 0.254229 acc: 0.899200\n",
      "Epoch 1515/2000 train_loss: 0.254228 acc: 0.899700\n",
      "Epoch 1516/2000 train_loss: 0.254227 acc: 0.899400\n",
      "Epoch 1517/2000 train_loss: 0.254227 acc: 0.899300\n",
      "Epoch 1518/2000 train_loss: 0.254233 acc: 0.899700\n",
      "Epoch 1519/2000 train_loss: 0.254222 acc: 0.899900\n",
      "Epoch 1520/2000 train_loss: 0.254209 acc: 0.899600\n",
      "Epoch 1521/2000 train_loss: 0.254211 acc: 0.899800\n",
      "Epoch 1522/2000 train_loss: 0.254206 acc: 0.899600\n",
      "Epoch 1523/2000 train_loss: 0.254211 acc: 0.899800\n",
      "Epoch 1524/2000 train_loss: 0.254201 acc: 0.899500\n",
      "Epoch 1525/2000 train_loss: 0.254204 acc: 0.899700\n",
      "Epoch 1526/2000 train_loss: 0.254189 acc: 0.899600\n",
      "Epoch 1527/2000 train_loss: 0.254193 acc: 0.899600\n",
      "Epoch 1528/2000 train_loss: 0.254187 acc: 0.899500\n",
      "Epoch 1529/2000 train_loss: 0.254173 acc: 0.899800\n",
      "Epoch 1530/2000 train_loss: 0.254180 acc: 0.899500\n",
      "Epoch 1531/2000 train_loss: 0.254171 acc: 0.899400\n",
      "Epoch 1532/2000 train_loss: 0.254174 acc: 0.899500\n",
      "Epoch 1533/2000 train_loss: 0.254175 acc: 0.899300\n",
      "Epoch 1534/2000 train_loss: 0.254173 acc: 0.899300\n",
      "Epoch 1535/2000 train_loss: 0.254165 acc: 0.899600\n",
      "Epoch 1536/2000 train_loss: 0.254160 acc: 0.899300\n",
      "Epoch 1537/2000 train_loss: 0.254159 acc: 0.899600\n",
      "Epoch 1538/2000 train_loss: 0.254158 acc: 0.899600\n",
      "Epoch 1539/2000 train_loss: 0.254151 acc: 0.899600\n",
      "Epoch 1540/2000 train_loss: 0.254143 acc: 0.899700\n",
      "Epoch 1541/2000 train_loss: 0.254145 acc: 0.899500\n",
      "Epoch 1542/2000 train_loss: 0.254159 acc: 0.899500\n",
      "Epoch 1543/2000 train_loss: 0.254148 acc: 0.899800\n",
      "Epoch 1544/2000 train_loss: 0.254142 acc: 0.899500\n",
      "Epoch 1545/2000 train_loss: 0.254135 acc: 0.899400\n",
      "Epoch 1546/2000 train_loss: 0.254131 acc: 0.899700\n",
      "Epoch 1547/2000 train_loss: 0.254129 acc: 0.899800\n",
      "Epoch 1548/2000 train_loss: 0.254122 acc: 0.899500\n",
      "Epoch 1549/2000 train_loss: 0.254128 acc: 0.899600\n",
      "Epoch 1550/2000 train_loss: 0.254124 acc: 0.899800\n",
      "Epoch 1551/2000 train_loss: 0.254128 acc: 0.899500\n",
      "Epoch 1552/2000 train_loss: 0.254124 acc: 0.899600\n",
      "Epoch 1553/2000 train_loss: 0.254111 acc: 0.899700\n",
      "Epoch 1554/2000 train_loss: 0.254108 acc: 0.899600\n",
      "Epoch 1555/2000 train_loss: 0.254107 acc: 0.899400\n",
      "Epoch 1556/2000 train_loss: 0.254107 acc: 0.899500\n",
      "Epoch 1557/2000 train_loss: 0.254100 acc: 0.899500\n",
      "Epoch 1558/2000 train_loss: 0.254099 acc: 0.899800\n",
      "Epoch 1559/2000 train_loss: 0.254106 acc: 0.899100\n",
      "Epoch 1560/2000 train_loss: 0.254082 acc: 0.899500\n",
      "Epoch 1561/2000 train_loss: 0.254088 acc: 0.899800\n",
      "Epoch 1562/2000 train_loss: 0.254093 acc: 0.899800\n",
      "Epoch 1563/2000 train_loss: 0.254074 acc: 0.899200\n",
      "Epoch 1564/2000 train_loss: 0.254082 acc: 0.899600\n",
      "Epoch 1565/2000 train_loss: 0.254083 acc: 0.899600\n",
      "Epoch 1566/2000 train_loss: 0.254075 acc: 0.899400\n",
      "Epoch 1567/2000 train_loss: 0.254075 acc: 0.899500\n",
      "Epoch 1568/2000 train_loss: 0.254073 acc: 0.899600\n",
      "Epoch 1569/2000 train_loss: 0.254065 acc: 0.899400\n",
      "Epoch 1570/2000 train_loss: 0.254063 acc: 0.899700\n",
      "Epoch 1571/2000 train_loss: 0.254049 acc: 0.899600\n",
      "Epoch 1572/2000 train_loss: 0.254056 acc: 0.899200\n",
      "Epoch 1573/2000 train_loss: 0.254052 acc: 0.899800\n",
      "Epoch 1574/2000 train_loss: 0.254054 acc: 0.899600\n",
      "Epoch 1575/2000 train_loss: 0.254044 acc: 0.899300\n",
      "Epoch 1576/2000 train_loss: 0.254046 acc: 0.899500\n",
      "Epoch 1577/2000 train_loss: 0.254047 acc: 0.899600\n",
      "Epoch 1578/2000 train_loss: 0.254035 acc: 0.899800\n",
      "Epoch 1579/2000 train_loss: 0.254039 acc: 0.899200\n",
      "Epoch 1580/2000 train_loss: 0.254030 acc: 0.899500\n",
      "Epoch 1581/2000 train_loss: 0.254029 acc: 0.899900\n",
      "Epoch 1582/2000 train_loss: 0.254017 acc: 0.899600\n",
      "Epoch 1583/2000 train_loss: 0.254028 acc: 0.899200\n",
      "Epoch 1584/2000 train_loss: 0.254021 acc: 0.899500\n",
      "Epoch 1585/2000 train_loss: 0.254015 acc: 0.899600\n",
      "Epoch 1586/2000 train_loss: 0.254022 acc: 0.899500\n",
      "Epoch 1587/2000 train_loss: 0.254026 acc: 0.899400\n",
      "Epoch 1588/2000 train_loss: 0.254009 acc: 0.899700\n",
      "Epoch 1589/2000 train_loss: 0.254008 acc: 0.899600\n",
      "Epoch 1590/2000 train_loss: 0.254001 acc: 0.898900\n",
      "Epoch 1591/2000 train_loss: 0.254006 acc: 0.899900\n",
      "Epoch 1592/2000 train_loss: 0.253999 acc: 0.899300\n",
      "Epoch 1593/2000 train_loss: 0.253996 acc: 0.899300\n",
      "Epoch 1594/2000 train_loss: 0.253994 acc: 0.899500\n",
      "Epoch 1595/2000 train_loss: 0.253996 acc: 0.899700\n",
      "Epoch 1596/2000 train_loss: 0.254000 acc: 0.899400\n",
      "Epoch 1597/2000 train_loss: 0.253989 acc: 0.899400\n",
      "Epoch 1598/2000 train_loss: 0.253980 acc: 0.899200\n",
      "Epoch 1599/2000 train_loss: 0.253982 acc: 0.899200\n",
      "Epoch 1600/2000 train_loss: 0.253978 acc: 0.899400\n",
      "Epoch 1601/2000 train_loss: 0.253966 acc: 0.899400\n",
      "Epoch 1602/2000 train_loss: 0.253974 acc: 0.899500\n",
      "Epoch 1603/2000 train_loss: 0.253969 acc: 0.899800\n",
      "Epoch 1604/2000 train_loss: 0.253974 acc: 0.899500\n",
      "Epoch 1605/2000 train_loss: 0.253964 acc: 0.899700\n",
      "Epoch 1606/2000 train_loss: 0.253960 acc: 0.899300\n",
      "Epoch 1607/2000 train_loss: 0.253960 acc: 0.899400\n",
      "Epoch 1608/2000 train_loss: 0.253952 acc: 0.899600\n",
      "Epoch 1609/2000 train_loss: 0.253952 acc: 0.899300\n",
      "Epoch 1610/2000 train_loss: 0.253950 acc: 0.899600\n",
      "Epoch 1611/2000 train_loss: 0.253951 acc: 0.899900\n",
      "Epoch 1612/2000 train_loss: 0.253940 acc: 0.899300\n",
      "Epoch 1613/2000 train_loss: 0.253941 acc: 0.899600\n",
      "Epoch 1614/2000 train_loss: 0.253937 acc: 0.899400\n",
      "Epoch 1615/2000 train_loss: 0.253940 acc: 0.899600\n",
      "Epoch 1616/2000 train_loss: 0.253934 acc: 0.899500\n",
      "Epoch 1617/2000 train_loss: 0.253918 acc: 0.899400\n",
      "Epoch 1618/2000 train_loss: 0.253936 acc: 0.899400\n",
      "Epoch 1619/2000 train_loss: 0.253921 acc: 0.899200\n",
      "Epoch 1620/2000 train_loss: 0.253929 acc: 0.899600\n",
      "Epoch 1621/2000 train_loss: 0.253927 acc: 0.899600\n",
      "Epoch 1622/2000 train_loss: 0.253916 acc: 0.899500\n",
      "Epoch 1623/2000 train_loss: 0.253922 acc: 0.899500\n",
      "Epoch 1624/2000 train_loss: 0.253917 acc: 0.899500\n",
      "Epoch 1625/2000 train_loss: 0.253917 acc: 0.899500\n",
      "Epoch 1626/2000 train_loss: 0.253911 acc: 0.899600\n",
      "Epoch 1627/2000 train_loss: 0.253905 acc: 0.899500\n",
      "Epoch 1628/2000 train_loss: 0.253905 acc: 0.899200\n",
      "Epoch 1629/2000 train_loss: 0.253904 acc: 0.899200\n",
      "Epoch 1630/2000 train_loss: 0.253895 acc: 0.899500\n",
      "Epoch 1631/2000 train_loss: 0.253894 acc: 0.899300\n",
      "Epoch 1632/2000 train_loss: 0.253894 acc: 0.899300\n",
      "Epoch 1633/2000 train_loss: 0.253887 acc: 0.899400\n",
      "Epoch 1634/2000 train_loss: 0.253886 acc: 0.899800\n",
      "Epoch 1635/2000 train_loss: 0.253881 acc: 0.899500\n",
      "Epoch 1636/2000 train_loss: 0.253883 acc: 0.899400\n",
      "Epoch 1637/2000 train_loss: 0.253882 acc: 0.899700\n",
      "Epoch 1638/2000 train_loss: 0.253874 acc: 0.899700\n",
      "Epoch 1639/2000 train_loss: 0.253878 acc: 0.899300\n",
      "Epoch 1640/2000 train_loss: 0.253870 acc: 0.899600\n",
      "Epoch 1641/2000 train_loss: 0.253872 acc: 0.899100\n",
      "Epoch 1642/2000 train_loss: 0.253870 acc: 0.899800\n",
      "Epoch 1643/2000 train_loss: 0.253871 acc: 0.899500\n",
      "Epoch 1644/2000 train_loss: 0.253858 acc: 0.899400\n",
      "Epoch 1645/2000 train_loss: 0.253855 acc: 0.899500\n",
      "Epoch 1646/2000 train_loss: 0.253856 acc: 0.899400\n",
      "Epoch 1647/2000 train_loss: 0.253861 acc: 0.899600\n",
      "Epoch 1648/2000 train_loss: 0.253849 acc: 0.899800\n",
      "Epoch 1649/2000 train_loss: 0.253847 acc: 0.899700\n",
      "Epoch 1650/2000 train_loss: 0.253850 acc: 0.899300\n",
      "Epoch 1651/2000 train_loss: 0.253843 acc: 0.899500\n",
      "Epoch 1652/2000 train_loss: 0.253839 acc: 0.899700\n",
      "Epoch 1653/2000 train_loss: 0.253834 acc: 0.899700\n",
      "Epoch 1654/2000 train_loss: 0.253834 acc: 0.899700\n",
      "Epoch 1655/2000 train_loss: 0.253829 acc: 0.899700\n",
      "Epoch 1656/2000 train_loss: 0.253821 acc: 0.899700\n",
      "Epoch 1657/2000 train_loss: 0.253835 acc: 0.899400\n",
      "Epoch 1658/2000 train_loss: 0.253827 acc: 0.899600\n",
      "Epoch 1659/2000 train_loss: 0.253812 acc: 0.899400\n",
      "Epoch 1660/2000 train_loss: 0.253816 acc: 0.899800\n",
      "Epoch 1661/2000 train_loss: 0.253815 acc: 0.899300\n",
      "Epoch 1662/2000 train_loss: 0.253809 acc: 0.899600\n",
      "Epoch 1663/2000 train_loss: 0.253809 acc: 0.899900\n",
      "Epoch 1664/2000 train_loss: 0.253807 acc: 0.899500\n",
      "Epoch 1665/2000 train_loss: 0.253811 acc: 0.899400\n",
      "Epoch 1666/2000 train_loss: 0.253792 acc: 0.899300\n",
      "Epoch 1667/2000 train_loss: 0.253794 acc: 0.899900\n",
      "Epoch 1668/2000 train_loss: 0.253794 acc: 0.899800\n",
      "Epoch 1669/2000 train_loss: 0.253791 acc: 0.899600\n",
      "Epoch 1670/2000 train_loss: 0.253794 acc: 0.899800\n",
      "Epoch 1671/2000 train_loss: 0.253791 acc: 0.899300\n",
      "Epoch 1672/2000 train_loss: 0.253788 acc: 0.899600\n",
      "Epoch 1673/2000 train_loss: 0.253787 acc: 0.899700\n",
      "Epoch 1674/2000 train_loss: 0.253784 acc: 0.899200\n",
      "Epoch 1675/2000 train_loss: 0.253768 acc: 0.899500\n",
      "Epoch 1676/2000 train_loss: 0.253776 acc: 0.899600\n",
      "Epoch 1677/2000 train_loss: 0.253764 acc: 0.899300\n",
      "Epoch 1678/2000 train_loss: 0.253777 acc: 0.899800\n",
      "Epoch 1679/2000 train_loss: 0.253762 acc: 0.899200\n",
      "Epoch 1680/2000 train_loss: 0.253764 acc: 0.899500\n",
      "Epoch 1681/2000 train_loss: 0.253759 acc: 0.899300\n",
      "Epoch 1682/2000 train_loss: 0.253760 acc: 0.899500\n",
      "Epoch 1683/2000 train_loss: 0.253761 acc: 0.899700\n",
      "Epoch 1684/2000 train_loss: 0.253751 acc: 0.899400\n",
      "Epoch 1685/2000 train_loss: 0.253755 acc: 0.899000\n",
      "Epoch 1686/2000 train_loss: 0.253752 acc: 0.899400\n",
      "Epoch 1687/2000 train_loss: 0.253745 acc: 0.899700\n",
      "Epoch 1688/2000 train_loss: 0.253753 acc: 0.899400\n",
      "Epoch 1689/2000 train_loss: 0.253739 acc: 0.899500\n",
      "Epoch 1690/2000 train_loss: 0.253750 acc: 0.899500\n",
      "Epoch 1691/2000 train_loss: 0.253732 acc: 0.899300\n",
      "Epoch 1692/2000 train_loss: 0.253737 acc: 0.899500\n",
      "Epoch 1693/2000 train_loss: 0.253736 acc: 0.899600\n",
      "Epoch 1694/2000 train_loss: 0.253730 acc: 0.899300\n",
      "Epoch 1695/2000 train_loss: 0.253727 acc: 0.899500\n",
      "Epoch 1696/2000 train_loss: 0.253720 acc: 0.899700\n",
      "Epoch 1697/2000 train_loss: 0.253728 acc: 0.899400\n",
      "Epoch 1698/2000 train_loss: 0.253721 acc: 0.899800\n",
      "Epoch 1699/2000 train_loss: 0.253717 acc: 0.899400\n",
      "Epoch 1700/2000 train_loss: 0.253712 acc: 0.899800\n",
      "Epoch 1701/2000 train_loss: 0.253710 acc: 0.899500\n",
      "Epoch 1702/2000 train_loss: 0.253705 acc: 0.899600\n",
      "Epoch 1703/2000 train_loss: 0.253715 acc: 0.899700\n",
      "Epoch 1704/2000 train_loss: 0.253698 acc: 0.899800\n",
      "Epoch 1705/2000 train_loss: 0.253698 acc: 0.899300\n",
      "Epoch 1706/2000 train_loss: 0.253707 acc: 0.899800\n",
      "Epoch 1707/2000 train_loss: 0.253697 acc: 0.899600\n",
      "Epoch 1708/2000 train_loss: 0.253696 acc: 0.899600\n",
      "Epoch 1709/2000 train_loss: 0.253695 acc: 0.899800\n",
      "Epoch 1710/2000 train_loss: 0.253690 acc: 0.899800\n",
      "Epoch 1711/2000 train_loss: 0.253682 acc: 0.899800\n",
      "Epoch 1712/2000 train_loss: 0.253682 acc: 0.899400\n",
      "Epoch 1713/2000 train_loss: 0.253684 acc: 0.899600\n",
      "Epoch 1714/2000 train_loss: 0.253679 acc: 0.899600\n",
      "Epoch 1715/2000 train_loss: 0.253675 acc: 0.899300\n",
      "Epoch 1716/2000 train_loss: 0.253680 acc: 0.899700\n",
      "Epoch 1717/2000 train_loss: 0.253673 acc: 0.899700\n",
      "Epoch 1718/2000 train_loss: 0.253666 acc: 0.899700\n",
      "Epoch 1719/2000 train_loss: 0.253676 acc: 0.899700\n",
      "Epoch 1720/2000 train_loss: 0.253667 acc: 0.899300\n",
      "Epoch 1721/2000 train_loss: 0.253665 acc: 0.899800\n",
      "Epoch 1722/2000 train_loss: 0.253650 acc: 0.899500\n",
      "Epoch 1723/2000 train_loss: 0.253662 acc: 0.899600\n",
      "Epoch 1724/2000 train_loss: 0.253657 acc: 0.899500\n",
      "Epoch 1725/2000 train_loss: 0.253652 acc: 0.899400\n",
      "Epoch 1726/2000 train_loss: 0.253644 acc: 0.899300\n",
      "Epoch 1727/2000 train_loss: 0.253626 acc: 0.899300\n",
      "Epoch 1728/2000 train_loss: 0.253657 acc: 0.899400\n",
      "Epoch 1729/2000 train_loss: 0.253641 acc: 0.899600\n",
      "Epoch 1730/2000 train_loss: 0.253637 acc: 0.899400\n",
      "Epoch 1731/2000 train_loss: 0.253618 acc: 0.899400\n",
      "Epoch 1732/2000 train_loss: 0.253646 acc: 0.899400\n",
      "Epoch 1733/2000 train_loss: 0.253627 acc: 0.899500\n",
      "Epoch 1734/2000 train_loss: 0.253630 acc: 0.899900\n",
      "Epoch 1735/2000 train_loss: 0.253622 acc: 0.899500\n",
      "Epoch 1736/2000 train_loss: 0.253628 acc: 0.899500\n",
      "Epoch 1737/2000 train_loss: 0.253616 acc: 0.899500\n",
      "Epoch 1738/2000 train_loss: 0.253617 acc: 0.899500\n",
      "Epoch 1739/2000 train_loss: 0.253617 acc: 0.899400\n",
      "Epoch 1740/2000 train_loss: 0.253610 acc: 0.899600\n",
      "Epoch 1741/2000 train_loss: 0.253598 acc: 0.899400\n",
      "Epoch 1742/2000 train_loss: 0.253618 acc: 0.899600\n",
      "Epoch 1743/2000 train_loss: 0.253612 acc: 0.899400\n",
      "Epoch 1744/2000 train_loss: 0.253607 acc: 0.899400\n",
      "Epoch 1745/2000 train_loss: 0.253601 acc: 0.899500\n",
      "Epoch 1746/2000 train_loss: 0.253608 acc: 0.899500\n",
      "Epoch 1747/2000 train_loss: 0.253588 acc: 0.899300\n",
      "Epoch 1748/2000 train_loss: 0.253590 acc: 0.899300\n",
      "Epoch 1749/2000 train_loss: 0.253593 acc: 0.899600\n",
      "Epoch 1750/2000 train_loss: 0.253591 acc: 0.899500\n",
      "Epoch 1751/2000 train_loss: 0.253588 acc: 0.899500\n",
      "Epoch 1752/2000 train_loss: 0.253584 acc: 0.899800\n",
      "Epoch 1753/2000 train_loss: 0.253585 acc: 0.899400\n",
      "Epoch 1754/2000 train_loss: 0.253573 acc: 0.899700\n",
      "Epoch 1755/2000 train_loss: 0.253546 acc: 0.900000\n",
      "Epoch 1756/2000 train_loss: 0.253583 acc: 0.899300\n",
      "Epoch 1757/2000 train_loss: 0.253580 acc: 0.899400\n",
      "Epoch 1758/2000 train_loss: 0.253553 acc: 0.899500\n",
      "Epoch 1759/2000 train_loss: 0.253569 acc: 0.899200\n",
      "Epoch 1760/2000 train_loss: 0.253572 acc: 0.899300\n",
      "Epoch 1761/2000 train_loss: 0.253558 acc: 0.899600\n",
      "Epoch 1762/2000 train_loss: 0.253563 acc: 0.899800\n",
      "Epoch 1763/2000 train_loss: 0.253555 acc: 0.899600\n",
      "Epoch 1764/2000 train_loss: 0.253549 acc: 0.899600\n",
      "Epoch 1765/2000 train_loss: 0.253548 acc: 0.899900\n",
      "Epoch 1766/2000 train_loss: 0.253544 acc: 0.899500\n",
      "Epoch 1767/2000 train_loss: 0.253550 acc: 0.899300\n",
      "Epoch 1768/2000 train_loss: 0.253553 acc: 0.899700\n",
      "Epoch 1769/2000 train_loss: 0.253545 acc: 0.899400\n",
      "Epoch 1770/2000 train_loss: 0.253540 acc: 0.899200\n",
      "Epoch 1771/2000 train_loss: 0.253547 acc: 0.899600\n",
      "Epoch 1772/2000 train_loss: 0.253537 acc: 0.899500\n",
      "Epoch 1773/2000 train_loss: 0.253539 acc: 0.899600\n",
      "Epoch 1774/2000 train_loss: 0.253535 acc: 0.899600\n",
      "Epoch 1775/2000 train_loss: 0.253534 acc: 0.899400\n",
      "Epoch 1776/2000 train_loss: 0.253523 acc: 0.899500\n",
      "Epoch 1777/2000 train_loss: 0.253519 acc: 0.899200\n",
      "Epoch 1778/2000 train_loss: 0.253518 acc: 0.899300\n",
      "Epoch 1779/2000 train_loss: 0.253539 acc: 0.899400\n",
      "Epoch 1780/2000 train_loss: 0.253519 acc: 0.899600\n",
      "Epoch 1781/2000 train_loss: 0.253515 acc: 0.899200\n",
      "Epoch 1782/2000 train_loss: 0.253516 acc: 0.899300\n",
      "Epoch 1783/2000 train_loss: 0.253519 acc: 0.899500\n",
      "Epoch 1784/2000 train_loss: 0.253487 acc: 0.899200\n",
      "Epoch 1785/2000 train_loss: 0.253520 acc: 0.899300\n",
      "Epoch 1786/2000 train_loss: 0.253504 acc: 0.899500\n",
      "Epoch 1787/2000 train_loss: 0.253495 acc: 0.899200\n",
      "Epoch 1788/2000 train_loss: 0.253511 acc: 0.899300\n",
      "Epoch 1789/2000 train_loss: 0.253490 acc: 0.899200\n",
      "Epoch 1790/2000 train_loss: 0.253501 acc: 0.899500\n",
      "Epoch 1791/2000 train_loss: 0.253493 acc: 0.899700\n",
      "Epoch 1792/2000 train_loss: 0.253488 acc: 0.899100\n",
      "Epoch 1793/2000 train_loss: 0.253486 acc: 0.899600\n",
      "Epoch 1794/2000 train_loss: 0.253491 acc: 0.899900\n",
      "Epoch 1795/2000 train_loss: 0.253492 acc: 0.899500\n",
      "Epoch 1796/2000 train_loss: 0.253473 acc: 0.899600\n",
      "Epoch 1797/2000 train_loss: 0.253470 acc: 0.899300\n",
      "Epoch 1798/2000 train_loss: 0.253484 acc: 0.899400\n",
      "Epoch 1799/2000 train_loss: 0.253478 acc: 0.899600\n",
      "Epoch 1800/2000 train_loss: 0.253477 acc: 0.899500\n",
      "Epoch 1801/2000 train_loss: 0.253469 acc: 0.899200\n",
      "Epoch 1802/2000 train_loss: 0.253467 acc: 0.898900\n",
      "Epoch 1803/2000 train_loss: 0.253467 acc: 0.899400\n",
      "Epoch 1804/2000 train_loss: 0.253454 acc: 0.899500\n",
      "Epoch 1805/2000 train_loss: 0.253463 acc: 0.899200\n",
      "Epoch 1806/2000 train_loss: 0.253460 acc: 0.899400\n",
      "Epoch 1807/2000 train_loss: 0.253459 acc: 0.899500\n",
      "Epoch 1808/2000 train_loss: 0.253448 acc: 0.899100\n",
      "Epoch 1809/2000 train_loss: 0.253439 acc: 0.899400\n",
      "Epoch 1810/2000 train_loss: 0.253457 acc: 0.899400\n",
      "Epoch 1811/2000 train_loss: 0.253445 acc: 0.899300\n",
      "Epoch 1812/2000 train_loss: 0.253441 acc: 0.899100\n",
      "Epoch 1813/2000 train_loss: 0.253429 acc: 0.899400\n",
      "Epoch 1814/2000 train_loss: 0.253446 acc: 0.899100\n",
      "Epoch 1815/2000 train_loss: 0.253433 acc: 0.899200\n",
      "Epoch 1816/2000 train_loss: 0.253441 acc: 0.899200\n",
      "Epoch 1817/2000 train_loss: 0.253434 acc: 0.899500\n",
      "Epoch 1818/2000 train_loss: 0.253430 acc: 0.899400\n",
      "Epoch 1819/2000 train_loss: 0.253434 acc: 0.899200\n",
      "Epoch 1820/2000 train_loss: 0.253427 acc: 0.899200\n",
      "Epoch 1821/2000 train_loss: 0.253433 acc: 0.899300\n",
      "Epoch 1822/2000 train_loss: 0.253423 acc: 0.899300\n",
      "Epoch 1823/2000 train_loss: 0.253424 acc: 0.899400\n",
      "Epoch 1824/2000 train_loss: 0.253425 acc: 0.899300\n",
      "Epoch 1825/2000 train_loss: 0.253409 acc: 0.899800\n",
      "Epoch 1826/2000 train_loss: 0.253418 acc: 0.899400\n",
      "Epoch 1827/2000 train_loss: 0.253416 acc: 0.900000\n",
      "Epoch 1828/2000 train_loss: 0.253414 acc: 0.899400\n",
      "Epoch 1829/2000 train_loss: 0.253390 acc: 0.899800\n",
      "Epoch 1830/2000 train_loss: 0.253394 acc: 0.899200\n",
      "Epoch 1831/2000 train_loss: 0.253408 acc: 0.899700\n",
      "Epoch 1832/2000 train_loss: 0.253401 acc: 0.899400\n",
      "Epoch 1833/2000 train_loss: 0.253394 acc: 0.899600\n",
      "Epoch 1834/2000 train_loss: 0.253393 acc: 0.899600\n",
      "Epoch 1835/2000 train_loss: 0.253394 acc: 0.899300\n",
      "Epoch 1836/2000 train_loss: 0.253394 acc: 0.899300\n",
      "Epoch 1837/2000 train_loss: 0.253386 acc: 0.899300\n",
      "Epoch 1838/2000 train_loss: 0.253382 acc: 0.899100\n",
      "Epoch 1839/2000 train_loss: 0.253383 acc: 0.899300\n",
      "Epoch 1840/2000 train_loss: 0.253391 acc: 0.899300\n",
      "Epoch 1841/2000 train_loss: 0.253376 acc: 0.899400\n",
      "Epoch 1842/2000 train_loss: 0.253378 acc: 0.899200\n",
      "Epoch 1843/2000 train_loss: 0.253374 acc: 0.899500\n",
      "Epoch 1844/2000 train_loss: 0.253364 acc: 0.899400\n",
      "Epoch 1845/2000 train_loss: 0.253375 acc: 0.899700\n",
      "Epoch 1846/2000 train_loss: 0.253368 acc: 0.899600\n",
      "Epoch 1847/2000 train_loss: 0.253366 acc: 0.899100\n",
      "Epoch 1848/2000 train_loss: 0.253369 acc: 0.899700\n",
      "Epoch 1849/2000 train_loss: 0.253369 acc: 0.899200\n",
      "Epoch 1850/2000 train_loss: 0.253359 acc: 0.899400\n",
      "Epoch 1851/2000 train_loss: 0.253356 acc: 0.899300\n",
      "Epoch 1852/2000 train_loss: 0.253356 acc: 0.899100\n",
      "Epoch 1853/2000 train_loss: 0.253356 acc: 0.899500\n",
      "Epoch 1854/2000 train_loss: 0.253356 acc: 0.899500\n",
      "Epoch 1855/2000 train_loss: 0.253343 acc: 0.899500\n",
      "Epoch 1856/2000 train_loss: 0.253351 acc: 0.899500\n",
      "Epoch 1857/2000 train_loss: 0.253346 acc: 0.899500\n",
      "Epoch 1858/2000 train_loss: 0.253338 acc: 0.899400\n",
      "Epoch 1859/2000 train_loss: 0.253334 acc: 0.899500\n",
      "Epoch 1860/2000 train_loss: 0.253340 acc: 0.899400\n",
      "Epoch 1861/2000 train_loss: 0.253341 acc: 0.899400\n",
      "Epoch 1862/2000 train_loss: 0.253341 acc: 0.899300\n",
      "Epoch 1863/2000 train_loss: 0.253340 acc: 0.899500\n",
      "Epoch 1864/2000 train_loss: 0.253331 acc: 0.899500\n",
      "Epoch 1865/2000 train_loss: 0.253320 acc: 0.899400\n",
      "Epoch 1866/2000 train_loss: 0.253324 acc: 0.899900\n",
      "Epoch 1867/2000 train_loss: 0.253326 acc: 0.899700\n",
      "Epoch 1868/2000 train_loss: 0.253321 acc: 0.899300\n",
      "Epoch 1869/2000 train_loss: 0.253316 acc: 0.899300\n",
      "Epoch 1870/2000 train_loss: 0.253318 acc: 0.899700\n",
      "Epoch 1871/2000 train_loss: 0.253318 acc: 0.899700\n",
      "Epoch 1872/2000 train_loss: 0.253312 acc: 0.899200\n",
      "Epoch 1873/2000 train_loss: 0.253315 acc: 0.899000\n",
      "Epoch 1874/2000 train_loss: 0.253310 acc: 0.899400\n",
      "Epoch 1875/2000 train_loss: 0.253305 acc: 0.899500\n",
      "Epoch 1876/2000 train_loss: 0.253297 acc: 0.899300\n",
      "Epoch 1877/2000 train_loss: 0.253295 acc: 0.899700\n",
      "Epoch 1878/2000 train_loss: 0.253299 acc: 0.899500\n",
      "Epoch 1879/2000 train_loss: 0.253304 acc: 0.899000\n",
      "Epoch 1880/2000 train_loss: 0.253295 acc: 0.899300\n",
      "Epoch 1881/2000 train_loss: 0.253292 acc: 0.899700\n",
      "Epoch 1882/2000 train_loss: 0.253293 acc: 0.899500\n",
      "Epoch 1883/2000 train_loss: 0.253271 acc: 0.898900\n",
      "Epoch 1884/2000 train_loss: 0.253289 acc: 0.899700\n",
      "Epoch 1885/2000 train_loss: 0.253290 acc: 0.899100\n",
      "Epoch 1886/2000 train_loss: 0.253275 acc: 0.899400\n",
      "Epoch 1887/2000 train_loss: 0.253285 acc: 0.899500\n",
      "Epoch 1888/2000 train_loss: 0.253266 acc: 0.898900\n",
      "Epoch 1889/2000 train_loss: 0.253284 acc: 0.899600\n",
      "Epoch 1890/2000 train_loss: 0.253272 acc: 0.899600\n",
      "Epoch 1891/2000 train_loss: 0.253278 acc: 0.899500\n",
      "Epoch 1892/2000 train_loss: 0.253266 acc: 0.899400\n",
      "Epoch 1893/2000 train_loss: 0.253256 acc: 0.899600\n",
      "Epoch 1894/2000 train_loss: 0.253258 acc: 0.899500\n",
      "Epoch 1895/2000 train_loss: 0.253260 acc: 0.899300\n",
      "Epoch 1896/2000 train_loss: 0.253265 acc: 0.899400\n",
      "Epoch 1897/2000 train_loss: 0.253263 acc: 0.899200\n",
      "Epoch 1898/2000 train_loss: 0.253250 acc: 0.899100\n",
      "Epoch 1899/2000 train_loss: 0.253235 acc: 0.899600\n",
      "Epoch 1900/2000 train_loss: 0.253245 acc: 0.899500\n",
      "Epoch 1901/2000 train_loss: 0.253250 acc: 0.899200\n",
      "Epoch 1902/2000 train_loss: 0.253254 acc: 0.899400\n",
      "Epoch 1903/2000 train_loss: 0.253247 acc: 0.899400\n",
      "Epoch 1904/2000 train_loss: 0.253239 acc: 0.899400\n",
      "Epoch 1905/2000 train_loss: 0.253238 acc: 0.899400\n",
      "Epoch 1906/2000 train_loss: 0.253238 acc: 0.899300\n",
      "Epoch 1907/2000 train_loss: 0.253237 acc: 0.899500\n",
      "Epoch 1908/2000 train_loss: 0.253210 acc: 0.899400\n",
      "Epoch 1909/2000 train_loss: 0.253232 acc: 0.899100\n",
      "Epoch 1910/2000 train_loss: 0.253229 acc: 0.899100\n",
      "Epoch 1911/2000 train_loss: 0.253223 acc: 0.899200\n",
      "Epoch 1912/2000 train_loss: 0.253216 acc: 0.899100\n",
      "Epoch 1913/2000 train_loss: 0.253215 acc: 0.899400\n",
      "Epoch 1914/2000 train_loss: 0.253229 acc: 0.899500\n",
      "Epoch 1915/2000 train_loss: 0.253222 acc: 0.899700\n",
      "Epoch 1916/2000 train_loss: 0.253218 acc: 0.899900\n",
      "Epoch 1917/2000 train_loss: 0.253217 acc: 0.899800\n",
      "Epoch 1918/2000 train_loss: 0.253210 acc: 0.899500\n",
      "Epoch 1919/2000 train_loss: 0.253212 acc: 0.899500\n",
      "Epoch 1920/2000 train_loss: 0.253205 acc: 0.899500\n",
      "Epoch 1921/2000 train_loss: 0.253211 acc: 0.899700\n",
      "Epoch 1922/2000 train_loss: 0.253206 acc: 0.899400\n",
      "Epoch 1923/2000 train_loss: 0.253197 acc: 0.899300\n",
      "Epoch 1924/2000 train_loss: 0.253203 acc: 0.899400\n",
      "Epoch 1925/2000 train_loss: 0.253207 acc: 0.899400\n",
      "Epoch 1926/2000 train_loss: 0.253195 acc: 0.899500\n",
      "Epoch 1927/2000 train_loss: 0.253192 acc: 0.899400\n",
      "Epoch 1928/2000 train_loss: 0.253192 acc: 0.899600\n",
      "Epoch 1929/2000 train_loss: 0.253184 acc: 0.899200\n",
      "Epoch 1930/2000 train_loss: 0.253197 acc: 0.899300\n",
      "Epoch 1931/2000 train_loss: 0.253183 acc: 0.899200\n",
      "Epoch 1932/2000 train_loss: 0.253193 acc: 0.899300\n",
      "Epoch 1933/2000 train_loss: 0.253185 acc: 0.899700\n",
      "Epoch 1934/2000 train_loss: 0.253177 acc: 0.899300\n",
      "Epoch 1935/2000 train_loss: 0.253169 acc: 0.899700\n",
      "Epoch 1936/2000 train_loss: 0.253162 acc: 0.899500\n",
      "Epoch 1937/2000 train_loss: 0.253179 acc: 0.899300\n",
      "Epoch 1938/2000 train_loss: 0.253181 acc: 0.899100\n",
      "Epoch 1939/2000 train_loss: 0.253170 acc: 0.899100\n",
      "Epoch 1940/2000 train_loss: 0.253142 acc: 0.899100\n",
      "Epoch 1941/2000 train_loss: 0.253171 acc: 0.899300\n",
      "Epoch 1942/2000 train_loss: 0.253172 acc: 0.899300\n",
      "Epoch 1943/2000 train_loss: 0.253162 acc: 0.899800\n",
      "Epoch 1944/2000 train_loss: 0.253156 acc: 0.899300\n",
      "Epoch 1945/2000 train_loss: 0.253140 acc: 0.899200\n",
      "Epoch 1946/2000 train_loss: 0.253156 acc: 0.899300\n",
      "Epoch 1947/2000 train_loss: 0.253150 acc: 0.899100\n",
      "Epoch 1948/2000 train_loss: 0.253157 acc: 0.899700\n",
      "Epoch 1949/2000 train_loss: 0.253159 acc: 0.899300\n",
      "Epoch 1950/2000 train_loss: 0.253149 acc: 0.899200\n",
      "Epoch 1951/2000 train_loss: 0.253148 acc: 0.899300\n",
      "Epoch 1952/2000 train_loss: 0.253151 acc: 0.899400\n",
      "Epoch 1953/2000 train_loss: 0.253143 acc: 0.899000\n",
      "Epoch 1954/2000 train_loss: 0.253135 acc: 0.899300\n",
      "Epoch 1955/2000 train_loss: 0.253144 acc: 0.899500\n",
      "Epoch 1956/2000 train_loss: 0.253120 acc: 0.899500\n",
      "Epoch 1957/2000 train_loss: 0.253136 acc: 0.899200\n",
      "Epoch 1958/2000 train_loss: 0.253133 acc: 0.899200\n",
      "Epoch 1959/2000 train_loss: 0.253125 acc: 0.898900\n",
      "Epoch 1960/2000 train_loss: 0.253132 acc: 0.899200\n",
      "Epoch 1961/2000 train_loss: 0.253121 acc: 0.899900\n",
      "Epoch 1962/2000 train_loss: 0.253126 acc: 0.899000\n",
      "Epoch 1963/2000 train_loss: 0.253125 acc: 0.899300\n",
      "Epoch 1964/2000 train_loss: 0.253124 acc: 0.899200\n",
      "Epoch 1965/2000 train_loss: 0.253114 acc: 0.899200\n",
      "Epoch 1966/2000 train_loss: 0.253111 acc: 0.899400\n",
      "Epoch 1967/2000 train_loss: 0.253116 acc: 0.899500\n",
      "Epoch 1968/2000 train_loss: 0.253110 acc: 0.899600\n",
      "Epoch 1969/2000 train_loss: 0.253107 acc: 0.899000\n",
      "Epoch 1970/2000 train_loss: 0.253106 acc: 0.899800\n",
      "Epoch 1971/2000 train_loss: 0.253101 acc: 0.899400\n",
      "Epoch 1972/2000 train_loss: 0.253107 acc: 0.899100\n",
      "Epoch 1973/2000 train_loss: 0.253108 acc: 0.899200\n",
      "Epoch 1974/2000 train_loss: 0.253105 acc: 0.899200\n",
      "Epoch 1975/2000 train_loss: 0.253099 acc: 0.899500\n",
      "Epoch 1976/2000 train_loss: 0.253080 acc: 0.899700\n",
      "Epoch 1977/2000 train_loss: 0.253098 acc: 0.899200\n",
      "Epoch 1978/2000 train_loss: 0.253099 acc: 0.899400\n",
      "Epoch 1979/2000 train_loss: 0.253089 acc: 0.899200\n",
      "Epoch 1980/2000 train_loss: 0.253086 acc: 0.899200\n",
      "Epoch 1981/2000 train_loss: 0.253091 acc: 0.898900\n",
      "Epoch 1982/2000 train_loss: 0.253082 acc: 0.899700\n",
      "Epoch 1983/2000 train_loss: 0.253082 acc: 0.899700\n",
      "Epoch 1984/2000 train_loss: 0.253083 acc: 0.899200\n",
      "Epoch 1985/2000 train_loss: 0.253074 acc: 0.899500\n",
      "Epoch 1986/2000 train_loss: 0.253076 acc: 0.899200\n",
      "Epoch 1987/2000 train_loss: 0.253082 acc: 0.899300\n",
      "Epoch 1988/2000 train_loss: 0.253068 acc: 0.898900\n",
      "Epoch 1989/2000 train_loss: 0.253065 acc: 0.899000\n",
      "Epoch 1990/2000 train_loss: 0.253068 acc: 0.899200\n",
      "Epoch 1991/2000 train_loss: 0.253066 acc: 0.899100\n",
      "Epoch 1992/2000 train_loss: 0.253066 acc: 0.899300\n",
      "Epoch 1993/2000 train_loss: 0.253032 acc: 0.899600\n",
      "Epoch 1994/2000 train_loss: 0.253062 acc: 0.899300\n",
      "Epoch 1995/2000 train_loss: 0.253062 acc: 0.899000\n",
      "Epoch 1996/2000 train_loss: 0.253054 acc: 0.899100\n",
      "Epoch 1997/2000 train_loss: 0.253053 acc: 0.899000\n",
      "Epoch 1998/2000 train_loss: 0.253053 acc: 0.898900\n",
      "Epoch 1999/2000 train_loss: 0.253052 acc: 0.898900\n",
      "Epoch 2000/2000 train_loss: 0.253047 acc: 0.899300\n"
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    loss_sum = 0.0\n",
    "    test_sum = 0.0\n",
    "\n",
    "    metric = BinaryAccuracy()\n",
    "\n",
    "    for (train_features,train_labels),(test_features,test_labels) in zip(train_dataloader,test_dataloader) :\n",
    "\n",
    "        epoch_y = net(train_features)\n",
    "        loss = loss_func(epoch_y.to(torch.float32),train_labels.to(torch.float32))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 100 == 0 :\n",
    "            for m in net.modules():\n",
    "                if isinstance(m,nn.Linear):\n",
    "                    \n",
    "                    img_tensor = m.weight.clone().detach()\n",
    "                    img = resize_tensor(img_tensor.squeeze())\n",
    "                    \n",
    "                    img = img.reshape(28,28)\n",
    "                    np_arr = np.array(img,dtype=np.uint8)\n",
    "                                        \n",
    "                    writer.add_image('weight_to_img',np_arr,epoch + 1,dataformats='WH')\n",
    "\n",
    "        loss_sum += loss\n",
    "        \n",
    "        epoch_test_y = net(test_features)\n",
    "        \n",
    "        result_set = []\n",
    "        \n",
    "        for result in epoch_test_y:\n",
    "            pre = result.item()\n",
    "            \n",
    "            if pre <= 0.5:\n",
    "                result_set.append(np.array([0]))\n",
    "            else:\n",
    "                result_set.append(np.array([1]))\n",
    "                \n",
    "        result_set = np.array(result_set)\n",
    "        result_set = torch.tensor(result_set)\n",
    "        \n",
    "        metric.update(result_set.squeeze(),test_labels.squeeze())\n",
    "    acc = metric.compute()\n",
    "\n",
    "    loss = loss_sum / len(train_dataloader)\n",
    "\n",
    "    print('Epoch {:4d}/{} train_loss: {:.6f} acc: {:.6f}'.format(\n",
    "        epoch+1, epochs, loss, acc\n",
    "    ))\n",
    "\n",
    "    if loss < 0.1 :\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m         test_result\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m---> 15\u001b[0m     \u001b[43mtest_y\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m(y)\n\u001b[1;32m     17\u001b[0m test_result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(test_result)\n\u001b[1;32m     18\u001b[0m test_result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(test_result)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "test_result = []\n",
    "\n",
    "test_y = []\n",
    "\n",
    "final_metric = BinaryAccuracy()\n",
    "\n",
    "for x ,y in test_dataset:\n",
    "    pre = net(torch.tensor(x))\n",
    "    \n",
    "    if pre <= 0.5:\n",
    "        test_result.append(np.array([0]))\n",
    "    else:\n",
    "        test_result.append(np.array([1]))\n",
    "        \n",
    "    test_y.append(y)\n",
    "    \n",
    "test_result = np.array(test_result)\n",
    "test_result = torch.tensor(test_result)\n",
    "\n",
    "test_origin = np.array(test_y)\n",
    "test_origin = torch.tensor(test_y)\n",
    "\n",
    "final_metric.update(test_result.squeeze(),test_origin.squeeze())\n",
    "acc = final_metric.compute()\n",
    "\n",
    "acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
