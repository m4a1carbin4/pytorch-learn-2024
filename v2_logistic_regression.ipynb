{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "\n",
    "data_train = datasets.MNIST(root=\"./dataset\",train=True,download=True,transform=transforms.ToTensor())\n",
    "data_test = datasets.MNIST(root=\"./dataset\",train=False,download=True,transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "image, label = data_train[0]\n",
    "\n",
    "plt.imshow(image.squeeze().numpy(),cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "class UnkownNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UnkownNet,self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784,1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,input):\n",
    "\n",
    "        f2 = self.fc1(input)\n",
    "\n",
    "        output = self.activation(f2)\n",
    "\n",
    "        return output\n",
    "    \n",
    "net = UnkownNet()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(),lr=0.01)\n",
    "loss_func = nn.BCELoss()\n",
    "loss_func_test = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = []\n",
    "\n",
    "for x , y in data_train:\n",
    "\n",
    "    if y % 2 != 0:\n",
    "        y = np.array([1])\n",
    "    else :\n",
    "        y = np.array([0])\n",
    "\n",
    "    x = x.squeeze().reshape(-1).numpy()\n",
    "\n",
    "    train_dataset.append([x,y])\n",
    "\n",
    "test_dataset = []\n",
    "\n",
    "for x , y in data_test:\n",
    "\n",
    "    if y % 2 != 0:\n",
    "        y = np.array([1])\n",
    "    else :\n",
    "        y = np.array([0])\n",
    "\n",
    "    x = x.squeeze().reshape(-1).numpy()\n",
    "\n",
    "    test_dataset.append([x,y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import dataset, DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=600,shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=100,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataloader))\n",
    "print(len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/2000 train_loss: 0.572560,test_loss: 0.564816\n",
      "Epoch    2/2000 train_loss: 0.465444,test_loss: 0.457203\n",
      "Epoch    3/2000 train_loss: 0.422180,test_loss: 0.414072\n",
      "Epoch    4/2000 train_loss: 0.398154,test_loss: 0.390033\n",
      "Epoch    5/2000 train_loss: 0.382498,test_loss: 0.374875\n",
      "Epoch    6/2000 train_loss: 0.371387,test_loss: 0.363798\n",
      "Epoch    7/2000 train_loss: 0.362979,test_loss: 0.355382\n",
      "Epoch    8/2000 train_loss: 0.356329,test_loss: 0.348871\n",
      "Epoch    9/2000 train_loss: 0.350901,test_loss: 0.343437\n",
      "Epoch   10/2000 train_loss: 0.346357,test_loss: 0.338946\n",
      "Epoch   11/2000 train_loss: 0.342477,test_loss: 0.335064\n",
      "Epoch   12/2000 train_loss: 0.339088,test_loss: 0.331828\n",
      "Epoch   13/2000 train_loss: 0.336135,test_loss: 0.328863\n",
      "Epoch   14/2000 train_loss: 0.333485,test_loss: 0.326325\n",
      "Epoch   15/2000 train_loss: 0.331095,test_loss: 0.324042\n",
      "Epoch   16/2000 train_loss: 0.328950,test_loss: 0.321812\n",
      "Epoch   17/2000 train_loss: 0.326980,test_loss: 0.319973\n",
      "Epoch   18/2000 train_loss: 0.325151,test_loss: 0.318171\n",
      "Epoch   19/2000 train_loss: 0.323509,test_loss: 0.316493\n",
      "Epoch   20/2000 train_loss: 0.321941,test_loss: 0.315013\n",
      "Epoch   21/2000 train_loss: 0.320493,test_loss: 0.313505\n",
      "Epoch   22/2000 train_loss: 0.319138,test_loss: 0.312273\n",
      "Epoch   23/2000 train_loss: 0.317865,test_loss: 0.311076\n",
      "Epoch   24/2000 train_loss: 0.316666,test_loss: 0.309905\n",
      "Epoch   25/2000 train_loss: 0.315531,test_loss: 0.308831\n",
      "Epoch   26/2000 train_loss: 0.314439,test_loss: 0.307740\n",
      "Epoch   27/2000 train_loss: 0.313421,test_loss: 0.306803\n",
      "Epoch   28/2000 train_loss: 0.312449,test_loss: 0.305883\n",
      "Epoch   29/2000 train_loss: 0.311523,test_loss: 0.304943\n",
      "Epoch   30/2000 train_loss: 0.310632,test_loss: 0.304208\n",
      "Epoch   31/2000 train_loss: 0.309774,test_loss: 0.303380\n",
      "Epoch   32/2000 train_loss: 0.308955,test_loss: 0.302529\n",
      "Epoch   33/2000 train_loss: 0.308161,test_loss: 0.301879\n",
      "Epoch   34/2000 train_loss: 0.307418,test_loss: 0.301129\n",
      "Epoch   35/2000 train_loss: 0.306684,test_loss: 0.300447\n",
      "Epoch   36/2000 train_loss: 0.305983,test_loss: 0.299801\n",
      "Epoch   37/2000 train_loss: 0.305305,test_loss: 0.299130\n",
      "Epoch   38/2000 train_loss: 0.304635,test_loss: 0.298517\n",
      "Epoch   39/2000 train_loss: 0.304014,test_loss: 0.298034\n",
      "Epoch   40/2000 train_loss: 0.303387,test_loss: 0.297413\n",
      "Epoch   41/2000 train_loss: 0.302811,test_loss: 0.296847\n",
      "Epoch   42/2000 train_loss: 0.302227,test_loss: 0.296315\n",
      "Epoch   43/2000 train_loss: 0.301657,test_loss: 0.295816\n",
      "Epoch   44/2000 train_loss: 0.301091,test_loss: 0.295367\n",
      "Epoch   45/2000 train_loss: 0.300597,test_loss: 0.294769\n",
      "Epoch   46/2000 train_loss: 0.300078,test_loss: 0.294335\n",
      "Epoch   47/2000 train_loss: 0.299563,test_loss: 0.293891\n",
      "Epoch   48/2000 train_loss: 0.299078,test_loss: 0.293544\n",
      "Epoch   49/2000 train_loss: 0.298594,test_loss: 0.292955\n",
      "Epoch   50/2000 train_loss: 0.298133,test_loss: 0.292572\n",
      "Epoch   51/2000 train_loss: 0.297671,test_loss: 0.292139\n",
      "Epoch   52/2000 train_loss: 0.297231,test_loss: 0.291787\n",
      "Epoch   53/2000 train_loss: 0.296787,test_loss: 0.291317\n",
      "Epoch   54/2000 train_loss: 0.296363,test_loss: 0.291038\n",
      "Epoch   55/2000 train_loss: 0.295942,test_loss: 0.290660\n",
      "Epoch   56/2000 train_loss: 0.295548,test_loss: 0.290321\n",
      "Epoch   57/2000 train_loss: 0.295140,test_loss: 0.289975\n",
      "Epoch   58/2000 train_loss: 0.294768,test_loss: 0.289592\n",
      "Epoch   59/2000 train_loss: 0.294370,test_loss: 0.289248\n",
      "Epoch   60/2000 train_loss: 0.294006,test_loss: 0.288935\n",
      "Epoch   61/2000 train_loss: 0.293641,test_loss: 0.288554\n",
      "Epoch   62/2000 train_loss: 0.293275,test_loss: 0.288256\n",
      "Epoch   63/2000 train_loss: 0.292930,test_loss: 0.287985\n",
      "Epoch   64/2000 train_loss: 0.292583,test_loss: 0.287685\n",
      "Epoch   65/2000 train_loss: 0.292245,test_loss: 0.287344\n",
      "Epoch   66/2000 train_loss: 0.291910,test_loss: 0.287082\n",
      "Epoch   67/2000 train_loss: 0.291586,test_loss: 0.286789\n",
      "Epoch   68/2000 train_loss: 0.291266,test_loss: 0.286423\n",
      "Epoch   69/2000 train_loss: 0.290949,test_loss: 0.286186\n",
      "Epoch   70/2000 train_loss: 0.290638,test_loss: 0.285967\n",
      "Epoch   71/2000 train_loss: 0.290330,test_loss: 0.285698\n",
      "Epoch   72/2000 train_loss: 0.290032,test_loss: 0.285425\n",
      "Epoch   73/2000 train_loss: 0.289743,test_loss: 0.285213\n",
      "Epoch   74/2000 train_loss: 0.289459,test_loss: 0.284919\n",
      "Epoch   75/2000 train_loss: 0.289167,test_loss: 0.284685\n",
      "Epoch   76/2000 train_loss: 0.288889,test_loss: 0.284457\n",
      "Epoch   77/2000 train_loss: 0.288619,test_loss: 0.284150\n",
      "Epoch   78/2000 train_loss: 0.288338,test_loss: 0.283952\n",
      "Epoch   79/2000 train_loss: 0.288073,test_loss: 0.283679\n",
      "Epoch   80/2000 train_loss: 0.287796,test_loss: 0.283616\n",
      "Epoch   81/2000 train_loss: 0.287565,test_loss: 0.283235\n",
      "Epoch   82/2000 train_loss: 0.287311,test_loss: 0.283072\n",
      "Epoch   83/2000 train_loss: 0.287049,test_loss: 0.282800\n",
      "Epoch   84/2000 train_loss: 0.286812,test_loss: 0.282663\n",
      "Epoch   85/2000 train_loss: 0.286565,test_loss: 0.282475\n",
      "Epoch   86/2000 train_loss: 0.286326,test_loss: 0.282236\n",
      "Epoch   87/2000 train_loss: 0.286100,test_loss: 0.281985\n",
      "Epoch   88/2000 train_loss: 0.285859,test_loss: 0.281809\n",
      "Epoch   89/2000 train_loss: 0.285624,test_loss: 0.281649\n",
      "Epoch   90/2000 train_loss: 0.285393,test_loss: 0.281419\n",
      "Epoch   91/2000 train_loss: 0.285170,test_loss: 0.281294\n",
      "Epoch   92/2000 train_loss: 0.284960,test_loss: 0.281066\n",
      "Epoch   93/2000 train_loss: 0.284742,test_loss: 0.280853\n",
      "Epoch   94/2000 train_loss: 0.284528,test_loss: 0.280715\n",
      "Epoch   95/2000 train_loss: 0.284329,test_loss: 0.280540\n",
      "Epoch   96/2000 train_loss: 0.284119,test_loss: 0.280355\n",
      "Epoch   97/2000 train_loss: 0.283911,test_loss: 0.280200\n",
      "Epoch   98/2000 train_loss: 0.283715,test_loss: 0.279978\n",
      "Epoch   99/2000 train_loss: 0.283497,test_loss: 0.279914\n",
      "Epoch  100/2000 train_loss: 0.283308,test_loss: 0.279601\n",
      "Epoch  101/2000 train_loss: 0.283118,test_loss: 0.279536\n",
      "Epoch  102/2000 train_loss: 0.282914,test_loss: 0.279303\n",
      "Epoch  103/2000 train_loss: 0.282733,test_loss: 0.279192\n",
      "Epoch  104/2000 train_loss: 0.282548,test_loss: 0.278980\n",
      "Epoch  105/2000 train_loss: 0.282362,test_loss: 0.278854\n",
      "Epoch  106/2000 train_loss: 0.282185,test_loss: 0.278754\n",
      "Epoch  107/2000 train_loss: 0.282002,test_loss: 0.278574\n",
      "Epoch  108/2000 train_loss: 0.281818,test_loss: 0.278483\n",
      "Epoch  109/2000 train_loss: 0.281650,test_loss: 0.278228\n",
      "Epoch  110/2000 train_loss: 0.281479,test_loss: 0.278133\n",
      "Epoch  111/2000 train_loss: 0.281311,test_loss: 0.277953\n",
      "Epoch  112/2000 train_loss: 0.281134,test_loss: 0.277845\n",
      "Epoch  113/2000 train_loss: 0.280966,test_loss: 0.277637\n",
      "Epoch  114/2000 train_loss: 0.280802,test_loss: 0.277527\n",
      "Epoch  115/2000 train_loss: 0.280644,test_loss: 0.277437\n",
      "Epoch  116/2000 train_loss: 0.280482,test_loss: 0.277299\n",
      "Epoch  117/2000 train_loss: 0.280319,test_loss: 0.277105\n",
      "Epoch  118/2000 train_loss: 0.280156,test_loss: 0.277059\n",
      "Epoch  119/2000 train_loss: 0.279991,test_loss: 0.276916\n",
      "Epoch  120/2000 train_loss: 0.279855,test_loss: 0.276786\n",
      "Epoch  121/2000 train_loss: 0.279699,test_loss: 0.276665\n",
      "Epoch  122/2000 train_loss: 0.279555,test_loss: 0.276520\n",
      "Epoch  123/2000 train_loss: 0.279390,test_loss: 0.276444\n",
      "Epoch  124/2000 train_loss: 0.279259,test_loss: 0.276254\n",
      "Epoch  125/2000 train_loss: 0.279116,test_loss: 0.276169\n",
      "Epoch  126/2000 train_loss: 0.278957,test_loss: 0.276109\n",
      "Epoch  127/2000 train_loss: 0.278820,test_loss: 0.275865\n",
      "Epoch  128/2000 train_loss: 0.278680,test_loss: 0.275844\n",
      "Epoch  129/2000 train_loss: 0.278546,test_loss: 0.275724\n",
      "Epoch  130/2000 train_loss: 0.278393,test_loss: 0.275575\n",
      "Epoch  131/2000 train_loss: 0.278271,test_loss: 0.275470\n",
      "Epoch  132/2000 train_loss: 0.278135,test_loss: 0.275383\n",
      "Epoch  133/2000 train_loss: 0.278017,test_loss: 0.275228\n",
      "Epoch  134/2000 train_loss: 0.277868,test_loss: 0.275181\n",
      "Epoch  135/2000 train_loss: 0.277739,test_loss: 0.274996\n",
      "Epoch  136/2000 train_loss: 0.277599,test_loss: 0.274905\n",
      "Epoch  137/2000 train_loss: 0.277475,test_loss: 0.274864\n",
      "Epoch  138/2000 train_loss: 0.277350,test_loss: 0.274771\n",
      "Epoch  139/2000 train_loss: 0.277234,test_loss: 0.274671\n",
      "Epoch  140/2000 train_loss: 0.277111,test_loss: 0.274504\n",
      "Epoch  141/2000 train_loss: 0.276987,test_loss: 0.274413\n",
      "Epoch  142/2000 train_loss: 0.276867,test_loss: 0.274324\n",
      "Epoch  143/2000 train_loss: 0.276726,test_loss: 0.274299\n",
      "Epoch  144/2000 train_loss: 0.276627,test_loss: 0.274150\n",
      "Epoch  145/2000 train_loss: 0.276507,test_loss: 0.274070\n",
      "Epoch  146/2000 train_loss: 0.276394,test_loss: 0.273956\n",
      "Epoch  147/2000 train_loss: 0.276283,test_loss: 0.273788\n",
      "Epoch  148/2000 train_loss: 0.276134,test_loss: 0.273768\n",
      "Epoch  149/2000 train_loss: 0.276041,test_loss: 0.273611\n",
      "Epoch  150/2000 train_loss: 0.275937,test_loss: 0.273595\n",
      "Epoch  151/2000 train_loss: 0.275825,test_loss: 0.273418\n",
      "Epoch  152/2000 train_loss: 0.275721,test_loss: 0.273395\n",
      "Epoch  153/2000 train_loss: 0.275599,test_loss: 0.273219\n",
      "Epoch  154/2000 train_loss: 0.275498,test_loss: 0.273181\n",
      "Epoch  155/2000 train_loss: 0.275401,test_loss: 0.273159\n",
      "Epoch  156/2000 train_loss: 0.275295,test_loss: 0.273045\n",
      "Epoch  157/2000 train_loss: 0.275188,test_loss: 0.272896\n",
      "Epoch  158/2000 train_loss: 0.275064,test_loss: 0.272888\n",
      "Epoch  159/2000 train_loss: 0.274976,test_loss: 0.272806\n",
      "Epoch  160/2000 train_loss: 0.274881,test_loss: 0.272724\n",
      "Epoch  161/2000 train_loss: 0.274781,test_loss: 0.272648\n",
      "Epoch  162/2000 train_loss: 0.274673,test_loss: 0.272551\n",
      "Epoch  163/2000 train_loss: 0.274581,test_loss: 0.272497\n",
      "Epoch  164/2000 train_loss: 0.274479,test_loss: 0.272403\n",
      "Epoch  165/2000 train_loss: 0.274382,test_loss: 0.272280\n",
      "Epoch  166/2000 train_loss: 0.274281,test_loss: 0.272238\n",
      "Epoch  167/2000 train_loss: 0.274193,test_loss: 0.272211\n",
      "Epoch  168/2000 train_loss: 0.274096,test_loss: 0.272049\n",
      "Epoch  169/2000 train_loss: 0.274001,test_loss: 0.271973\n",
      "Epoch  170/2000 train_loss: 0.273897,test_loss: 0.271825\n",
      "Epoch  171/2000 train_loss: 0.273815,test_loss: 0.271896\n",
      "Epoch  172/2000 train_loss: 0.273722,test_loss: 0.271813\n",
      "Epoch  173/2000 train_loss: 0.273627,test_loss: 0.271726\n",
      "Epoch  174/2000 train_loss: 0.273534,test_loss: 0.271594\n",
      "Epoch  175/2000 train_loss: 0.273444,test_loss: 0.271515\n",
      "Epoch  176/2000 train_loss: 0.273360,test_loss: 0.271449\n",
      "Epoch  177/2000 train_loss: 0.273272,test_loss: 0.271414\n",
      "Epoch  178/2000 train_loss: 0.273190,test_loss: 0.271354\n",
      "Epoch  179/2000 train_loss: 0.273103,test_loss: 0.271276\n",
      "Epoch  180/2000 train_loss: 0.273034,test_loss: 0.271185\n",
      "Epoch  181/2000 train_loss: 0.272939,test_loss: 0.271108\n",
      "Epoch  182/2000 train_loss: 0.272856,test_loss: 0.271131\n",
      "Epoch  183/2000 train_loss: 0.272762,test_loss: 0.271035\n",
      "Epoch  184/2000 train_loss: 0.272656,test_loss: 0.270860\n",
      "Epoch  185/2000 train_loss: 0.272619,test_loss: 0.270865\n",
      "Epoch  186/2000 train_loss: 0.272527,test_loss: 0.270771\n",
      "Epoch  187/2000 train_loss: 0.272451,test_loss: 0.270715\n",
      "Epoch  188/2000 train_loss: 0.272369,test_loss: 0.270676\n",
      "Epoch  189/2000 train_loss: 0.272284,test_loss: 0.270622\n",
      "Epoch  190/2000 train_loss: 0.272203,test_loss: 0.270663\n",
      "Epoch  191/2000 train_loss: 0.272133,test_loss: 0.270422\n",
      "Epoch  192/2000 train_loss: 0.272022,test_loss: 0.270527\n",
      "Epoch  193/2000 train_loss: 0.271983,test_loss: 0.270392\n",
      "Epoch  194/2000 train_loss: 0.271910,test_loss: 0.270345\n",
      "Epoch  195/2000 train_loss: 0.271821,test_loss: 0.270338\n",
      "Epoch  196/2000 train_loss: 0.271745,test_loss: 0.270184\n",
      "Epoch  197/2000 train_loss: 0.271685,test_loss: 0.270126\n",
      "Epoch  198/2000 train_loss: 0.271603,test_loss: 0.270091\n",
      "Epoch  199/2000 train_loss: 0.271513,test_loss: 0.269977\n",
      "Epoch  200/2000 train_loss: 0.271464,test_loss: 0.269986\n",
      "Epoch  201/2000 train_loss: 0.271392,test_loss: 0.269897\n",
      "Epoch  202/2000 train_loss: 0.271311,test_loss: 0.269825\n",
      "Epoch  203/2000 train_loss: 0.271237,test_loss: 0.269812\n",
      "Epoch  204/2000 train_loss: 0.271173,test_loss: 0.269807\n",
      "Epoch  205/2000 train_loss: 0.271107,test_loss: 0.269675\n",
      "Epoch  206/2000 train_loss: 0.271034,test_loss: 0.269662\n",
      "Epoch  207/2000 train_loss: 0.270964,test_loss: 0.269637\n",
      "Epoch  208/2000 train_loss: 0.270891,test_loss: 0.269547\n",
      "Epoch  209/2000 train_loss: 0.270816,test_loss: 0.269478\n",
      "Epoch  210/2000 train_loss: 0.270755,test_loss: 0.269449\n",
      "Epoch  211/2000 train_loss: 0.270700,test_loss: 0.269403\n",
      "Epoch  212/2000 train_loss: 0.270627,test_loss: 0.269377\n",
      "Epoch  213/2000 train_loss: 0.270566,test_loss: 0.269266\n",
      "Epoch  214/2000 train_loss: 0.270498,test_loss: 0.269212\n",
      "Epoch  215/2000 train_loss: 0.270432,test_loss: 0.269212\n",
      "Epoch  216/2000 train_loss: 0.270366,test_loss: 0.269123\n",
      "Epoch  217/2000 train_loss: 0.270304,test_loss: 0.269106\n",
      "Epoch  218/2000 train_loss: 0.270242,test_loss: 0.269048\n",
      "Epoch  219/2000 train_loss: 0.270183,test_loss: 0.269006\n",
      "Epoch  220/2000 train_loss: 0.270108,test_loss: 0.268955\n",
      "Epoch  221/2000 train_loss: 0.270050,test_loss: 0.268791\n",
      "Epoch  222/2000 train_loss: 0.269981,test_loss: 0.268834\n",
      "Epoch  223/2000 train_loss: 0.269941,test_loss: 0.268849\n",
      "Epoch  224/2000 train_loss: 0.269872,test_loss: 0.268756\n",
      "Epoch  225/2000 train_loss: 0.269810,test_loss: 0.268608\n",
      "Epoch  226/2000 train_loss: 0.269746,test_loss: 0.268674\n",
      "Epoch  227/2000 train_loss: 0.269694,test_loss: 0.268608\n",
      "Epoch  228/2000 train_loss: 0.269628,test_loss: 0.268592\n",
      "Epoch  229/2000 train_loss: 0.269568,test_loss: 0.268526\n",
      "Epoch  230/2000 train_loss: 0.269523,test_loss: 0.268462\n",
      "Epoch  231/2000 train_loss: 0.269454,test_loss: 0.268411\n",
      "Epoch  232/2000 train_loss: 0.269398,test_loss: 0.268346\n",
      "Epoch  233/2000 train_loss: 0.269345,test_loss: 0.268249\n",
      "Epoch  234/2000 train_loss: 0.269262,test_loss: 0.268289\n",
      "Epoch  235/2000 train_loss: 0.269219,test_loss: 0.268199\n",
      "Epoch  236/2000 train_loss: 0.269156,test_loss: 0.268149\n",
      "Epoch  237/2000 train_loss: 0.269116,test_loss: 0.268167\n",
      "Epoch  238/2000 train_loss: 0.269070,test_loss: 0.268081\n",
      "Epoch  239/2000 train_loss: 0.269015,test_loss: 0.268113\n",
      "Epoch  240/2000 train_loss: 0.268961,test_loss: 0.268062\n",
      "Epoch  241/2000 train_loss: 0.268897,test_loss: 0.267878\n",
      "Epoch  242/2000 train_loss: 0.268838,test_loss: 0.267962\n",
      "Epoch  243/2000 train_loss: 0.268790,test_loss: 0.267950\n",
      "Epoch  244/2000 train_loss: 0.268744,test_loss: 0.267821\n",
      "Epoch  245/2000 train_loss: 0.268688,test_loss: 0.267745\n",
      "Epoch  246/2000 train_loss: 0.268625,test_loss: 0.267790\n",
      "Epoch  247/2000 train_loss: 0.268570,test_loss: 0.267743\n",
      "Epoch  248/2000 train_loss: 0.268529,test_loss: 0.267817\n",
      "Epoch  249/2000 train_loss: 0.268486,test_loss: 0.267654\n",
      "Epoch  250/2000 train_loss: 0.268430,test_loss: 0.267645\n",
      "Epoch  251/2000 train_loss: 0.268337,test_loss: 0.267674\n",
      "Epoch  252/2000 train_loss: 0.268341,test_loss: 0.267542\n",
      "Epoch  253/2000 train_loss: 0.268276,test_loss: 0.267504\n",
      "Epoch  254/2000 train_loss: 0.268233,test_loss: 0.267469\n",
      "Epoch  255/2000 train_loss: 0.268178,test_loss: 0.267455\n",
      "Epoch  256/2000 train_loss: 0.268132,test_loss: 0.267398\n",
      "Epoch  257/2000 train_loss: 0.268084,test_loss: 0.267329\n",
      "Epoch  258/2000 train_loss: 0.268035,test_loss: 0.267320\n",
      "Epoch  259/2000 train_loss: 0.267984,test_loss: 0.267278\n",
      "Epoch  260/2000 train_loss: 0.267935,test_loss: 0.267331\n",
      "Epoch  261/2000 train_loss: 0.267877,test_loss: 0.267061\n",
      "Epoch  262/2000 train_loss: 0.267843,test_loss: 0.267156\n",
      "Epoch  263/2000 train_loss: 0.267804,test_loss: 0.267111\n",
      "Epoch  264/2000 train_loss: 0.267749,test_loss: 0.267100\n",
      "Epoch  265/2000 train_loss: 0.267698,test_loss: 0.267098\n",
      "Epoch  266/2000 train_loss: 0.267649,test_loss: 0.267041\n",
      "Epoch  267/2000 train_loss: 0.267607,test_loss: 0.266967\n",
      "Epoch  268/2000 train_loss: 0.267562,test_loss: 0.266983\n",
      "Epoch  269/2000 train_loss: 0.267523,test_loss: 0.266900\n",
      "Epoch  270/2000 train_loss: 0.267474,test_loss: 0.266816\n",
      "Epoch  271/2000 train_loss: 0.267424,test_loss: 0.266875\n",
      "Epoch  272/2000 train_loss: 0.267390,test_loss: 0.266820\n",
      "Epoch  273/2000 train_loss: 0.267341,test_loss: 0.266792\n",
      "Epoch  274/2000 train_loss: 0.267289,test_loss: 0.266754\n",
      "Epoch  275/2000 train_loss: 0.267237,test_loss: 0.266691\n",
      "Epoch  276/2000 train_loss: 0.267197,test_loss: 0.266704\n",
      "Epoch  277/2000 train_loss: 0.267156,test_loss: 0.266647\n",
      "Epoch  278/2000 train_loss: 0.267123,test_loss: 0.266686\n",
      "Epoch  279/2000 train_loss: 0.267077,test_loss: 0.266551\n",
      "Epoch  280/2000 train_loss: 0.267039,test_loss: 0.266534\n",
      "Epoch  281/2000 train_loss: 0.266992,test_loss: 0.266497\n",
      "Epoch  282/2000 train_loss: 0.266948,test_loss: 0.266449\n",
      "Epoch  283/2000 train_loss: 0.266907,test_loss: 0.266461\n",
      "Epoch  284/2000 train_loss: 0.266863,test_loss: 0.266463\n",
      "Epoch  285/2000 train_loss: 0.266829,test_loss: 0.266438\n",
      "Epoch  286/2000 train_loss: 0.266783,test_loss: 0.266345\n",
      "Epoch  287/2000 train_loss: 0.266744,test_loss: 0.266309\n",
      "Epoch  288/2000 train_loss: 0.266704,test_loss: 0.266235\n",
      "Epoch  289/2000 train_loss: 0.266670,test_loss: 0.266315\n",
      "Epoch  290/2000 train_loss: 0.266622,test_loss: 0.266179\n",
      "Epoch  291/2000 train_loss: 0.266586,test_loss: 0.266222\n",
      "Epoch  292/2000 train_loss: 0.266550,test_loss: 0.266182\n",
      "Epoch  293/2000 train_loss: 0.266500,test_loss: 0.266149\n",
      "Epoch  294/2000 train_loss: 0.266446,test_loss: 0.266230\n",
      "Epoch  295/2000 train_loss: 0.266400,test_loss: 0.266072\n",
      "Epoch  296/2000 train_loss: 0.266386,test_loss: 0.266025\n",
      "Epoch  297/2000 train_loss: 0.266344,test_loss: 0.266001\n",
      "Epoch  298/2000 train_loss: 0.266314,test_loss: 0.265984\n",
      "Epoch  299/2000 train_loss: 0.266275,test_loss: 0.265960\n",
      "Epoch  300/2000 train_loss: 0.266239,test_loss: 0.265980\n",
      "Epoch  301/2000 train_loss: 0.266187,test_loss: 0.265904\n",
      "Epoch  302/2000 train_loss: 0.266162,test_loss: 0.265843\n",
      "Epoch  303/2000 train_loss: 0.266112,test_loss: 0.265886\n",
      "Epoch  304/2000 train_loss: 0.266093,test_loss: 0.265827\n",
      "Epoch  305/2000 train_loss: 0.266045,test_loss: 0.265790\n",
      "Epoch  306/2000 train_loss: 0.266009,test_loss: 0.265720\n",
      "Epoch  307/2000 train_loss: 0.265962,test_loss: 0.265745\n",
      "Epoch  308/2000 train_loss: 0.265940,test_loss: 0.265767\n",
      "Epoch  309/2000 train_loss: 0.265899,test_loss: 0.265715\n",
      "Epoch  310/2000 train_loss: 0.265847,test_loss: 0.265623\n",
      "Epoch  311/2000 train_loss: 0.265831,test_loss: 0.265628\n",
      "Epoch  312/2000 train_loss: 0.265790,test_loss: 0.265638\n",
      "Epoch  313/2000 train_loss: 0.265749,test_loss: 0.265618\n",
      "Epoch  314/2000 train_loss: 0.265714,test_loss: 0.265518\n",
      "Epoch  315/2000 train_loss: 0.265686,test_loss: 0.265567\n",
      "Epoch  316/2000 train_loss: 0.265646,test_loss: 0.265528\n",
      "Epoch  317/2000 train_loss: 0.265622,test_loss: 0.265498\n",
      "Epoch  318/2000 train_loss: 0.265584,test_loss: 0.265461\n",
      "Epoch  319/2000 train_loss: 0.265541,test_loss: 0.265321\n",
      "Epoch  320/2000 train_loss: 0.265511,test_loss: 0.265393\n",
      "Epoch  321/2000 train_loss: 0.265473,test_loss: 0.265352\n",
      "Epoch  322/2000 train_loss: 0.265454,test_loss: 0.265408\n",
      "Epoch  323/2000 train_loss: 0.265419,test_loss: 0.265394\n",
      "Epoch  324/2000 train_loss: 0.265374,test_loss: 0.265309\n",
      "Epoch  325/2000 train_loss: 0.265341,test_loss: 0.265204\n",
      "Epoch  326/2000 train_loss: 0.265313,test_loss: 0.265212\n",
      "Epoch  327/2000 train_loss: 0.265276,test_loss: 0.265205\n",
      "Epoch  328/2000 train_loss: 0.265235,test_loss: 0.265153\n",
      "Epoch  329/2000 train_loss: 0.265200,test_loss: 0.265212\n",
      "Epoch  330/2000 train_loss: 0.265177,test_loss: 0.265166\n",
      "Epoch  331/2000 train_loss: 0.265141,test_loss: 0.265140\n",
      "Epoch  332/2000 train_loss: 0.265110,test_loss: 0.265117\n",
      "Epoch  333/2000 train_loss: 0.265081,test_loss: 0.265098\n",
      "Epoch  334/2000 train_loss: 0.265047,test_loss: 0.265021\n",
      "Epoch  335/2000 train_loss: 0.265006,test_loss: 0.265045\n",
      "Epoch  336/2000 train_loss: 0.264979,test_loss: 0.265015\n",
      "Epoch  337/2000 train_loss: 0.264950,test_loss: 0.265022\n",
      "Epoch  338/2000 train_loss: 0.264924,test_loss: 0.264969\n",
      "Epoch  339/2000 train_loss: 0.264894,test_loss: 0.265029\n",
      "Epoch  340/2000 train_loss: 0.264855,test_loss: 0.264817\n",
      "Epoch  341/2000 train_loss: 0.264831,test_loss: 0.264934\n",
      "Epoch  342/2000 train_loss: 0.264795,test_loss: 0.264887\n",
      "Epoch  343/2000 train_loss: 0.264767,test_loss: 0.264856\n",
      "Epoch  344/2000 train_loss: 0.264723,test_loss: 0.264798\n",
      "Epoch  345/2000 train_loss: 0.264705,test_loss: 0.264783\n",
      "Epoch  346/2000 train_loss: 0.264674,test_loss: 0.264738\n",
      "Epoch  347/2000 train_loss: 0.264646,test_loss: 0.264774\n",
      "Epoch  348/2000 train_loss: 0.264615,test_loss: 0.264737\n",
      "Epoch  349/2000 train_loss: 0.264580,test_loss: 0.264699\n",
      "Epoch  350/2000 train_loss: 0.264554,test_loss: 0.264657\n",
      "Epoch  351/2000 train_loss: 0.264528,test_loss: 0.264682\n",
      "Epoch  352/2000 train_loss: 0.264497,test_loss: 0.264640\n",
      "Epoch  353/2000 train_loss: 0.264462,test_loss: 0.264585\n",
      "Epoch  354/2000 train_loss: 0.264432,test_loss: 0.264597\n",
      "Epoch  355/2000 train_loss: 0.264414,test_loss: 0.264628\n",
      "Epoch  356/2000 train_loss: 0.264384,test_loss: 0.264536\n",
      "Epoch  357/2000 train_loss: 0.264341,test_loss: 0.264507\n",
      "Epoch  358/2000 train_loss: 0.264321,test_loss: 0.264512\n",
      "Epoch  359/2000 train_loss: 0.264285,test_loss: 0.264495\n",
      "Epoch  360/2000 train_loss: 0.264266,test_loss: 0.264467\n",
      "Epoch  361/2000 train_loss: 0.264235,test_loss: 0.264451\n",
      "Epoch  362/2000 train_loss: 0.264211,test_loss: 0.264325\n",
      "Epoch  363/2000 train_loss: 0.264186,test_loss: 0.264452\n",
      "Epoch  364/2000 train_loss: 0.264150,test_loss: 0.264407\n",
      "Epoch  365/2000 train_loss: 0.264106,test_loss: 0.264245\n",
      "Epoch  366/2000 train_loss: 0.264087,test_loss: 0.264383\n",
      "Epoch  367/2000 train_loss: 0.264057,test_loss: 0.264273\n",
      "Epoch  368/2000 train_loss: 0.264042,test_loss: 0.264305\n",
      "Epoch  369/2000 train_loss: 0.264014,test_loss: 0.264325\n",
      "Epoch  370/2000 train_loss: 0.263969,test_loss: 0.264295\n",
      "Epoch  371/2000 train_loss: 0.263963,test_loss: 0.264231\n",
      "Epoch  372/2000 train_loss: 0.263932,test_loss: 0.264179\n",
      "Epoch  373/2000 train_loss: 0.263910,test_loss: 0.264223\n",
      "Epoch  374/2000 train_loss: 0.263880,test_loss: 0.264142\n",
      "Epoch  375/2000 train_loss: 0.263848,test_loss: 0.264218\n",
      "Epoch  376/2000 train_loss: 0.263821,test_loss: 0.264144\n",
      "Epoch  377/2000 train_loss: 0.263796,test_loss: 0.264179\n",
      "Epoch  378/2000 train_loss: 0.263782,test_loss: 0.264077\n",
      "Epoch  379/2000 train_loss: 0.263728,test_loss: 0.264065\n",
      "Epoch  380/2000 train_loss: 0.263713,test_loss: 0.264019\n",
      "Epoch  381/2000 train_loss: 0.263693,test_loss: 0.264042\n",
      "Epoch  382/2000 train_loss: 0.263666,test_loss: 0.263979\n",
      "Epoch  383/2000 train_loss: 0.263641,test_loss: 0.264012\n",
      "Epoch  384/2000 train_loss: 0.263615,test_loss: 0.264026\n",
      "Epoch  385/2000 train_loss: 0.263589,test_loss: 0.263950\n",
      "Epoch  386/2000 train_loss: 0.263553,test_loss: 0.264062\n",
      "Epoch  387/2000 train_loss: 0.263549,test_loss: 0.263937\n",
      "Epoch  388/2000 train_loss: 0.263520,test_loss: 0.263869\n",
      "Epoch  389/2000 train_loss: 0.263497,test_loss: 0.263931\n",
      "Epoch  390/2000 train_loss: 0.263464,test_loss: 0.263911\n",
      "Epoch  391/2000 train_loss: 0.263436,test_loss: 0.263869\n",
      "Epoch  392/2000 train_loss: 0.263409,test_loss: 0.263823\n",
      "Epoch  393/2000 train_loss: 0.263388,test_loss: 0.263838\n",
      "Epoch  394/2000 train_loss: 0.263371,test_loss: 0.263876\n",
      "Epoch  395/2000 train_loss: 0.263336,test_loss: 0.263798\n",
      "Epoch  396/2000 train_loss: 0.263317,test_loss: 0.263769\n",
      "Epoch  397/2000 train_loss: 0.263294,test_loss: 0.263772\n",
      "Epoch  398/2000 train_loss: 0.263266,test_loss: 0.263640\n",
      "Epoch  399/2000 train_loss: 0.263243,test_loss: 0.263728\n",
      "Epoch  400/2000 train_loss: 0.263217,test_loss: 0.263685\n",
      "Epoch  401/2000 train_loss: 0.263199,test_loss: 0.263631\n",
      "Epoch  402/2000 train_loss: 0.263168,test_loss: 0.263676\n",
      "Epoch  403/2000 train_loss: 0.263154,test_loss: 0.263624\n",
      "Epoch  404/2000 train_loss: 0.263123,test_loss: 0.263595\n",
      "Epoch  405/2000 train_loss: 0.263097,test_loss: 0.263647\n",
      "Epoch  406/2000 train_loss: 0.263065,test_loss: 0.263648\n",
      "Epoch  407/2000 train_loss: 0.263040,test_loss: 0.263621\n",
      "Epoch  408/2000 train_loss: 0.263040,test_loss: 0.263575\n",
      "Epoch  409/2000 train_loss: 0.263012,test_loss: 0.263579\n",
      "Epoch  410/2000 train_loss: 0.262975,test_loss: 0.263589\n",
      "Epoch  411/2000 train_loss: 0.262955,test_loss: 0.263513\n",
      "Epoch  412/2000 train_loss: 0.262937,test_loss: 0.263466\n",
      "Epoch  413/2000 train_loss: 0.262916,test_loss: 0.263384\n",
      "Epoch  414/2000 train_loss: 0.262893,test_loss: 0.263418\n",
      "Epoch  415/2000 train_loss: 0.262877,test_loss: 0.263472\n",
      "Epoch  416/2000 train_loss: 0.262839,test_loss: 0.263429\n",
      "Epoch  417/2000 train_loss: 0.262831,test_loss: 0.263485\n",
      "Epoch  418/2000 train_loss: 0.262807,test_loss: 0.263400\n",
      "Epoch  419/2000 train_loss: 0.262784,test_loss: 0.263358\n",
      "Epoch  420/2000 train_loss: 0.262753,test_loss: 0.263352\n",
      "Epoch  421/2000 train_loss: 0.262745,test_loss: 0.263407\n",
      "Epoch  422/2000 train_loss: 0.262715,test_loss: 0.263452\n",
      "Epoch  423/2000 train_loss: 0.262696,test_loss: 0.263276\n",
      "Epoch  424/2000 train_loss: 0.262675,test_loss: 0.263334\n",
      "Epoch  425/2000 train_loss: 0.262658,test_loss: 0.263291\n",
      "Epoch  426/2000 train_loss: 0.262618,test_loss: 0.263229\n",
      "Epoch  427/2000 train_loss: 0.262594,test_loss: 0.263299\n",
      "Epoch  428/2000 train_loss: 0.262585,test_loss: 0.263261\n",
      "Epoch  429/2000 train_loss: 0.262565,test_loss: 0.263251\n",
      "Epoch  430/2000 train_loss: 0.262545,test_loss: 0.263201\n",
      "Epoch  431/2000 train_loss: 0.262513,test_loss: 0.263167\n",
      "Epoch  432/2000 train_loss: 0.262503,test_loss: 0.263133\n",
      "Epoch  433/2000 train_loss: 0.262462,test_loss: 0.263186\n",
      "Epoch  434/2000 train_loss: 0.262460,test_loss: 0.263091\n",
      "Epoch  435/2000 train_loss: 0.262432,test_loss: 0.263155\n",
      "Epoch  436/2000 train_loss: 0.262405,test_loss: 0.263091\n",
      "Epoch  437/2000 train_loss: 0.262379,test_loss: 0.263095\n",
      "Epoch  438/2000 train_loss: 0.262364,test_loss: 0.263078\n",
      "Epoch  439/2000 train_loss: 0.262346,test_loss: 0.263028\n",
      "Epoch  440/2000 train_loss: 0.262333,test_loss: 0.263063\n",
      "Epoch  441/2000 train_loss: 0.262307,test_loss: 0.263029\n",
      "Epoch  442/2000 train_loss: 0.262292,test_loss: 0.263099\n",
      "Epoch  443/2000 train_loss: 0.262256,test_loss: 0.263055\n",
      "Epoch  444/2000 train_loss: 0.262245,test_loss: 0.262978\n",
      "Epoch  445/2000 train_loss: 0.262230,test_loss: 0.263019\n",
      "Epoch  446/2000 train_loss: 0.262212,test_loss: 0.262947\n",
      "Epoch  447/2000 train_loss: 0.262187,test_loss: 0.262935\n",
      "Epoch  448/2000 train_loss: 0.262171,test_loss: 0.262987\n",
      "Epoch  449/2000 train_loss: 0.262155,test_loss: 0.262887\n",
      "Epoch  450/2000 train_loss: 0.262127,test_loss: 0.262895\n",
      "Epoch  451/2000 train_loss: 0.262110,test_loss: 0.262889\n",
      "Epoch  452/2000 train_loss: 0.262085,test_loss: 0.262804\n",
      "Epoch  453/2000 train_loss: 0.262066,test_loss: 0.262893\n",
      "Epoch  454/2000 train_loss: 0.262052,test_loss: 0.262949\n",
      "Epoch  455/2000 train_loss: 0.262032,test_loss: 0.262817\n",
      "Epoch  456/2000 train_loss: 0.262009,test_loss: 0.262826\n",
      "Epoch  457/2000 train_loss: 0.261996,test_loss: 0.262729\n",
      "Epoch  458/2000 train_loss: 0.261977,test_loss: 0.262795\n",
      "Epoch  459/2000 train_loss: 0.261949,test_loss: 0.262740\n",
      "Epoch  460/2000 train_loss: 0.261937,test_loss: 0.262789\n",
      "Epoch  461/2000 train_loss: 0.261911,test_loss: 0.262753\n",
      "Epoch  462/2000 train_loss: 0.261898,test_loss: 0.262763\n",
      "Epoch  463/2000 train_loss: 0.261882,test_loss: 0.262717\n",
      "Epoch  464/2000 train_loss: 0.261864,test_loss: 0.262714\n",
      "Epoch  465/2000 train_loss: 0.261834,test_loss: 0.262692\n",
      "Epoch  466/2000 train_loss: 0.261822,test_loss: 0.262711\n",
      "Epoch  467/2000 train_loss: 0.261800,test_loss: 0.262675\n",
      "Epoch  468/2000 train_loss: 0.261779,test_loss: 0.262642\n",
      "Epoch  469/2000 train_loss: 0.261743,test_loss: 0.262682\n",
      "Epoch  470/2000 train_loss: 0.261746,test_loss: 0.262679\n",
      "Epoch  471/2000 train_loss: 0.261726,test_loss: 0.262594\n",
      "Epoch  472/2000 train_loss: 0.261714,test_loss: 0.262661\n",
      "Epoch  473/2000 train_loss: 0.261684,test_loss: 0.262560\n",
      "Epoch  474/2000 train_loss: 0.261671,test_loss: 0.262559\n",
      "Epoch  475/2000 train_loss: 0.261652,test_loss: 0.262604\n",
      "Epoch  476/2000 train_loss: 0.261630,test_loss: 0.262553\n",
      "Epoch  477/2000 train_loss: 0.261618,test_loss: 0.262563\n",
      "Epoch  478/2000 train_loss: 0.261595,test_loss: 0.262525\n",
      "Epoch  479/2000 train_loss: 0.261577,test_loss: 0.262515\n",
      "Epoch  480/2000 train_loss: 0.261560,test_loss: 0.262505\n",
      "Epoch  481/2000 train_loss: 0.261537,test_loss: 0.262476\n",
      "Epoch  482/2000 train_loss: 0.261523,test_loss: 0.262495\n",
      "Epoch  483/2000 train_loss: 0.261516,test_loss: 0.262462\n",
      "Epoch  484/2000 train_loss: 0.261485,test_loss: 0.262438\n",
      "Epoch  485/2000 train_loss: 0.261475,test_loss: 0.262464\n",
      "Epoch  486/2000 train_loss: 0.261446,test_loss: 0.262396\n",
      "Epoch  487/2000 train_loss: 0.261436,test_loss: 0.262370\n",
      "Epoch  488/2000 train_loss: 0.261422,test_loss: 0.262431\n",
      "Epoch  489/2000 train_loss: 0.261399,test_loss: 0.262383\n",
      "Epoch  490/2000 train_loss: 0.261383,test_loss: 0.262432\n",
      "Epoch  491/2000 train_loss: 0.261370,test_loss: 0.262406\n",
      "Epoch  492/2000 train_loss: 0.261348,test_loss: 0.262347\n",
      "Epoch  493/2000 train_loss: 0.261319,test_loss: 0.262373\n",
      "Epoch  494/2000 train_loss: 0.261322,test_loss: 0.262314\n",
      "Epoch  495/2000 train_loss: 0.261286,test_loss: 0.262355\n",
      "Epoch  496/2000 train_loss: 0.261281,test_loss: 0.262376\n",
      "Epoch  497/2000 train_loss: 0.261252,test_loss: 0.262364\n",
      "Epoch  498/2000 train_loss: 0.261247,test_loss: 0.262288\n",
      "Epoch  499/2000 train_loss: 0.261229,test_loss: 0.262261\n",
      "Epoch  500/2000 train_loss: 0.261213,test_loss: 0.262250\n",
      "Epoch  501/2000 train_loss: 0.261181,test_loss: 0.262230\n",
      "Epoch  502/2000 train_loss: 0.261171,test_loss: 0.262175\n",
      "Epoch  503/2000 train_loss: 0.261162,test_loss: 0.262217\n",
      "Epoch  504/2000 train_loss: 0.261153,test_loss: 0.262178\n",
      "Epoch  505/2000 train_loss: 0.261126,test_loss: 0.262180\n",
      "Epoch  506/2000 train_loss: 0.261115,test_loss: 0.262212\n",
      "Epoch  507/2000 train_loss: 0.261097,test_loss: 0.262167\n",
      "Epoch  508/2000 train_loss: 0.261085,test_loss: 0.262186\n",
      "Epoch  509/2000 train_loss: 0.261039,test_loss: 0.262218\n",
      "Epoch  510/2000 train_loss: 0.261060,test_loss: 0.262133\n",
      "Epoch  511/2000 train_loss: 0.261034,test_loss: 0.262108\n",
      "Epoch  512/2000 train_loss: 0.261014,test_loss: 0.262099\n",
      "Epoch  513/2000 train_loss: 0.261003,test_loss: 0.262077\n",
      "Epoch  514/2000 train_loss: 0.260985,test_loss: 0.262142\n",
      "Epoch  515/2000 train_loss: 0.260967,test_loss: 0.262100\n",
      "Epoch  516/2000 train_loss: 0.260951,test_loss: 0.262058\n",
      "Epoch  517/2000 train_loss: 0.260924,test_loss: 0.262140\n",
      "Epoch  518/2000 train_loss: 0.260908,test_loss: 0.262064\n",
      "Epoch  519/2000 train_loss: 0.260882,test_loss: 0.262001\n",
      "Epoch  520/2000 train_loss: 0.260890,test_loss: 0.262045\n",
      "Epoch  521/2000 train_loss: 0.260856,test_loss: 0.262064\n",
      "Epoch  522/2000 train_loss: 0.260851,test_loss: 0.261923\n",
      "Epoch  523/2000 train_loss: 0.260839,test_loss: 0.261985\n",
      "Epoch  524/2000 train_loss: 0.260814,test_loss: 0.262041\n",
      "Epoch  525/2000 train_loss: 0.260804,test_loss: 0.262014\n",
      "Epoch  526/2000 train_loss: 0.260798,test_loss: 0.261995\n",
      "Epoch  527/2000 train_loss: 0.260777,test_loss: 0.261903\n",
      "Epoch  528/2000 train_loss: 0.260757,test_loss: 0.261978\n",
      "Epoch  529/2000 train_loss: 0.260742,test_loss: 0.261936\n",
      "Epoch  530/2000 train_loss: 0.260734,test_loss: 0.261963\n",
      "Epoch  531/2000 train_loss: 0.260707,test_loss: 0.261907\n",
      "Epoch  532/2000 train_loss: 0.260697,test_loss: 0.261945\n",
      "Epoch  533/2000 train_loss: 0.260677,test_loss: 0.261921\n",
      "Epoch  534/2000 train_loss: 0.260665,test_loss: 0.261796\n",
      "Epoch  535/2000 train_loss: 0.260645,test_loss: 0.261834\n",
      "Epoch  536/2000 train_loss: 0.260621,test_loss: 0.261792\n",
      "Epoch  537/2000 train_loss: 0.260629,test_loss: 0.261806\n",
      "Epoch  538/2000 train_loss: 0.260602,test_loss: 0.261853\n",
      "Epoch  539/2000 train_loss: 0.260586,test_loss: 0.261848\n",
      "Epoch  540/2000 train_loss: 0.260584,test_loss: 0.261807\n",
      "Epoch  541/2000 train_loss: 0.260569,test_loss: 0.261809\n",
      "Epoch  542/2000 train_loss: 0.260544,test_loss: 0.261802\n",
      "Epoch  543/2000 train_loss: 0.260532,test_loss: 0.261814\n",
      "Epoch  544/2000 train_loss: 0.260505,test_loss: 0.261778\n",
      "Epoch  545/2000 train_loss: 0.260509,test_loss: 0.261776\n",
      "Epoch  546/2000 train_loss: 0.260490,test_loss: 0.261799\n",
      "Epoch  547/2000 train_loss: 0.260479,test_loss: 0.261684\n",
      "Epoch  548/2000 train_loss: 0.260452,test_loss: 0.261668\n",
      "Epoch  549/2000 train_loss: 0.260450,test_loss: 0.261707\n",
      "Epoch  550/2000 train_loss: 0.260425,test_loss: 0.261672\n",
      "Epoch  551/2000 train_loss: 0.260417,test_loss: 0.261765\n",
      "Epoch  552/2000 train_loss: 0.260392,test_loss: 0.261712\n",
      "Epoch  553/2000 train_loss: 0.260378,test_loss: 0.261682\n",
      "Epoch  554/2000 train_loss: 0.260371,test_loss: 0.261638\n",
      "Epoch  555/2000 train_loss: 0.260355,test_loss: 0.261711\n",
      "Epoch  556/2000 train_loss: 0.260338,test_loss: 0.261665\n",
      "Epoch  557/2000 train_loss: 0.260325,test_loss: 0.261638\n",
      "Epoch  558/2000 train_loss: 0.260315,test_loss: 0.261653\n",
      "Epoch  559/2000 train_loss: 0.260301,test_loss: 0.261603\n",
      "Epoch  560/2000 train_loss: 0.260281,test_loss: 0.261533\n",
      "Epoch  561/2000 train_loss: 0.260261,test_loss: 0.261594\n",
      "Epoch  562/2000 train_loss: 0.260254,test_loss: 0.261577\n",
      "Epoch  563/2000 train_loss: 0.260245,test_loss: 0.261561\n",
      "Epoch  564/2000 train_loss: 0.260231,test_loss: 0.261602\n",
      "Epoch  565/2000 train_loss: 0.260220,test_loss: 0.261505\n",
      "Epoch  566/2000 train_loss: 0.260197,test_loss: 0.261599\n",
      "Epoch  567/2000 train_loss: 0.260189,test_loss: 0.261538\n",
      "Epoch  568/2000 train_loss: 0.260165,test_loss: 0.261548\n",
      "Epoch  569/2000 train_loss: 0.260159,test_loss: 0.261508\n",
      "Epoch  570/2000 train_loss: 0.260147,test_loss: 0.261516\n",
      "Epoch  571/2000 train_loss: 0.260130,test_loss: 0.261462\n",
      "Epoch  572/2000 train_loss: 0.260120,test_loss: 0.261422\n",
      "Epoch  573/2000 train_loss: 0.260100,test_loss: 0.261473\n",
      "Epoch  574/2000 train_loss: 0.260092,test_loss: 0.261460\n",
      "Epoch  575/2000 train_loss: 0.260078,test_loss: 0.261551\n",
      "Epoch  576/2000 train_loss: 0.260068,test_loss: 0.261479\n",
      "Epoch  577/2000 train_loss: 0.260060,test_loss: 0.261421\n",
      "Epoch  578/2000 train_loss: 0.260033,test_loss: 0.261356\n",
      "Epoch  579/2000 train_loss: 0.260029,test_loss: 0.261429\n",
      "Epoch  580/2000 train_loss: 0.260013,test_loss: 0.261393\n",
      "Epoch  581/2000 train_loss: 0.259994,test_loss: 0.261438\n",
      "Epoch  582/2000 train_loss: 0.259972,test_loss: 0.261415\n",
      "Epoch  583/2000 train_loss: 0.259966,test_loss: 0.261395\n",
      "Epoch  584/2000 train_loss: 0.259956,test_loss: 0.261460\n",
      "Epoch  585/2000 train_loss: 0.259943,test_loss: 0.261365\n",
      "Epoch  586/2000 train_loss: 0.259935,test_loss: 0.261448\n",
      "Epoch  587/2000 train_loss: 0.259918,test_loss: 0.261359\n",
      "Epoch  588/2000 train_loss: 0.259901,test_loss: 0.261324\n",
      "Epoch  589/2000 train_loss: 0.259891,test_loss: 0.261258\n",
      "Epoch  590/2000 train_loss: 0.259877,test_loss: 0.261280\n",
      "Epoch  591/2000 train_loss: 0.259872,test_loss: 0.261287\n",
      "Epoch  592/2000 train_loss: 0.259848,test_loss: 0.261312\n",
      "Epoch  593/2000 train_loss: 0.259846,test_loss: 0.261284\n",
      "Epoch  594/2000 train_loss: 0.259830,test_loss: 0.261241\n",
      "Epoch  595/2000 train_loss: 0.259797,test_loss: 0.261293\n",
      "Epoch  596/2000 train_loss: 0.259797,test_loss: 0.261199\n",
      "Epoch  597/2000 train_loss: 0.259780,test_loss: 0.261255\n",
      "Epoch  598/2000 train_loss: 0.259782,test_loss: 0.261212\n",
      "Epoch  599/2000 train_loss: 0.259752,test_loss: 0.261223\n",
      "Epoch  600/2000 train_loss: 0.259752,test_loss: 0.261232\n",
      "Epoch  601/2000 train_loss: 0.259740,test_loss: 0.261254\n",
      "Epoch  602/2000 train_loss: 0.259729,test_loss: 0.261248\n",
      "Epoch  603/2000 train_loss: 0.259716,test_loss: 0.261191\n",
      "Epoch  604/2000 train_loss: 0.259690,test_loss: 0.261168\n",
      "Epoch  605/2000 train_loss: 0.259686,test_loss: 0.261127\n",
      "Epoch  606/2000 train_loss: 0.259676,test_loss: 0.261188\n",
      "Epoch  607/2000 train_loss: 0.259660,test_loss: 0.261115\n",
      "Epoch  608/2000 train_loss: 0.259643,test_loss: 0.261137\n",
      "Epoch  609/2000 train_loss: 0.259640,test_loss: 0.261177\n",
      "Epoch  610/2000 train_loss: 0.259626,test_loss: 0.261113\n",
      "Epoch  611/2000 train_loss: 0.259610,test_loss: 0.261072\n",
      "Epoch  612/2000 train_loss: 0.259607,test_loss: 0.261121\n",
      "Epoch  613/2000 train_loss: 0.259579,test_loss: 0.261121\n",
      "Epoch  614/2000 train_loss: 0.259576,test_loss: 0.261066\n",
      "Epoch  615/2000 train_loss: 0.259571,test_loss: 0.261030\n",
      "Epoch  616/2000 train_loss: 0.259548,test_loss: 0.261050\n",
      "Epoch  617/2000 train_loss: 0.259538,test_loss: 0.261118\n",
      "Epoch  618/2000 train_loss: 0.259523,test_loss: 0.261046\n",
      "Epoch  619/2000 train_loss: 0.259518,test_loss: 0.261085\n",
      "Epoch  620/2000 train_loss: 0.259504,test_loss: 0.261081\n",
      "Epoch  621/2000 train_loss: 0.259493,test_loss: 0.261058\n",
      "Epoch  622/2000 train_loss: 0.259473,test_loss: 0.260991\n",
      "Epoch  623/2000 train_loss: 0.259471,test_loss: 0.261005\n",
      "Epoch  624/2000 train_loss: 0.259446,test_loss: 0.261021\n",
      "Epoch  625/2000 train_loss: 0.259444,test_loss: 0.261039\n",
      "Epoch  626/2000 train_loss: 0.259422,test_loss: 0.260960\n",
      "Epoch  627/2000 train_loss: 0.259411,test_loss: 0.260993\n",
      "Epoch  628/2000 train_loss: 0.259402,test_loss: 0.260996\n",
      "Epoch  629/2000 train_loss: 0.259409,test_loss: 0.260978\n",
      "Epoch  630/2000 train_loss: 0.259381,test_loss: 0.260945\n",
      "Epoch  631/2000 train_loss: 0.259372,test_loss: 0.261010\n",
      "Epoch  632/2000 train_loss: 0.259360,test_loss: 0.260919\n",
      "Epoch  633/2000 train_loss: 0.259353,test_loss: 0.260921\n",
      "Epoch  634/2000 train_loss: 0.259338,test_loss: 0.260945\n",
      "Epoch  635/2000 train_loss: 0.259329,test_loss: 0.260941\n",
      "Epoch  636/2000 train_loss: 0.259312,test_loss: 0.260918\n",
      "Epoch  637/2000 train_loss: 0.259301,test_loss: 0.260904\n",
      "Epoch  638/2000 train_loss: 0.259289,test_loss: 0.260881\n",
      "Epoch  639/2000 train_loss: 0.259272,test_loss: 0.260918\n",
      "Epoch  640/2000 train_loss: 0.259264,test_loss: 0.260901\n",
      "Epoch  641/2000 train_loss: 0.259255,test_loss: 0.260884\n",
      "Epoch  642/2000 train_loss: 0.259255,test_loss: 0.260860\n",
      "Epoch  643/2000 train_loss: 0.259233,test_loss: 0.260840\n",
      "Epoch  644/2000 train_loss: 0.259221,test_loss: 0.260906\n",
      "Epoch  645/2000 train_loss: 0.259207,test_loss: 0.260809\n",
      "Epoch  646/2000 train_loss: 0.259201,test_loss: 0.260870\n",
      "Epoch  647/2000 train_loss: 0.259192,test_loss: 0.260811\n",
      "Epoch  648/2000 train_loss: 0.259174,test_loss: 0.260804\n",
      "Epoch  649/2000 train_loss: 0.259143,test_loss: 0.260797\n",
      "Epoch  650/2000 train_loss: 0.259160,test_loss: 0.260844\n",
      "Epoch  651/2000 train_loss: 0.259132,test_loss: 0.260904\n",
      "Epoch  652/2000 train_loss: 0.259119,test_loss: 0.260713\n",
      "Epoch  653/2000 train_loss: 0.259126,test_loss: 0.260790\n",
      "Epoch  654/2000 train_loss: 0.259111,test_loss: 0.260748\n",
      "Epoch  655/2000 train_loss: 0.259099,test_loss: 0.260757\n",
      "Epoch  656/2000 train_loss: 0.259082,test_loss: 0.260770\n",
      "Epoch  657/2000 train_loss: 0.259053,test_loss: 0.260802\n",
      "Epoch  658/2000 train_loss: 0.259075,test_loss: 0.260720\n",
      "Epoch  659/2000 train_loss: 0.259050,test_loss: 0.260699\n",
      "Epoch  660/2000 train_loss: 0.259043,test_loss: 0.260718\n",
      "Epoch  661/2000 train_loss: 0.259030,test_loss: 0.260676\n",
      "Epoch  662/2000 train_loss: 0.259007,test_loss: 0.260638\n",
      "Epoch  663/2000 train_loss: 0.259018,test_loss: 0.260679\n",
      "Epoch  664/2000 train_loss: 0.259004,test_loss: 0.260655\n",
      "Epoch  665/2000 train_loss: 0.258985,test_loss: 0.260731\n",
      "Epoch  666/2000 train_loss: 0.258973,test_loss: 0.260700\n",
      "Epoch  667/2000 train_loss: 0.258965,test_loss: 0.260617\n",
      "Epoch  668/2000 train_loss: 0.258954,test_loss: 0.260669\n",
      "Epoch  669/2000 train_loss: 0.258938,test_loss: 0.260631\n",
      "Epoch  670/2000 train_loss: 0.258931,test_loss: 0.260635\n",
      "Epoch  671/2000 train_loss: 0.258919,test_loss: 0.260665\n",
      "Epoch  672/2000 train_loss: 0.258906,test_loss: 0.260687\n",
      "Epoch  673/2000 train_loss: 0.258904,test_loss: 0.260622\n",
      "Epoch  674/2000 train_loss: 0.258892,test_loss: 0.260662\n",
      "Epoch  675/2000 train_loss: 0.258882,test_loss: 0.260636\n",
      "Epoch  676/2000 train_loss: 0.258861,test_loss: 0.260676\n",
      "Epoch  677/2000 train_loss: 0.258858,test_loss: 0.260641\n",
      "Epoch  678/2000 train_loss: 0.258844,test_loss: 0.260611\n",
      "Epoch  679/2000 train_loss: 0.258836,test_loss: 0.260573\n",
      "Epoch  680/2000 train_loss: 0.258823,test_loss: 0.260616\n",
      "Epoch  681/2000 train_loss: 0.258816,test_loss: 0.260597\n",
      "Epoch  682/2000 train_loss: 0.258801,test_loss: 0.260510\n",
      "Epoch  683/2000 train_loss: 0.258784,test_loss: 0.260515\n",
      "Epoch  684/2000 train_loss: 0.258788,test_loss: 0.260516\n",
      "Epoch  685/2000 train_loss: 0.258775,test_loss: 0.260604\n",
      "Epoch  686/2000 train_loss: 0.258763,test_loss: 0.260475\n",
      "Epoch  687/2000 train_loss: 0.258756,test_loss: 0.260525\n",
      "Epoch  688/2000 train_loss: 0.258735,test_loss: 0.260476\n",
      "Epoch  689/2000 train_loss: 0.258727,test_loss: 0.260435\n",
      "Epoch  690/2000 train_loss: 0.258724,test_loss: 0.260540\n",
      "Epoch  691/2000 train_loss: 0.258712,test_loss: 0.260494\n",
      "Epoch  692/2000 train_loss: 0.258692,test_loss: 0.260480\n",
      "Epoch  693/2000 train_loss: 0.258693,test_loss: 0.260495\n",
      "Epoch  694/2000 train_loss: 0.258675,test_loss: 0.260491\n",
      "Epoch  695/2000 train_loss: 0.258666,test_loss: 0.260437\n",
      "Epoch  696/2000 train_loss: 0.258667,test_loss: 0.260469\n",
      "Epoch  697/2000 train_loss: 0.258641,test_loss: 0.260414\n",
      "Epoch  698/2000 train_loss: 0.258644,test_loss: 0.260433\n",
      "Epoch  699/2000 train_loss: 0.258630,test_loss: 0.260489\n",
      "Epoch  700/2000 train_loss: 0.258618,test_loss: 0.260497\n",
      "Epoch  701/2000 train_loss: 0.258621,test_loss: 0.260402\n",
      "Epoch  702/2000 train_loss: 0.258601,test_loss: 0.260427\n",
      "Epoch  703/2000 train_loss: 0.258589,test_loss: 0.260428\n",
      "Epoch  704/2000 train_loss: 0.258589,test_loss: 0.260442\n",
      "Epoch  705/2000 train_loss: 0.258571,test_loss: 0.260434\n",
      "Epoch  706/2000 train_loss: 0.258560,test_loss: 0.260366\n",
      "Epoch  707/2000 train_loss: 0.258556,test_loss: 0.260351\n",
      "Epoch  708/2000 train_loss: 0.258548,test_loss: 0.260330\n",
      "Epoch  709/2000 train_loss: 0.258536,test_loss: 0.260357\n",
      "Epoch  710/2000 train_loss: 0.258521,test_loss: 0.260465\n",
      "Epoch  711/2000 train_loss: 0.258523,test_loss: 0.260307\n",
      "Epoch  712/2000 train_loss: 0.258503,test_loss: 0.260340\n",
      "Epoch  713/2000 train_loss: 0.258500,test_loss: 0.260368\n",
      "Epoch  714/2000 train_loss: 0.258484,test_loss: 0.260315\n",
      "Epoch  715/2000 train_loss: 0.258471,test_loss: 0.260304\n",
      "Epoch  716/2000 train_loss: 0.258464,test_loss: 0.260296\n",
      "Epoch  717/2000 train_loss: 0.258453,test_loss: 0.260342\n",
      "Epoch  718/2000 train_loss: 0.258443,test_loss: 0.260295\n",
      "Epoch  719/2000 train_loss: 0.258427,test_loss: 0.260266\n",
      "Epoch  720/2000 train_loss: 0.258426,test_loss: 0.260329\n",
      "Epoch  721/2000 train_loss: 0.258416,test_loss: 0.260285\n",
      "Epoch  722/2000 train_loss: 0.258405,test_loss: 0.260223\n",
      "Epoch  723/2000 train_loss: 0.258403,test_loss: 0.260297\n",
      "Epoch  724/2000 train_loss: 0.258367,test_loss: 0.260234\n",
      "Epoch  725/2000 train_loss: 0.258365,test_loss: 0.260253\n",
      "Epoch  726/2000 train_loss: 0.258379,test_loss: 0.260234\n",
      "Epoch  727/2000 train_loss: 0.258358,test_loss: 0.260274\n",
      "Epoch  728/2000 train_loss: 0.258349,test_loss: 0.260212\n",
      "Epoch  729/2000 train_loss: 0.258342,test_loss: 0.260277\n",
      "Epoch  730/2000 train_loss: 0.258333,test_loss: 0.260254\n",
      "Epoch  731/2000 train_loss: 0.258315,test_loss: 0.260199\n",
      "Epoch  732/2000 train_loss: 0.258320,test_loss: 0.260237\n",
      "Epoch  733/2000 train_loss: 0.258296,test_loss: 0.260170\n",
      "Epoch  734/2000 train_loss: 0.258292,test_loss: 0.260190\n",
      "Epoch  735/2000 train_loss: 0.258285,test_loss: 0.260200\n",
      "Epoch  736/2000 train_loss: 0.258275,test_loss: 0.260150\n",
      "Epoch  737/2000 train_loss: 0.258267,test_loss: 0.260148\n",
      "Epoch  738/2000 train_loss: 0.258250,test_loss: 0.260151\n",
      "Epoch  739/2000 train_loss: 0.258246,test_loss: 0.260186\n",
      "Epoch  740/2000 train_loss: 0.258227,test_loss: 0.260233\n",
      "Epoch  741/2000 train_loss: 0.258234,test_loss: 0.260171\n",
      "Epoch  742/2000 train_loss: 0.258217,test_loss: 0.260195\n",
      "Epoch  743/2000 train_loss: 0.258213,test_loss: 0.260175\n",
      "Epoch  744/2000 train_loss: 0.258195,test_loss: 0.260100\n",
      "Epoch  745/2000 train_loss: 0.258186,test_loss: 0.260190\n",
      "Epoch  746/2000 train_loss: 0.258185,test_loss: 0.260155\n",
      "Epoch  747/2000 train_loss: 0.258178,test_loss: 0.260090\n",
      "Epoch  748/2000 train_loss: 0.258164,test_loss: 0.260141\n",
      "Epoch  749/2000 train_loss: 0.258154,test_loss: 0.260114\n",
      "Epoch  750/2000 train_loss: 0.258144,test_loss: 0.260079\n",
      "Epoch  751/2000 train_loss: 0.258127,test_loss: 0.260077\n",
      "Epoch  752/2000 train_loss: 0.258127,test_loss: 0.260114\n",
      "Epoch  753/2000 train_loss: 0.258117,test_loss: 0.260100\n",
      "Epoch  754/2000 train_loss: 0.258108,test_loss: 0.260117\n",
      "Epoch  755/2000 train_loss: 0.258096,test_loss: 0.260122\n",
      "Epoch  756/2000 train_loss: 0.258096,test_loss: 0.260105\n",
      "Epoch  757/2000 train_loss: 0.258085,test_loss: 0.260069\n",
      "Epoch  758/2000 train_loss: 0.258071,test_loss: 0.260124\n",
      "Epoch  759/2000 train_loss: 0.258063,test_loss: 0.260077\n",
      "Epoch  760/2000 train_loss: 0.258062,test_loss: 0.260015\n",
      "Epoch  761/2000 train_loss: 0.258043,test_loss: 0.260107\n",
      "Epoch  762/2000 train_loss: 0.258041,test_loss: 0.260053\n",
      "Epoch  763/2000 train_loss: 0.258025,test_loss: 0.259991\n",
      "Epoch  764/2000 train_loss: 0.258025,test_loss: 0.259988\n",
      "Epoch  765/2000 train_loss: 0.258010,test_loss: 0.260012\n",
      "Epoch  766/2000 train_loss: 0.258006,test_loss: 0.260058\n",
      "Epoch  767/2000 train_loss: 0.257993,test_loss: 0.259997\n",
      "Epoch  768/2000 train_loss: 0.257986,test_loss: 0.259958\n",
      "Epoch  769/2000 train_loss: 0.257971,test_loss: 0.259996\n",
      "Epoch  770/2000 train_loss: 0.257974,test_loss: 0.259918\n",
      "Epoch  771/2000 train_loss: 0.257951,test_loss: 0.260029\n",
      "Epoch  772/2000 train_loss: 0.257955,test_loss: 0.259969\n",
      "Epoch  773/2000 train_loss: 0.257945,test_loss: 0.259987\n",
      "Epoch  774/2000 train_loss: 0.257938,test_loss: 0.259932\n",
      "Epoch  775/2000 train_loss: 0.257925,test_loss: 0.259917\n",
      "Epoch  776/2000 train_loss: 0.257913,test_loss: 0.259969\n",
      "Epoch  777/2000 train_loss: 0.257912,test_loss: 0.259984\n",
      "Epoch  778/2000 train_loss: 0.257904,test_loss: 0.259921\n",
      "Epoch  779/2000 train_loss: 0.257896,test_loss: 0.259944\n",
      "Epoch  780/2000 train_loss: 0.257882,test_loss: 0.259955\n",
      "Epoch  781/2000 train_loss: 0.257879,test_loss: 0.259946\n",
      "Epoch  782/2000 train_loss: 0.257872,test_loss: 0.259952\n",
      "Epoch  783/2000 train_loss: 0.257862,test_loss: 0.259848\n",
      "Epoch  784/2000 train_loss: 0.257851,test_loss: 0.259928\n",
      "Epoch  785/2000 train_loss: 0.257847,test_loss: 0.259907\n",
      "Epoch  786/2000 train_loss: 0.257825,test_loss: 0.259852\n",
      "Epoch  787/2000 train_loss: 0.257823,test_loss: 0.259884\n",
      "Epoch  788/2000 train_loss: 0.257827,test_loss: 0.259839\n",
      "Epoch  789/2000 train_loss: 0.257813,test_loss: 0.259920\n",
      "Epoch  790/2000 train_loss: 0.257809,test_loss: 0.259844\n",
      "Epoch  791/2000 train_loss: 0.257786,test_loss: 0.259823\n",
      "Epoch  792/2000 train_loss: 0.257763,test_loss: 0.259864\n",
      "Epoch  793/2000 train_loss: 0.257774,test_loss: 0.259873\n",
      "Epoch  794/2000 train_loss: 0.257734,test_loss: 0.259849\n",
      "Epoch  795/2000 train_loss: 0.257764,test_loss: 0.259870\n",
      "Epoch  796/2000 train_loss: 0.257748,test_loss: 0.259809\n",
      "Epoch  797/2000 train_loss: 0.257740,test_loss: 0.259859\n",
      "Epoch  798/2000 train_loss: 0.257732,test_loss: 0.259823\n",
      "Epoch  799/2000 train_loss: 0.257724,test_loss: 0.259801\n",
      "Epoch  800/2000 train_loss: 0.257718,test_loss: 0.259801\n",
      "Epoch  801/2000 train_loss: 0.257713,test_loss: 0.259834\n",
      "Epoch  802/2000 train_loss: 0.257701,test_loss: 0.259810\n",
      "Epoch  803/2000 train_loss: 0.257694,test_loss: 0.259765\n",
      "Epoch  804/2000 train_loss: 0.257670,test_loss: 0.259789\n",
      "Epoch  805/2000 train_loss: 0.257679,test_loss: 0.259773\n",
      "Epoch  806/2000 train_loss: 0.257665,test_loss: 0.259767\n",
      "Epoch  807/2000 train_loss: 0.257665,test_loss: 0.259803\n",
      "Epoch  808/2000 train_loss: 0.257666,test_loss: 0.259790\n",
      "Epoch  809/2000 train_loss: 0.257644,test_loss: 0.259753\n",
      "Epoch  810/2000 train_loss: 0.257627,test_loss: 0.259831\n",
      "Epoch  811/2000 train_loss: 0.257634,test_loss: 0.259771\n",
      "Epoch  812/2000 train_loss: 0.257618,test_loss: 0.259743\n",
      "Epoch  813/2000 train_loss: 0.257615,test_loss: 0.259773\n",
      "Epoch  814/2000 train_loss: 0.257598,test_loss: 0.259780\n",
      "Epoch  815/2000 train_loss: 0.257582,test_loss: 0.259700\n",
      "Epoch  816/2000 train_loss: 0.257594,test_loss: 0.259686\n",
      "Epoch  817/2000 train_loss: 0.257582,test_loss: 0.259708\n",
      "Epoch  818/2000 train_loss: 0.257571,test_loss: 0.259744\n",
      "Epoch  819/2000 train_loss: 0.257562,test_loss: 0.259721\n",
      "Epoch  820/2000 train_loss: 0.257556,test_loss: 0.259709\n",
      "Epoch  821/2000 train_loss: 0.257555,test_loss: 0.259803\n",
      "Epoch  822/2000 train_loss: 0.257547,test_loss: 0.259689\n",
      "Epoch  823/2000 train_loss: 0.257524,test_loss: 0.259703\n",
      "Epoch  824/2000 train_loss: 0.257532,test_loss: 0.259726\n",
      "Epoch  825/2000 train_loss: 0.257513,test_loss: 0.259625\n",
      "Epoch  826/2000 train_loss: 0.257505,test_loss: 0.259625\n",
      "Epoch  827/2000 train_loss: 0.257504,test_loss: 0.259618\n",
      "Epoch  828/2000 train_loss: 0.257491,test_loss: 0.259674\n",
      "Epoch  829/2000 train_loss: 0.257491,test_loss: 0.259659\n",
      "Epoch  830/2000 train_loss: 0.257469,test_loss: 0.259639\n",
      "Epoch  831/2000 train_loss: 0.257466,test_loss: 0.259617\n",
      "Epoch  832/2000 train_loss: 0.257454,test_loss: 0.259626\n",
      "Epoch  833/2000 train_loss: 0.257460,test_loss: 0.259631\n",
      "Epoch  834/2000 train_loss: 0.257446,test_loss: 0.259615\n",
      "Epoch  835/2000 train_loss: 0.257438,test_loss: 0.259614\n",
      "Epoch  836/2000 train_loss: 0.257424,test_loss: 0.259615\n",
      "Epoch  837/2000 train_loss: 0.257430,test_loss: 0.259575\n",
      "Epoch  838/2000 train_loss: 0.257419,test_loss: 0.259642\n",
      "Epoch  839/2000 train_loss: 0.257409,test_loss: 0.259599\n",
      "Epoch  840/2000 train_loss: 0.257403,test_loss: 0.259592\n",
      "Epoch  841/2000 train_loss: 0.257391,test_loss: 0.259641\n",
      "Epoch  842/2000 train_loss: 0.257391,test_loss: 0.259596\n",
      "Epoch  843/2000 train_loss: 0.257382,test_loss: 0.259588\n",
      "Epoch  844/2000 train_loss: 0.257360,test_loss: 0.259656\n",
      "Epoch  845/2000 train_loss: 0.257374,test_loss: 0.259587\n",
      "Epoch  846/2000 train_loss: 0.257346,test_loss: 0.259631\n",
      "Epoch  847/2000 train_loss: 0.257354,test_loss: 0.259541\n",
      "Epoch  848/2000 train_loss: 0.257333,test_loss: 0.259575\n",
      "Epoch  849/2000 train_loss: 0.257337,test_loss: 0.259606\n",
      "Epoch  850/2000 train_loss: 0.257330,test_loss: 0.259589\n",
      "Epoch  851/2000 train_loss: 0.257323,test_loss: 0.259540\n",
      "Epoch  852/2000 train_loss: 0.257317,test_loss: 0.259548\n",
      "Epoch  853/2000 train_loss: 0.257310,test_loss: 0.259596\n",
      "Epoch  854/2000 train_loss: 0.257295,test_loss: 0.259495\n",
      "Epoch  855/2000 train_loss: 0.257285,test_loss: 0.259530\n",
      "Epoch  856/2000 train_loss: 0.257284,test_loss: 0.259496\n",
      "Epoch  857/2000 train_loss: 0.257272,test_loss: 0.259553\n",
      "Epoch  858/2000 train_loss: 0.257268,test_loss: 0.259546\n",
      "Epoch  859/2000 train_loss: 0.257260,test_loss: 0.259451\n",
      "Epoch  860/2000 train_loss: 0.257234,test_loss: 0.259546\n",
      "Epoch  861/2000 train_loss: 0.257238,test_loss: 0.259464\n",
      "Epoch  862/2000 train_loss: 0.257241,test_loss: 0.259530\n",
      "Epoch  863/2000 train_loss: 0.257228,test_loss: 0.259470\n",
      "Epoch  864/2000 train_loss: 0.257220,test_loss: 0.259484\n",
      "Epoch  865/2000 train_loss: 0.257215,test_loss: 0.259535\n",
      "Epoch  866/2000 train_loss: 0.257215,test_loss: 0.259476\n",
      "Epoch  867/2000 train_loss: 0.257201,test_loss: 0.259481\n",
      "Epoch  868/2000 train_loss: 0.257201,test_loss: 0.259451\n",
      "Epoch  869/2000 train_loss: 0.257188,test_loss: 0.259473\n",
      "Epoch  870/2000 train_loss: 0.257158,test_loss: 0.259535\n",
      "Epoch  871/2000 train_loss: 0.257176,test_loss: 0.259516\n",
      "Epoch  872/2000 train_loss: 0.257168,test_loss: 0.259473\n",
      "Epoch  873/2000 train_loss: 0.257158,test_loss: 0.259455\n",
      "Epoch  874/2000 train_loss: 0.257149,test_loss: 0.259464\n",
      "Epoch  875/2000 train_loss: 0.257150,test_loss: 0.259447\n",
      "Epoch  876/2000 train_loss: 0.257138,test_loss: 0.259461\n",
      "Epoch  877/2000 train_loss: 0.257125,test_loss: 0.259468\n",
      "Epoch  878/2000 train_loss: 0.257123,test_loss: 0.259388\n",
      "Epoch  879/2000 train_loss: 0.257115,test_loss: 0.259388\n",
      "Epoch  880/2000 train_loss: 0.257108,test_loss: 0.259322\n",
      "Epoch  881/2000 train_loss: 0.257104,test_loss: 0.259404\n",
      "Epoch  882/2000 train_loss: 0.257083,test_loss: 0.259514\n",
      "Epoch  883/2000 train_loss: 0.257093,test_loss: 0.259421\n",
      "Epoch  884/2000 train_loss: 0.257078,test_loss: 0.259373\n",
      "Epoch  885/2000 train_loss: 0.257076,test_loss: 0.259364\n",
      "Epoch  886/2000 train_loss: 0.257065,test_loss: 0.259408\n",
      "Epoch  887/2000 train_loss: 0.257065,test_loss: 0.259410\n",
      "Epoch  888/2000 train_loss: 0.257059,test_loss: 0.259345\n",
      "Epoch  889/2000 train_loss: 0.257040,test_loss: 0.259392\n",
      "Epoch  890/2000 train_loss: 0.257040,test_loss: 0.259321\n",
      "Epoch  891/2000 train_loss: 0.257030,test_loss: 0.259340\n",
      "Epoch  892/2000 train_loss: 0.257026,test_loss: 0.259428\n",
      "Epoch  893/2000 train_loss: 0.257023,test_loss: 0.259320\n",
      "Epoch  894/2000 train_loss: 0.257009,test_loss: 0.259295\n",
      "Epoch  895/2000 train_loss: 0.257003,test_loss: 0.259376\n",
      "Epoch  896/2000 train_loss: 0.256988,test_loss: 0.259364\n",
      "Epoch  897/2000 train_loss: 0.256985,test_loss: 0.259357\n",
      "Epoch  898/2000 train_loss: 0.256986,test_loss: 0.259366\n",
      "Epoch  899/2000 train_loss: 0.256976,test_loss: 0.259290\n",
      "Epoch  900/2000 train_loss: 0.256983,test_loss: 0.259239\n",
      "Epoch  901/2000 train_loss: 0.256971,test_loss: 0.259307\n",
      "Epoch  902/2000 train_loss: 0.256967,test_loss: 0.259257\n",
      "Epoch  903/2000 train_loss: 0.256948,test_loss: 0.259284\n",
      "Epoch  904/2000 train_loss: 0.256950,test_loss: 0.259340\n",
      "Epoch  905/2000 train_loss: 0.256939,test_loss: 0.259255\n",
      "Epoch  906/2000 train_loss: 0.256937,test_loss: 0.259282\n",
      "Epoch  907/2000 train_loss: 0.256922,test_loss: 0.259268\n",
      "Epoch  908/2000 train_loss: 0.256921,test_loss: 0.259304\n",
      "Epoch  909/2000 train_loss: 0.256913,test_loss: 0.259296\n",
      "Epoch  910/2000 train_loss: 0.256900,test_loss: 0.259242\n",
      "Epoch  911/2000 train_loss: 0.256902,test_loss: 0.259287\n",
      "Epoch  912/2000 train_loss: 0.256892,test_loss: 0.259218\n",
      "Epoch  913/2000 train_loss: 0.256887,test_loss: 0.259250\n",
      "Epoch  914/2000 train_loss: 0.256878,test_loss: 0.259264\n",
      "Epoch  915/2000 train_loss: 0.256868,test_loss: 0.259263\n",
      "Epoch  916/2000 train_loss: 0.256859,test_loss: 0.259270\n",
      "Epoch  917/2000 train_loss: 0.256849,test_loss: 0.259401\n",
      "Epoch  918/2000 train_loss: 0.256854,test_loss: 0.259245\n",
      "Epoch  919/2000 train_loss: 0.256836,test_loss: 0.259261\n",
      "Epoch  920/2000 train_loss: 0.256831,test_loss: 0.259208\n",
      "Epoch  921/2000 train_loss: 0.256828,test_loss: 0.259209\n",
      "Epoch  922/2000 train_loss: 0.256818,test_loss: 0.259255\n",
      "Epoch  923/2000 train_loss: 0.256802,test_loss: 0.259275\n",
      "Epoch  924/2000 train_loss: 0.256799,test_loss: 0.259178\n",
      "Epoch  925/2000 train_loss: 0.256805,test_loss: 0.259217\n",
      "Epoch  926/2000 train_loss: 0.256800,test_loss: 0.259124\n",
      "Epoch  927/2000 train_loss: 0.256794,test_loss: 0.259205\n",
      "Epoch  928/2000 train_loss: 0.256781,test_loss: 0.259124\n",
      "Epoch  929/2000 train_loss: 0.256775,test_loss: 0.259177\n",
      "Epoch  930/2000 train_loss: 0.256775,test_loss: 0.259209\n",
      "Epoch  931/2000 train_loss: 0.256770,test_loss: 0.259189\n",
      "Epoch  932/2000 train_loss: 0.256757,test_loss: 0.259191\n",
      "Epoch  933/2000 train_loss: 0.256755,test_loss: 0.259164\n",
      "Epoch  934/2000 train_loss: 0.256752,test_loss: 0.259202\n",
      "Epoch  935/2000 train_loss: 0.256743,test_loss: 0.259187\n",
      "Epoch  936/2000 train_loss: 0.256731,test_loss: 0.259209\n",
      "Epoch  937/2000 train_loss: 0.256727,test_loss: 0.259158\n",
      "Epoch  938/2000 train_loss: 0.256720,test_loss: 0.259178\n",
      "Epoch  939/2000 train_loss: 0.256716,test_loss: 0.259166\n",
      "Epoch  940/2000 train_loss: 0.256675,test_loss: 0.259129\n",
      "Epoch  941/2000 train_loss: 0.256710,test_loss: 0.259117\n",
      "Epoch  942/2000 train_loss: 0.256685,test_loss: 0.259144\n",
      "Epoch  943/2000 train_loss: 0.256677,test_loss: 0.259150\n",
      "Epoch  944/2000 train_loss: 0.256681,test_loss: 0.259066\n",
      "Epoch  945/2000 train_loss: 0.256672,test_loss: 0.259172\n",
      "Epoch  946/2000 train_loss: 0.256675,test_loss: 0.259137\n",
      "Epoch  947/2000 train_loss: 0.256660,test_loss: 0.259122\n",
      "Epoch  948/2000 train_loss: 0.256637,test_loss: 0.259144\n",
      "Epoch  949/2000 train_loss: 0.256657,test_loss: 0.259101\n",
      "Epoch  950/2000 train_loss: 0.256642,test_loss: 0.259050\n",
      "Epoch  951/2000 train_loss: 0.256633,test_loss: 0.259102\n",
      "Epoch  952/2000 train_loss: 0.256626,test_loss: 0.259141\n",
      "Epoch  953/2000 train_loss: 0.256630,test_loss: 0.259050\n",
      "Epoch  954/2000 train_loss: 0.256619,test_loss: 0.259088\n",
      "Epoch  955/2000 train_loss: 0.256615,test_loss: 0.259084\n",
      "Epoch  956/2000 train_loss: 0.256599,test_loss: 0.259087\n",
      "Epoch  957/2000 train_loss: 0.256601,test_loss: 0.259106\n",
      "Epoch  958/2000 train_loss: 0.256599,test_loss: 0.259040\n",
      "Epoch  959/2000 train_loss: 0.256581,test_loss: 0.259104\n",
      "Epoch  960/2000 train_loss: 0.256580,test_loss: 0.259020\n",
      "Epoch  961/2000 train_loss: 0.256569,test_loss: 0.259095\n",
      "Epoch  962/2000 train_loss: 0.256560,test_loss: 0.259075\n",
      "Epoch  963/2000 train_loss: 0.256553,test_loss: 0.259095\n",
      "Epoch  964/2000 train_loss: 0.256563,test_loss: 0.259035\n",
      "Epoch  965/2000 train_loss: 0.256557,test_loss: 0.259043\n",
      "Epoch  966/2000 train_loss: 0.256549,test_loss: 0.259003\n",
      "Epoch  967/2000 train_loss: 0.256533,test_loss: 0.258979\n",
      "Epoch  968/2000 train_loss: 0.256533,test_loss: 0.259057\n",
      "Epoch  969/2000 train_loss: 0.256531,test_loss: 0.259016\n",
      "Epoch  970/2000 train_loss: 0.256521,test_loss: 0.259012\n",
      "Epoch  971/2000 train_loss: 0.256523,test_loss: 0.258984\n",
      "Epoch  972/2000 train_loss: 0.256506,test_loss: 0.259057\n",
      "Epoch  973/2000 train_loss: 0.256511,test_loss: 0.259005\n",
      "Epoch  974/2000 train_loss: 0.256493,test_loss: 0.259029\n",
      "Epoch  975/2000 train_loss: 0.256487,test_loss: 0.259014\n",
      "Epoch  976/2000 train_loss: 0.256477,test_loss: 0.258988\n",
      "Epoch  977/2000 train_loss: 0.256477,test_loss: 0.259028\n",
      "Epoch  978/2000 train_loss: 0.256469,test_loss: 0.258998\n",
      "Epoch  979/2000 train_loss: 0.256475,test_loss: 0.259011\n",
      "Epoch  980/2000 train_loss: 0.256448,test_loss: 0.259075\n",
      "Epoch  981/2000 train_loss: 0.256453,test_loss: 0.258930\n",
      "Epoch  982/2000 train_loss: 0.256451,test_loss: 0.258981\n",
      "Epoch  983/2000 train_loss: 0.256439,test_loss: 0.258971\n",
      "Epoch  984/2000 train_loss: 0.256437,test_loss: 0.258955\n",
      "Epoch  985/2000 train_loss: 0.256428,test_loss: 0.258973\n",
      "Epoch  986/2000 train_loss: 0.256421,test_loss: 0.258981\n",
      "Epoch  987/2000 train_loss: 0.256423,test_loss: 0.258982\n",
      "Epoch  988/2000 train_loss: 0.256409,test_loss: 0.258924\n",
      "Epoch  989/2000 train_loss: 0.256402,test_loss: 0.258985\n",
      "Epoch  990/2000 train_loss: 0.256392,test_loss: 0.258921\n",
      "Epoch  991/2000 train_loss: 0.256395,test_loss: 0.258979\n",
      "Epoch  992/2000 train_loss: 0.256384,test_loss: 0.258975\n",
      "Epoch  993/2000 train_loss: 0.256381,test_loss: 0.258934\n",
      "Epoch  994/2000 train_loss: 0.256379,test_loss: 0.258958\n",
      "Epoch  995/2000 train_loss: 0.256366,test_loss: 0.258905\n",
      "Epoch  996/2000 train_loss: 0.256369,test_loss: 0.258916\n",
      "Epoch  997/2000 train_loss: 0.256359,test_loss: 0.258880\n",
      "Epoch  998/2000 train_loss: 0.256354,test_loss: 0.258909\n",
      "Epoch  999/2000 train_loss: 0.256349,test_loss: 0.258894\n",
      "Epoch 1000/2000 train_loss: 0.256343,test_loss: 0.258906\n",
      "Epoch 1001/2000 train_loss: 0.256338,test_loss: 0.258914\n",
      "Epoch 1002/2000 train_loss: 0.256337,test_loss: 0.258906\n",
      "Epoch 1003/2000 train_loss: 0.256328,test_loss: 0.258937\n",
      "Epoch 1004/2000 train_loss: 0.256317,test_loss: 0.258871\n",
      "Epoch 1005/2000 train_loss: 0.256266,test_loss: 0.258817\n",
      "Epoch 1006/2000 train_loss: 0.256318,test_loss: 0.258904\n",
      "Epoch 1007/2000 train_loss: 0.256313,test_loss: 0.258861\n",
      "Epoch 1008/2000 train_loss: 0.256299,test_loss: 0.258826\n",
      "Epoch 1009/2000 train_loss: 0.256296,test_loss: 0.258906\n",
      "Epoch 1010/2000 train_loss: 0.256272,test_loss: 0.258870\n",
      "Epoch 1011/2000 train_loss: 0.256289,test_loss: 0.258896\n",
      "Epoch 1012/2000 train_loss: 0.256262,test_loss: 0.258880\n",
      "Epoch 1013/2000 train_loss: 0.256272,test_loss: 0.258903\n",
      "Epoch 1014/2000 train_loss: 0.256266,test_loss: 0.258813\n",
      "Epoch 1015/2000 train_loss: 0.256247,test_loss: 0.258829\n",
      "Epoch 1016/2000 train_loss: 0.256252,test_loss: 0.258860\n",
      "Epoch 1017/2000 train_loss: 0.256245,test_loss: 0.258908\n",
      "Epoch 1018/2000 train_loss: 0.256234,test_loss: 0.258879\n",
      "Epoch 1019/2000 train_loss: 0.256234,test_loss: 0.258855\n",
      "Epoch 1020/2000 train_loss: 0.256220,test_loss: 0.258787\n",
      "Epoch 1021/2000 train_loss: 0.256222,test_loss: 0.258809\n",
      "Epoch 1022/2000 train_loss: 0.256215,test_loss: 0.258799\n",
      "Epoch 1023/2000 train_loss: 0.256211,test_loss: 0.258758\n",
      "Epoch 1024/2000 train_loss: 0.256204,test_loss: 0.258776\n",
      "Epoch 1025/2000 train_loss: 0.256205,test_loss: 0.258807\n",
      "Epoch 1026/2000 train_loss: 0.256195,test_loss: 0.258807\n",
      "Epoch 1027/2000 train_loss: 0.256190,test_loss: 0.258836\n",
      "Epoch 1028/2000 train_loss: 0.256179,test_loss: 0.258804\n",
      "Epoch 1029/2000 train_loss: 0.256173,test_loss: 0.258814\n",
      "Epoch 1030/2000 train_loss: 0.256177,test_loss: 0.258797\n",
      "Epoch 1031/2000 train_loss: 0.256161,test_loss: 0.258678\n",
      "Epoch 1032/2000 train_loss: 0.256164,test_loss: 0.258714\n",
      "Epoch 1033/2000 train_loss: 0.256153,test_loss: 0.258830\n",
      "Epoch 1034/2000 train_loss: 0.256152,test_loss: 0.258750\n",
      "Epoch 1035/2000 train_loss: 0.256143,test_loss: 0.258772\n",
      "Epoch 1036/2000 train_loss: 0.256144,test_loss: 0.258806\n",
      "Epoch 1037/2000 train_loss: 0.256134,test_loss: 0.258775\n",
      "Epoch 1038/2000 train_loss: 0.256129,test_loss: 0.258698\n",
      "Epoch 1039/2000 train_loss: 0.256120,test_loss: 0.258743\n",
      "Epoch 1040/2000 train_loss: 0.256118,test_loss: 0.258761\n",
      "Epoch 1041/2000 train_loss: 0.256103,test_loss: 0.258761\n",
      "Epoch 1042/2000 train_loss: 0.256105,test_loss: 0.258732\n",
      "Epoch 1043/2000 train_loss: 0.256104,test_loss: 0.258800\n",
      "Epoch 1044/2000 train_loss: 0.256091,test_loss: 0.258827\n",
      "Epoch 1045/2000 train_loss: 0.256093,test_loss: 0.258676\n",
      "Epoch 1046/2000 train_loss: 0.256085,test_loss: 0.258750\n",
      "Epoch 1047/2000 train_loss: 0.256068,test_loss: 0.258764\n",
      "Epoch 1048/2000 train_loss: 0.256071,test_loss: 0.258740\n",
      "Epoch 1049/2000 train_loss: 0.256061,test_loss: 0.258692\n",
      "Epoch 1050/2000 train_loss: 0.256074,test_loss: 0.258733\n",
      "Epoch 1051/2000 train_loss: 0.256063,test_loss: 0.258680\n",
      "Epoch 1052/2000 train_loss: 0.256055,test_loss: 0.258695\n",
      "Epoch 1053/2000 train_loss: 0.256048,test_loss: 0.258692\n",
      "Epoch 1054/2000 train_loss: 0.256045,test_loss: 0.258713\n",
      "Epoch 1055/2000 train_loss: 0.256035,test_loss: 0.258755\n",
      "Epoch 1056/2000 train_loss: 0.256036,test_loss: 0.258759\n",
      "Epoch 1057/2000 train_loss: 0.256023,test_loss: 0.258779\n",
      "Epoch 1058/2000 train_loss: 0.256020,test_loss: 0.258736\n",
      "Epoch 1059/2000 train_loss: 0.256018,test_loss: 0.258720\n",
      "Epoch 1060/2000 train_loss: 0.256001,test_loss: 0.258711\n",
      "Epoch 1061/2000 train_loss: 0.256001,test_loss: 0.258697\n",
      "Epoch 1062/2000 train_loss: 0.255998,test_loss: 0.258654\n",
      "Epoch 1063/2000 train_loss: 0.255996,test_loss: 0.258718\n",
      "Epoch 1064/2000 train_loss: 0.255986,test_loss: 0.258689\n",
      "Epoch 1065/2000 train_loss: 0.255982,test_loss: 0.258649\n",
      "Epoch 1066/2000 train_loss: 0.255980,test_loss: 0.258655\n",
      "Epoch 1067/2000 train_loss: 0.255966,test_loss: 0.258689\n",
      "Epoch 1068/2000 train_loss: 0.255966,test_loss: 0.258668\n",
      "Epoch 1069/2000 train_loss: 0.255956,test_loss: 0.258719\n",
      "Epoch 1070/2000 train_loss: 0.255961,test_loss: 0.258609\n",
      "Epoch 1071/2000 train_loss: 0.255954,test_loss: 0.258667\n",
      "Epoch 1072/2000 train_loss: 0.255936,test_loss: 0.258607\n",
      "Epoch 1073/2000 train_loss: 0.255937,test_loss: 0.258594\n",
      "Epoch 1074/2000 train_loss: 0.255943,test_loss: 0.258690\n",
      "Epoch 1075/2000 train_loss: 0.255936,test_loss: 0.258620\n",
      "Epoch 1076/2000 train_loss: 0.255919,test_loss: 0.258639\n",
      "Epoch 1077/2000 train_loss: 0.255915,test_loss: 0.258651\n",
      "Epoch 1078/2000 train_loss: 0.255913,test_loss: 0.258596\n",
      "Epoch 1079/2000 train_loss: 0.255905,test_loss: 0.258666\n",
      "Epoch 1080/2000 train_loss: 0.255898,test_loss: 0.258615\n",
      "Epoch 1081/2000 train_loss: 0.255901,test_loss: 0.258646\n",
      "Epoch 1082/2000 train_loss: 0.255899,test_loss: 0.258599\n",
      "Epoch 1083/2000 train_loss: 0.255887,test_loss: 0.258609\n",
      "Epoch 1084/2000 train_loss: 0.255880,test_loss: 0.258599\n",
      "Epoch 1085/2000 train_loss: 0.255883,test_loss: 0.258578\n",
      "Epoch 1086/2000 train_loss: 0.255876,test_loss: 0.258596\n",
      "Epoch 1087/2000 train_loss: 0.255870,test_loss: 0.258627\n",
      "Epoch 1088/2000 train_loss: 0.255862,test_loss: 0.258561\n",
      "Epoch 1089/2000 train_loss: 0.255847,test_loss: 0.258568\n",
      "Epoch 1090/2000 train_loss: 0.255862,test_loss: 0.258607\n",
      "Epoch 1091/2000 train_loss: 0.255857,test_loss: 0.258624\n",
      "Epoch 1092/2000 train_loss: 0.255849,test_loss: 0.258579\n",
      "Epoch 1093/2000 train_loss: 0.255849,test_loss: 0.258568\n",
      "Epoch 1094/2000 train_loss: 0.255844,test_loss: 0.258537\n",
      "Epoch 1095/2000 train_loss: 0.255832,test_loss: 0.258604\n",
      "Epoch 1096/2000 train_loss: 0.255826,test_loss: 0.258537\n",
      "Epoch 1097/2000 train_loss: 0.255809,test_loss: 0.258537\n",
      "Epoch 1098/2000 train_loss: 0.255809,test_loss: 0.258548\n",
      "Epoch 1099/2000 train_loss: 0.255810,test_loss: 0.258573\n",
      "Epoch 1100/2000 train_loss: 0.255800,test_loss: 0.258490\n",
      "Epoch 1101/2000 train_loss: 0.255796,test_loss: 0.258537\n",
      "Epoch 1102/2000 train_loss: 0.255790,test_loss: 0.258574\n",
      "Epoch 1103/2000 train_loss: 0.255799,test_loss: 0.258476\n",
      "Epoch 1104/2000 train_loss: 0.255783,test_loss: 0.258552\n",
      "Epoch 1105/2000 train_loss: 0.255783,test_loss: 0.258516\n",
      "Epoch 1106/2000 train_loss: 0.255772,test_loss: 0.258551\n",
      "Epoch 1107/2000 train_loss: 0.255776,test_loss: 0.258555\n",
      "Epoch 1108/2000 train_loss: 0.255767,test_loss: 0.258558\n",
      "Epoch 1109/2000 train_loss: 0.255755,test_loss: 0.258511\n",
      "Epoch 1110/2000 train_loss: 0.255756,test_loss: 0.258499\n",
      "Epoch 1111/2000 train_loss: 0.255755,test_loss: 0.258547\n",
      "Epoch 1112/2000 train_loss: 0.255739,test_loss: 0.258570\n",
      "Epoch 1113/2000 train_loss: 0.255748,test_loss: 0.258506\n",
      "Epoch 1114/2000 train_loss: 0.255741,test_loss: 0.258562\n",
      "Epoch 1115/2000 train_loss: 0.255735,test_loss: 0.258507\n",
      "Epoch 1116/2000 train_loss: 0.255721,test_loss: 0.258484\n",
      "Epoch 1117/2000 train_loss: 0.255720,test_loss: 0.258523\n",
      "Epoch 1118/2000 train_loss: 0.255699,test_loss: 0.258576\n",
      "Epoch 1119/2000 train_loss: 0.255712,test_loss: 0.258440\n",
      "Epoch 1120/2000 train_loss: 0.255702,test_loss: 0.258435\n",
      "Epoch 1121/2000 train_loss: 0.255706,test_loss: 0.258503\n",
      "Epoch 1122/2000 train_loss: 0.255697,test_loss: 0.258478\n",
      "Epoch 1123/2000 train_loss: 0.255704,test_loss: 0.258491\n",
      "Epoch 1124/2000 train_loss: 0.255681,test_loss: 0.258438\n",
      "Epoch 1125/2000 train_loss: 0.255682,test_loss: 0.258457\n",
      "Epoch 1126/2000 train_loss: 0.255667,test_loss: 0.258485\n",
      "Epoch 1127/2000 train_loss: 0.255659,test_loss: 0.258414\n",
      "Epoch 1128/2000 train_loss: 0.255670,test_loss: 0.258514\n",
      "Epoch 1129/2000 train_loss: 0.255660,test_loss: 0.258434\n",
      "Epoch 1130/2000 train_loss: 0.255641,test_loss: 0.258491\n",
      "Epoch 1131/2000 train_loss: 0.255659,test_loss: 0.258512\n",
      "Epoch 1132/2000 train_loss: 0.255649,test_loss: 0.258407\n",
      "Epoch 1133/2000 train_loss: 0.255647,test_loss: 0.258452\n",
      "Epoch 1134/2000 train_loss: 0.255632,test_loss: 0.258447\n",
      "Epoch 1135/2000 train_loss: 0.255637,test_loss: 0.258481\n",
      "Epoch 1136/2000 train_loss: 0.255626,test_loss: 0.258436\n",
      "Epoch 1137/2000 train_loss: 0.255627,test_loss: 0.258438\n",
      "Epoch 1138/2000 train_loss: 0.255622,test_loss: 0.258465\n",
      "Epoch 1139/2000 train_loss: 0.255607,test_loss: 0.258428\n",
      "Epoch 1140/2000 train_loss: 0.255619,test_loss: 0.258434\n",
      "Epoch 1141/2000 train_loss: 0.255602,test_loss: 0.258432\n",
      "Epoch 1142/2000 train_loss: 0.255597,test_loss: 0.258437\n",
      "Epoch 1143/2000 train_loss: 0.255599,test_loss: 0.258426\n",
      "Epoch 1144/2000 train_loss: 0.255588,test_loss: 0.258420\n",
      "Epoch 1145/2000 train_loss: 0.255585,test_loss: 0.258390\n",
      "Epoch 1146/2000 train_loss: 0.255588,test_loss: 0.258404\n",
      "Epoch 1147/2000 train_loss: 0.255567,test_loss: 0.258484\n",
      "Epoch 1148/2000 train_loss: 0.255574,test_loss: 0.258404\n",
      "Epoch 1149/2000 train_loss: 0.255571,test_loss: 0.258391\n",
      "Epoch 1150/2000 train_loss: 0.255562,test_loss: 0.258330\n",
      "Epoch 1151/2000 train_loss: 0.255560,test_loss: 0.258402\n",
      "Epoch 1152/2000 train_loss: 0.255550,test_loss: 0.258376\n",
      "Epoch 1153/2000 train_loss: 0.255548,test_loss: 0.258429\n",
      "Epoch 1154/2000 train_loss: 0.255541,test_loss: 0.258406\n",
      "Epoch 1155/2000 train_loss: 0.255538,test_loss: 0.258387\n",
      "Epoch 1156/2000 train_loss: 0.255524,test_loss: 0.258494\n",
      "Epoch 1157/2000 train_loss: 0.255523,test_loss: 0.258356\n",
      "Epoch 1158/2000 train_loss: 0.255528,test_loss: 0.258379\n",
      "Epoch 1159/2000 train_loss: 0.255517,test_loss: 0.258391\n",
      "Epoch 1160/2000 train_loss: 0.255515,test_loss: 0.258331\n",
      "Epoch 1161/2000 train_loss: 0.255510,test_loss: 0.258323\n",
      "Epoch 1162/2000 train_loss: 0.255501,test_loss: 0.258337\n",
      "Epoch 1163/2000 train_loss: 0.255498,test_loss: 0.258351\n",
      "Epoch 1164/2000 train_loss: 0.255501,test_loss: 0.258299\n",
      "Epoch 1165/2000 train_loss: 0.255489,test_loss: 0.258364\n",
      "Epoch 1166/2000 train_loss: 0.255487,test_loss: 0.258370\n",
      "Epoch 1167/2000 train_loss: 0.255487,test_loss: 0.258386\n",
      "Epoch 1168/2000 train_loss: 0.255472,test_loss: 0.258395\n",
      "Epoch 1169/2000 train_loss: 0.255476,test_loss: 0.258364\n",
      "Epoch 1170/2000 train_loss: 0.255465,test_loss: 0.258295\n",
      "Epoch 1171/2000 train_loss: 0.255465,test_loss: 0.258335\n",
      "Epoch 1172/2000 train_loss: 0.255460,test_loss: 0.258337\n",
      "Epoch 1173/2000 train_loss: 0.255424,test_loss: 0.258436\n",
      "Epoch 1174/2000 train_loss: 0.255464,test_loss: 0.258316\n",
      "Epoch 1175/2000 train_loss: 0.255444,test_loss: 0.258332\n",
      "Epoch 1176/2000 train_loss: 0.255446,test_loss: 0.258286\n",
      "Epoch 1177/2000 train_loss: 0.255436,test_loss: 0.258256\n",
      "Epoch 1178/2000 train_loss: 0.255423,test_loss: 0.258306\n",
      "Epoch 1179/2000 train_loss: 0.255431,test_loss: 0.258315\n",
      "Epoch 1180/2000 train_loss: 0.255429,test_loss: 0.258323\n",
      "Epoch 1181/2000 train_loss: 0.255419,test_loss: 0.258350\n",
      "Epoch 1182/2000 train_loss: 0.255412,test_loss: 0.258298\n",
      "Epoch 1183/2000 train_loss: 0.255405,test_loss: 0.258281\n",
      "Epoch 1184/2000 train_loss: 0.255409,test_loss: 0.258358\n",
      "Epoch 1185/2000 train_loss: 0.255401,test_loss: 0.258306\n",
      "Epoch 1186/2000 train_loss: 0.255389,test_loss: 0.258351\n",
      "Epoch 1187/2000 train_loss: 0.255383,test_loss: 0.258321\n",
      "Epoch 1188/2000 train_loss: 0.255384,test_loss: 0.258324\n",
      "Epoch 1189/2000 train_loss: 0.255386,test_loss: 0.258305\n",
      "Epoch 1190/2000 train_loss: 0.255384,test_loss: 0.258274\n",
      "Epoch 1191/2000 train_loss: 0.255377,test_loss: 0.258305\n",
      "Epoch 1192/2000 train_loss: 0.255345,test_loss: 0.258373\n",
      "Epoch 1193/2000 train_loss: 0.255352,test_loss: 0.258311\n",
      "Epoch 1194/2000 train_loss: 0.255363,test_loss: 0.258250\n",
      "Epoch 1195/2000 train_loss: 0.255359,test_loss: 0.258336\n",
      "Epoch 1196/2000 train_loss: 0.255347,test_loss: 0.258243\n",
      "Epoch 1197/2000 train_loss: 0.255343,test_loss: 0.258298\n",
      "Epoch 1198/2000 train_loss: 0.255336,test_loss: 0.258240\n",
      "Epoch 1199/2000 train_loss: 0.255335,test_loss: 0.258203\n",
      "Epoch 1200/2000 train_loss: 0.255332,test_loss: 0.258242\n",
      "Epoch 1201/2000 train_loss: 0.255325,test_loss: 0.258279\n",
      "Epoch 1202/2000 train_loss: 0.255333,test_loss: 0.258239\n",
      "Epoch 1203/2000 train_loss: 0.255320,test_loss: 0.258265\n",
      "Epoch 1204/2000 train_loss: 0.255317,test_loss: 0.258313\n",
      "Epoch 1205/2000 train_loss: 0.255313,test_loss: 0.258263\n",
      "Epoch 1206/2000 train_loss: 0.255311,test_loss: 0.258246\n",
      "Epoch 1207/2000 train_loss: 0.255308,test_loss: 0.258245\n",
      "Epoch 1208/2000 train_loss: 0.255304,test_loss: 0.258259\n",
      "Epoch 1209/2000 train_loss: 0.255295,test_loss: 0.258239\n",
      "Epoch 1210/2000 train_loss: 0.255287,test_loss: 0.258204\n",
      "Epoch 1211/2000 train_loss: 0.255295,test_loss: 0.258198\n",
      "Epoch 1212/2000 train_loss: 0.255275,test_loss: 0.258238\n",
      "Epoch 1213/2000 train_loss: 0.255278,test_loss: 0.258281\n",
      "Epoch 1214/2000 train_loss: 0.255288,test_loss: 0.258235\n",
      "Epoch 1215/2000 train_loss: 0.255265,test_loss: 0.258140\n",
      "Epoch 1216/2000 train_loss: 0.255250,test_loss: 0.258214\n",
      "Epoch 1217/2000 train_loss: 0.255263,test_loss: 0.258337\n",
      "Epoch 1218/2000 train_loss: 0.255267,test_loss: 0.258199\n",
      "Epoch 1219/2000 train_loss: 0.255256,test_loss: 0.258217\n",
      "Epoch 1220/2000 train_loss: 0.255259,test_loss: 0.258211\n",
      "Epoch 1221/2000 train_loss: 0.255243,test_loss: 0.258155\n",
      "Epoch 1222/2000 train_loss: 0.255241,test_loss: 0.258211\n",
      "Epoch 1223/2000 train_loss: 0.255228,test_loss: 0.258144\n",
      "Epoch 1224/2000 train_loss: 0.255232,test_loss: 0.258210\n",
      "Epoch 1225/2000 train_loss: 0.255227,test_loss: 0.258178\n",
      "Epoch 1226/2000 train_loss: 0.255228,test_loss: 0.258156\n",
      "Epoch 1227/2000 train_loss: 0.255223,test_loss: 0.258237\n",
      "Epoch 1228/2000 train_loss: 0.255217,test_loss: 0.258173\n",
      "Epoch 1229/2000 train_loss: 0.255209,test_loss: 0.258163\n",
      "Epoch 1230/2000 train_loss: 0.255203,test_loss: 0.258181\n",
      "Epoch 1231/2000 train_loss: 0.255201,test_loss: 0.258131\n",
      "Epoch 1232/2000 train_loss: 0.255199,test_loss: 0.258166\n",
      "Epoch 1233/2000 train_loss: 0.255182,test_loss: 0.258189\n",
      "Epoch 1234/2000 train_loss: 0.255187,test_loss: 0.258150\n",
      "Epoch 1235/2000 train_loss: 0.255161,test_loss: 0.258248\n",
      "Epoch 1236/2000 train_loss: 0.255184,test_loss: 0.258182\n",
      "Epoch 1237/2000 train_loss: 0.255179,test_loss: 0.258144\n",
      "Epoch 1238/2000 train_loss: 0.255168,test_loss: 0.258165\n",
      "Epoch 1239/2000 train_loss: 0.255170,test_loss: 0.258149\n",
      "Epoch 1240/2000 train_loss: 0.255168,test_loss: 0.258115\n",
      "Epoch 1241/2000 train_loss: 0.255154,test_loss: 0.258146\n",
      "Epoch 1242/2000 train_loss: 0.255144,test_loss: 0.258155\n",
      "Epoch 1243/2000 train_loss: 0.255142,test_loss: 0.258123\n",
      "Epoch 1244/2000 train_loss: 0.255139,test_loss: 0.258184\n",
      "Epoch 1245/2000 train_loss: 0.255136,test_loss: 0.258186\n",
      "Epoch 1246/2000 train_loss: 0.255147,test_loss: 0.258111\n",
      "Epoch 1247/2000 train_loss: 0.255135,test_loss: 0.258150\n",
      "Epoch 1248/2000 train_loss: 0.255133,test_loss: 0.258182\n",
      "Epoch 1249/2000 train_loss: 0.255131,test_loss: 0.258179\n",
      "Epoch 1250/2000 train_loss: 0.255127,test_loss: 0.258145\n",
      "Epoch 1251/2000 train_loss: 0.255119,test_loss: 0.258128\n",
      "Epoch 1252/2000 train_loss: 0.255097,test_loss: 0.258156\n",
      "Epoch 1253/2000 train_loss: 0.255110,test_loss: 0.258102\n",
      "Epoch 1254/2000 train_loss: 0.255108,test_loss: 0.258102\n",
      "Epoch 1255/2000 train_loss: 0.255104,test_loss: 0.258111\n",
      "Epoch 1256/2000 train_loss: 0.255103,test_loss: 0.258120\n",
      "Epoch 1257/2000 train_loss: 0.255100,test_loss: 0.258057\n",
      "Epoch 1258/2000 train_loss: 0.255083,test_loss: 0.258087\n",
      "Epoch 1259/2000 train_loss: 0.255081,test_loss: 0.258135\n",
      "Epoch 1260/2000 train_loss: 0.255073,test_loss: 0.258128\n",
      "Epoch 1261/2000 train_loss: 0.255076,test_loss: 0.258026\n",
      "Epoch 1262/2000 train_loss: 0.255073,test_loss: 0.258120\n",
      "Epoch 1263/2000 train_loss: 0.255066,test_loss: 0.258084\n",
      "Epoch 1264/2000 train_loss: 0.255075,test_loss: 0.258091\n",
      "Epoch 1265/2000 train_loss: 0.255061,test_loss: 0.258087\n",
      "Epoch 1266/2000 train_loss: 0.255054,test_loss: 0.258134\n",
      "Epoch 1267/2000 train_loss: 0.255047,test_loss: 0.258085\n",
      "Epoch 1268/2000 train_loss: 0.255060,test_loss: 0.258063\n",
      "Epoch 1269/2000 train_loss: 0.255048,test_loss: 0.258036\n",
      "Epoch 1270/2000 train_loss: 0.255045,test_loss: 0.258078\n",
      "Epoch 1271/2000 train_loss: 0.255030,test_loss: 0.258082\n",
      "Epoch 1272/2000 train_loss: 0.255038,test_loss: 0.258106\n",
      "Epoch 1273/2000 train_loss: 0.255033,test_loss: 0.258073\n",
      "Epoch 1274/2000 train_loss: 0.255022,test_loss: 0.258003\n",
      "Epoch 1275/2000 train_loss: 0.255014,test_loss: 0.258053\n",
      "Epoch 1276/2000 train_loss: 0.255015,test_loss: 0.258036\n",
      "Epoch 1277/2000 train_loss: 0.255009,test_loss: 0.258087\n",
      "Epoch 1278/2000 train_loss: 0.255013,test_loss: 0.258100\n",
      "Epoch 1279/2000 train_loss: 0.255002,test_loss: 0.257979\n",
      "Epoch 1280/2000 train_loss: 0.255001,test_loss: 0.258055\n",
      "Epoch 1281/2000 train_loss: 0.254997,test_loss: 0.258124\n",
      "Epoch 1282/2000 train_loss: 0.254982,test_loss: 0.258037\n",
      "Epoch 1283/2000 train_loss: 0.254991,test_loss: 0.258037\n",
      "Epoch 1284/2000 train_loss: 0.254985,test_loss: 0.258085\n",
      "Epoch 1285/2000 train_loss: 0.254979,test_loss: 0.258027\n",
      "Epoch 1286/2000 train_loss: 0.254983,test_loss: 0.258027\n",
      "Epoch 1287/2000 train_loss: 0.254974,test_loss: 0.258018\n",
      "Epoch 1288/2000 train_loss: 0.254969,test_loss: 0.258068\n",
      "Epoch 1289/2000 train_loss: 0.254964,test_loss: 0.257978\n",
      "Epoch 1290/2000 train_loss: 0.254961,test_loss: 0.258073\n",
      "Epoch 1291/2000 train_loss: 0.254947,test_loss: 0.258024\n",
      "Epoch 1292/2000 train_loss: 0.254961,test_loss: 0.258020\n",
      "Epoch 1293/2000 train_loss: 0.254952,test_loss: 0.258023\n",
      "Epoch 1294/2000 train_loss: 0.254942,test_loss: 0.258051\n",
      "Epoch 1295/2000 train_loss: 0.254937,test_loss: 0.258006\n",
      "Epoch 1296/2000 train_loss: 0.254938,test_loss: 0.258007\n",
      "Epoch 1297/2000 train_loss: 0.254934,test_loss: 0.257949\n",
      "Epoch 1298/2000 train_loss: 0.254925,test_loss: 0.258056\n",
      "Epoch 1299/2000 train_loss: 0.254930,test_loss: 0.258022\n",
      "Epoch 1300/2000 train_loss: 0.254924,test_loss: 0.258004\n",
      "Epoch 1301/2000 train_loss: 0.254923,test_loss: 0.258015\n",
      "Epoch 1302/2000 train_loss: 0.254916,test_loss: 0.258047\n",
      "Epoch 1303/2000 train_loss: 0.254913,test_loss: 0.258002\n",
      "Epoch 1304/2000 train_loss: 0.254910,test_loss: 0.257984\n",
      "Epoch 1305/2000 train_loss: 0.254904,test_loss: 0.257944\n",
      "Epoch 1306/2000 train_loss: 0.254905,test_loss: 0.257995\n",
      "Epoch 1307/2000 train_loss: 0.254899,test_loss: 0.257983\n",
      "Epoch 1308/2000 train_loss: 0.254882,test_loss: 0.257977\n",
      "Epoch 1309/2000 train_loss: 0.254892,test_loss: 0.258020\n",
      "Epoch 1310/2000 train_loss: 0.254885,test_loss: 0.257979\n",
      "Epoch 1311/2000 train_loss: 0.254883,test_loss: 0.258018\n",
      "Epoch 1312/2000 train_loss: 0.254877,test_loss: 0.257945\n",
      "Epoch 1313/2000 train_loss: 0.254870,test_loss: 0.258014\n",
      "Epoch 1314/2000 train_loss: 0.254856,test_loss: 0.258010\n",
      "Epoch 1315/2000 train_loss: 0.254868,test_loss: 0.257941\n",
      "Epoch 1316/2000 train_loss: 0.254863,test_loss: 0.258039\n",
      "Epoch 1317/2000 train_loss: 0.254856,test_loss: 0.257951\n",
      "Epoch 1318/2000 train_loss: 0.254860,test_loss: 0.257927\n",
      "Epoch 1319/2000 train_loss: 0.254855,test_loss: 0.258029\n",
      "Epoch 1320/2000 train_loss: 0.254857,test_loss: 0.257980\n",
      "Epoch 1321/2000 train_loss: 0.254844,test_loss: 0.257993\n",
      "Epoch 1322/2000 train_loss: 0.254840,test_loss: 0.258017\n",
      "Epoch 1323/2000 train_loss: 0.254838,test_loss: 0.257959\n",
      "Epoch 1324/2000 train_loss: 0.254835,test_loss: 0.258029\n",
      "Epoch 1325/2000 train_loss: 0.254831,test_loss: 0.257876\n",
      "Epoch 1326/2000 train_loss: 0.254826,test_loss: 0.257903\n",
      "Epoch 1327/2000 train_loss: 0.254809,test_loss: 0.257916\n",
      "Epoch 1328/2000 train_loss: 0.254816,test_loss: 0.257932\n",
      "Epoch 1329/2000 train_loss: 0.254814,test_loss: 0.257901\n",
      "Epoch 1330/2000 train_loss: 0.254815,test_loss: 0.257920\n",
      "Epoch 1331/2000 train_loss: 0.254811,test_loss: 0.257923\n",
      "Epoch 1332/2000 train_loss: 0.254807,test_loss: 0.257913\n",
      "Epoch 1333/2000 train_loss: 0.254800,test_loss: 0.257950\n",
      "Epoch 1334/2000 train_loss: 0.254792,test_loss: 0.257845\n",
      "Epoch 1335/2000 train_loss: 0.254795,test_loss: 0.257898\n",
      "Epoch 1336/2000 train_loss: 0.254783,test_loss: 0.257919\n",
      "Epoch 1337/2000 train_loss: 0.254790,test_loss: 0.257907\n",
      "Epoch 1338/2000 train_loss: 0.254779,test_loss: 0.257901\n",
      "Epoch 1339/2000 train_loss: 0.254770,test_loss: 0.257943\n",
      "Epoch 1340/2000 train_loss: 0.254773,test_loss: 0.257978\n",
      "Epoch 1341/2000 train_loss: 0.254768,test_loss: 0.257874\n",
      "Epoch 1342/2000 train_loss: 0.254770,test_loss: 0.257944\n",
      "Epoch 1343/2000 train_loss: 0.254770,test_loss: 0.257870\n",
      "Epoch 1344/2000 train_loss: 0.254759,test_loss: 0.257983\n",
      "Epoch 1345/2000 train_loss: 0.254765,test_loss: 0.257944\n",
      "Epoch 1346/2000 train_loss: 0.254748,test_loss: 0.257874\n",
      "Epoch 1347/2000 train_loss: 0.254740,test_loss: 0.257922\n",
      "Epoch 1348/2000 train_loss: 0.254741,test_loss: 0.257852\n",
      "Epoch 1349/2000 train_loss: 0.254735,test_loss: 0.257906\n",
      "Epoch 1350/2000 train_loss: 0.254737,test_loss: 0.257840\n",
      "Epoch 1351/2000 train_loss: 0.254741,test_loss: 0.257919\n",
      "Epoch 1352/2000 train_loss: 0.254737,test_loss: 0.257883\n",
      "Epoch 1353/2000 train_loss: 0.254725,test_loss: 0.257914\n",
      "Epoch 1354/2000 train_loss: 0.254719,test_loss: 0.257864\n",
      "Epoch 1355/2000 train_loss: 0.254722,test_loss: 0.257835\n",
      "Epoch 1356/2000 train_loss: 0.254705,test_loss: 0.257878\n",
      "Epoch 1357/2000 train_loss: 0.254713,test_loss: 0.257856\n",
      "Epoch 1358/2000 train_loss: 0.254707,test_loss: 0.257924\n",
      "Epoch 1359/2000 train_loss: 0.254707,test_loss: 0.257908\n",
      "Epoch 1360/2000 train_loss: 0.254703,test_loss: 0.257900\n",
      "Epoch 1361/2000 train_loss: 0.254701,test_loss: 0.257840\n",
      "Epoch 1362/2000 train_loss: 0.254696,test_loss: 0.257870\n",
      "Epoch 1363/2000 train_loss: 0.254670,test_loss: 0.257898\n",
      "Epoch 1364/2000 train_loss: 0.254691,test_loss: 0.257930\n",
      "Epoch 1365/2000 train_loss: 0.254694,test_loss: 0.257821\n",
      "Epoch 1366/2000 train_loss: 0.254667,test_loss: 0.257897\n",
      "Epoch 1367/2000 train_loss: 0.254683,test_loss: 0.257816\n",
      "Epoch 1368/2000 train_loss: 0.254660,test_loss: 0.257845\n",
      "Epoch 1369/2000 train_loss: 0.254677,test_loss: 0.257862\n",
      "Epoch 1370/2000 train_loss: 0.254671,test_loss: 0.257885\n",
      "Epoch 1371/2000 train_loss: 0.254661,test_loss: 0.257889\n",
      "Epoch 1372/2000 train_loss: 0.254659,test_loss: 0.257819\n",
      "Epoch 1373/2000 train_loss: 0.254658,test_loss: 0.257833\n",
      "Epoch 1374/2000 train_loss: 0.254646,test_loss: 0.257893\n",
      "Epoch 1375/2000 train_loss: 0.254645,test_loss: 0.257839\n",
      "Epoch 1376/2000 train_loss: 0.254650,test_loss: 0.257843\n",
      "Epoch 1377/2000 train_loss: 0.254639,test_loss: 0.257868\n",
      "Epoch 1378/2000 train_loss: 0.254634,test_loss: 0.257822\n",
      "Epoch 1379/2000 train_loss: 0.254632,test_loss: 0.257853\n",
      "Epoch 1380/2000 train_loss: 0.254619,test_loss: 0.257813\n",
      "Epoch 1381/2000 train_loss: 0.254639,test_loss: 0.257833\n",
      "Epoch 1382/2000 train_loss: 0.254624,test_loss: 0.257749\n",
      "Epoch 1383/2000 train_loss: 0.254619,test_loss: 0.257813\n",
      "Epoch 1384/2000 train_loss: 0.254613,test_loss: 0.257823\n",
      "Epoch 1385/2000 train_loss: 0.254619,test_loss: 0.257766\n",
      "Epoch 1386/2000 train_loss: 0.254615,test_loss: 0.257794\n",
      "Epoch 1387/2000 train_loss: 0.254602,test_loss: 0.257759\n",
      "Epoch 1388/2000 train_loss: 0.254610,test_loss: 0.257817\n",
      "Epoch 1389/2000 train_loss: 0.254601,test_loss: 0.257788\n",
      "Epoch 1390/2000 train_loss: 0.254599,test_loss: 0.257803\n",
      "Epoch 1391/2000 train_loss: 0.254597,test_loss: 0.257836\n",
      "Epoch 1392/2000 train_loss: 0.254589,test_loss: 0.257850\n",
      "Epoch 1393/2000 train_loss: 0.254583,test_loss: 0.257787\n",
      "Epoch 1394/2000 train_loss: 0.254582,test_loss: 0.257819\n",
      "Epoch 1395/2000 train_loss: 0.254564,test_loss: 0.257822\n",
      "Epoch 1396/2000 train_loss: 0.254576,test_loss: 0.257789\n",
      "Epoch 1397/2000 train_loss: 0.254571,test_loss: 0.257788\n",
      "Epoch 1398/2000 train_loss: 0.254566,test_loss: 0.257815\n",
      "Epoch 1399/2000 train_loss: 0.254559,test_loss: 0.257800\n",
      "Epoch 1400/2000 train_loss: 0.254564,test_loss: 0.257829\n",
      "Epoch 1401/2000 train_loss: 0.254558,test_loss: 0.257760\n",
      "Epoch 1402/2000 train_loss: 0.254548,test_loss: 0.257784\n",
      "Epoch 1403/2000 train_loss: 0.254546,test_loss: 0.257785\n",
      "Epoch 1404/2000 train_loss: 0.254555,test_loss: 0.257759\n",
      "Epoch 1405/2000 train_loss: 0.254545,test_loss: 0.257831\n",
      "Epoch 1406/2000 train_loss: 0.254541,test_loss: 0.257752\n",
      "Epoch 1407/2000 train_loss: 0.254542,test_loss: 0.257822\n",
      "Epoch 1408/2000 train_loss: 0.254532,test_loss: 0.257800\n",
      "Epoch 1409/2000 train_loss: 0.254530,test_loss: 0.257753\n",
      "Epoch 1410/2000 train_loss: 0.254527,test_loss: 0.257762\n",
      "Epoch 1411/2000 train_loss: 0.254522,test_loss: 0.257769\n",
      "Epoch 1412/2000 train_loss: 0.254511,test_loss: 0.257680\n",
      "Epoch 1413/2000 train_loss: 0.254514,test_loss: 0.257749\n",
      "Epoch 1414/2000 train_loss: 0.254512,test_loss: 0.257717\n",
      "Epoch 1415/2000 train_loss: 0.254510,test_loss: 0.257778\n",
      "Epoch 1416/2000 train_loss: 0.254507,test_loss: 0.257779\n",
      "Epoch 1417/2000 train_loss: 0.254500,test_loss: 0.257743\n",
      "Epoch 1418/2000 train_loss: 0.254504,test_loss: 0.257749\n",
      "Epoch 1419/2000 train_loss: 0.254503,test_loss: 0.257752\n",
      "Epoch 1420/2000 train_loss: 0.254490,test_loss: 0.257753\n",
      "Epoch 1421/2000 train_loss: 0.254500,test_loss: 0.257717\n",
      "Epoch 1422/2000 train_loss: 0.254482,test_loss: 0.257757\n",
      "Epoch 1423/2000 train_loss: 0.254481,test_loss: 0.257724\n",
      "Epoch 1424/2000 train_loss: 0.254472,test_loss: 0.257784\n",
      "Epoch 1425/2000 train_loss: 0.254471,test_loss: 0.257732\n",
      "Epoch 1426/2000 train_loss: 0.254460,test_loss: 0.257723\n",
      "Epoch 1427/2000 train_loss: 0.254471,test_loss: 0.257655\n",
      "Epoch 1428/2000 train_loss: 0.254464,test_loss: 0.257783\n",
      "Epoch 1429/2000 train_loss: 0.254459,test_loss: 0.257711\n",
      "Epoch 1430/2000 train_loss: 0.254457,test_loss: 0.257701\n",
      "Epoch 1431/2000 train_loss: 0.254458,test_loss: 0.257717\n",
      "Epoch 1432/2000 train_loss: 0.254453,test_loss: 0.257725\n",
      "Epoch 1433/2000 train_loss: 0.254440,test_loss: 0.257737\n",
      "Epoch 1434/2000 train_loss: 0.254450,test_loss: 0.257724\n",
      "Epoch 1435/2000 train_loss: 0.254446,test_loss: 0.257711\n",
      "Epoch 1436/2000 train_loss: 0.254439,test_loss: 0.257736\n",
      "Epoch 1437/2000 train_loss: 0.254420,test_loss: 0.257628\n",
      "Epoch 1438/2000 train_loss: 0.254441,test_loss: 0.257755\n",
      "Epoch 1439/2000 train_loss: 0.254429,test_loss: 0.257720\n",
      "Epoch 1440/2000 train_loss: 0.254421,test_loss: 0.257694\n",
      "Epoch 1441/2000 train_loss: 0.254428,test_loss: 0.257684\n",
      "Epoch 1442/2000 train_loss: 0.254422,test_loss: 0.257669\n",
      "Epoch 1443/2000 train_loss: 0.254415,test_loss: 0.257739\n",
      "Epoch 1444/2000 train_loss: 0.254398,test_loss: 0.257711\n",
      "Epoch 1445/2000 train_loss: 0.254409,test_loss: 0.257704\n",
      "Epoch 1446/2000 train_loss: 0.254414,test_loss: 0.257678\n",
      "Epoch 1447/2000 train_loss: 0.254402,test_loss: 0.257668\n",
      "Epoch 1448/2000 train_loss: 0.254394,test_loss: 0.257733\n",
      "Epoch 1449/2000 train_loss: 0.254391,test_loss: 0.257684\n",
      "Epoch 1450/2000 train_loss: 0.254394,test_loss: 0.257657\n",
      "Epoch 1451/2000 train_loss: 0.254388,test_loss: 0.257689\n",
      "Epoch 1452/2000 train_loss: 0.254386,test_loss: 0.257658\n",
      "Epoch 1453/2000 train_loss: 0.254378,test_loss: 0.257662\n",
      "Epoch 1454/2000 train_loss: 0.254380,test_loss: 0.257664\n",
      "Epoch 1455/2000 train_loss: 0.254372,test_loss: 0.257629\n",
      "Epoch 1456/2000 train_loss: 0.254380,test_loss: 0.257677\n",
      "Epoch 1457/2000 train_loss: 0.254375,test_loss: 0.257603\n",
      "Epoch 1458/2000 train_loss: 0.254351,test_loss: 0.257581\n",
      "Epoch 1459/2000 train_loss: 0.254353,test_loss: 0.257778\n",
      "Epoch 1460/2000 train_loss: 0.254369,test_loss: 0.257567\n",
      "Epoch 1461/2000 train_loss: 0.254361,test_loss: 0.257713\n",
      "Epoch 1462/2000 train_loss: 0.254348,test_loss: 0.257716\n",
      "Epoch 1463/2000 train_loss: 0.254354,test_loss: 0.257690\n",
      "Epoch 1464/2000 train_loss: 0.254357,test_loss: 0.257684\n",
      "Epoch 1465/2000 train_loss: 0.254341,test_loss: 0.257650\n",
      "Epoch 1466/2000 train_loss: 0.254339,test_loss: 0.257681\n",
      "Epoch 1467/2000 train_loss: 0.254338,test_loss: 0.257628\n",
      "Epoch 1468/2000 train_loss: 0.254332,test_loss: 0.257572\n",
      "Epoch 1469/2000 train_loss: 0.254330,test_loss: 0.257654\n",
      "Epoch 1470/2000 train_loss: 0.254333,test_loss: 0.257611\n",
      "Epoch 1471/2000 train_loss: 0.254327,test_loss: 0.257634\n",
      "Epoch 1472/2000 train_loss: 0.254309,test_loss: 0.257680\n",
      "Epoch 1473/2000 train_loss: 0.254320,test_loss: 0.257620\n",
      "Epoch 1474/2000 train_loss: 0.254313,test_loss: 0.257614\n",
      "Epoch 1475/2000 train_loss: 0.254321,test_loss: 0.257678\n",
      "Epoch 1476/2000 train_loss: 0.254316,test_loss: 0.257582\n",
      "Epoch 1477/2000 train_loss: 0.254307,test_loss: 0.257574\n",
      "Epoch 1478/2000 train_loss: 0.254298,test_loss: 0.257589\n",
      "Epoch 1479/2000 train_loss: 0.254302,test_loss: 0.257596\n",
      "Epoch 1480/2000 train_loss: 0.254295,test_loss: 0.257635\n",
      "Epoch 1481/2000 train_loss: 0.254291,test_loss: 0.257637\n",
      "Epoch 1482/2000 train_loss: 0.254297,test_loss: 0.257648\n",
      "Epoch 1483/2000 train_loss: 0.254285,test_loss: 0.257636\n",
      "Epoch 1484/2000 train_loss: 0.254277,test_loss: 0.257700\n",
      "Epoch 1485/2000 train_loss: 0.254285,test_loss: 0.257589\n",
      "Epoch 1486/2000 train_loss: 0.254278,test_loss: 0.257583\n",
      "Epoch 1487/2000 train_loss: 0.254287,test_loss: 0.257646\n",
      "Epoch 1488/2000 train_loss: 0.254264,test_loss: 0.257611\n",
      "Epoch 1489/2000 train_loss: 0.254274,test_loss: 0.257615\n",
      "Epoch 1490/2000 train_loss: 0.254265,test_loss: 0.257566\n",
      "Epoch 1491/2000 train_loss: 0.254269,test_loss: 0.257619\n",
      "Epoch 1492/2000 train_loss: 0.254255,test_loss: 0.257667\n",
      "Epoch 1493/2000 train_loss: 0.254258,test_loss: 0.257595\n",
      "Epoch 1494/2000 train_loss: 0.254262,test_loss: 0.257581\n",
      "Epoch 1495/2000 train_loss: 0.254255,test_loss: 0.257534\n",
      "Epoch 1496/2000 train_loss: 0.254249,test_loss: 0.257619\n",
      "Epoch 1497/2000 train_loss: 0.254237,test_loss: 0.257696\n",
      "Epoch 1498/2000 train_loss: 0.254246,test_loss: 0.257654\n",
      "Epoch 1499/2000 train_loss: 0.254240,test_loss: 0.257594\n",
      "Epoch 1500/2000 train_loss: 0.254228,test_loss: 0.257608\n",
      "Epoch 1501/2000 train_loss: 0.254232,test_loss: 0.257620\n",
      "Epoch 1502/2000 train_loss: 0.254238,test_loss: 0.257555\n",
      "Epoch 1503/2000 train_loss: 0.254226,test_loss: 0.257625\n",
      "Epoch 1504/2000 train_loss: 0.254219,test_loss: 0.257527\n",
      "Epoch 1505/2000 train_loss: 0.254225,test_loss: 0.257577\n",
      "Epoch 1506/2000 train_loss: 0.254207,test_loss: 0.257576\n",
      "Epoch 1507/2000 train_loss: 0.254217,test_loss: 0.257616\n",
      "Epoch 1508/2000 train_loss: 0.254213,test_loss: 0.257565\n",
      "Epoch 1509/2000 train_loss: 0.254207,test_loss: 0.257596\n",
      "Epoch 1510/2000 train_loss: 0.254205,test_loss: 0.257580\n",
      "Epoch 1511/2000 train_loss: 0.254201,test_loss: 0.257582\n",
      "Epoch 1512/2000 train_loss: 0.254189,test_loss: 0.257527\n",
      "Epoch 1513/2000 train_loss: 0.254201,test_loss: 0.257547\n",
      "Epoch 1514/2000 train_loss: 0.254193,test_loss: 0.257552\n",
      "Epoch 1515/2000 train_loss: 0.254187,test_loss: 0.257578\n",
      "Epoch 1516/2000 train_loss: 0.254184,test_loss: 0.257559\n",
      "Epoch 1517/2000 train_loss: 0.254181,test_loss: 0.257587\n",
      "Epoch 1518/2000 train_loss: 0.254183,test_loss: 0.257566\n",
      "Epoch 1519/2000 train_loss: 0.254156,test_loss: 0.257507\n",
      "Epoch 1520/2000 train_loss: 0.254183,test_loss: 0.257600\n",
      "Epoch 1521/2000 train_loss: 0.254170,test_loss: 0.257634\n",
      "Epoch 1522/2000 train_loss: 0.254169,test_loss: 0.257556\n",
      "Epoch 1523/2000 train_loss: 0.254168,test_loss: 0.257525\n",
      "Epoch 1524/2000 train_loss: 0.254165,test_loss: 0.257554\n",
      "Epoch 1525/2000 train_loss: 0.254158,test_loss: 0.257536\n",
      "Epoch 1526/2000 train_loss: 0.254159,test_loss: 0.257527\n",
      "Epoch 1527/2000 train_loss: 0.254157,test_loss: 0.257540\n",
      "Epoch 1528/2000 train_loss: 0.254156,test_loss: 0.257527\n",
      "Epoch 1529/2000 train_loss: 0.254158,test_loss: 0.257543\n",
      "Epoch 1530/2000 train_loss: 0.254146,test_loss: 0.257523\n",
      "Epoch 1531/2000 train_loss: 0.254140,test_loss: 0.257519\n",
      "Epoch 1532/2000 train_loss: 0.254136,test_loss: 0.257471\n",
      "Epoch 1533/2000 train_loss: 0.254143,test_loss: 0.257508\n",
      "Epoch 1534/2000 train_loss: 0.254122,test_loss: 0.257495\n",
      "Epoch 1535/2000 train_loss: 0.254134,test_loss: 0.257532\n",
      "Epoch 1536/2000 train_loss: 0.254130,test_loss: 0.257515\n",
      "Epoch 1537/2000 train_loss: 0.254122,test_loss: 0.257514\n",
      "Epoch 1538/2000 train_loss: 0.254131,test_loss: 0.257511\n",
      "Epoch 1539/2000 train_loss: 0.254119,test_loss: 0.257548\n",
      "Epoch 1540/2000 train_loss: 0.254112,test_loss: 0.257551\n",
      "Epoch 1541/2000 train_loss: 0.254116,test_loss: 0.257564\n",
      "Epoch 1542/2000 train_loss: 0.254113,test_loss: 0.257414\n",
      "Epoch 1543/2000 train_loss: 0.254099,test_loss: 0.257436\n",
      "Epoch 1544/2000 train_loss: 0.254109,test_loss: 0.257586\n",
      "Epoch 1545/2000 train_loss: 0.254096,test_loss: 0.257525\n",
      "Epoch 1546/2000 train_loss: 0.254104,test_loss: 0.257486\n",
      "Epoch 1547/2000 train_loss: 0.254090,test_loss: 0.257502\n",
      "Epoch 1548/2000 train_loss: 0.254089,test_loss: 0.257545\n",
      "Epoch 1549/2000 train_loss: 0.254089,test_loss: 0.257467\n",
      "Epoch 1550/2000 train_loss: 0.254080,test_loss: 0.257507\n",
      "Epoch 1551/2000 train_loss: 0.254085,test_loss: 0.257454\n",
      "Epoch 1552/2000 train_loss: 0.254079,test_loss: 0.257490\n",
      "Epoch 1553/2000 train_loss: 0.254072,test_loss: 0.257499\n",
      "Epoch 1554/2000 train_loss: 0.254072,test_loss: 0.257514\n",
      "Epoch 1555/2000 train_loss: 0.254061,test_loss: 0.257440\n",
      "Epoch 1556/2000 train_loss: 0.254063,test_loss: 0.257507\n",
      "Epoch 1557/2000 train_loss: 0.254068,test_loss: 0.257498\n",
      "Epoch 1558/2000 train_loss: 0.254063,test_loss: 0.257431\n",
      "Epoch 1559/2000 train_loss: 0.254059,test_loss: 0.257432\n",
      "Epoch 1560/2000 train_loss: 0.254056,test_loss: 0.257468\n",
      "Epoch 1561/2000 train_loss: 0.254034,test_loss: 0.257448\n",
      "Epoch 1562/2000 train_loss: 0.254059,test_loss: 0.257509\n",
      "Epoch 1563/2000 train_loss: 0.254046,test_loss: 0.257485\n",
      "Epoch 1564/2000 train_loss: 0.254039,test_loss: 0.257485\n",
      "Epoch 1565/2000 train_loss: 0.254040,test_loss: 0.257492\n",
      "Epoch 1566/2000 train_loss: 0.254036,test_loss: 0.257500\n",
      "Epoch 1567/2000 train_loss: 0.254024,test_loss: 0.257453\n",
      "Epoch 1568/2000 train_loss: 0.254041,test_loss: 0.257473\n",
      "Epoch 1569/2000 train_loss: 0.254034,test_loss: 0.257457\n",
      "Epoch 1570/2000 train_loss: 0.254024,test_loss: 0.257450\n",
      "Epoch 1571/2000 train_loss: 0.254036,test_loss: 0.257505\n",
      "Epoch 1572/2000 train_loss: 0.253992,test_loss: 0.257511\n",
      "Epoch 1573/2000 train_loss: 0.254012,test_loss: 0.257453\n",
      "Epoch 1574/2000 train_loss: 0.254008,test_loss: 0.257445\n",
      "Epoch 1575/2000 train_loss: 0.254018,test_loss: 0.257470\n",
      "Epoch 1576/2000 train_loss: 0.254017,test_loss: 0.257515\n",
      "Epoch 1577/2000 train_loss: 0.254016,test_loss: 0.257463\n",
      "Epoch 1578/2000 train_loss: 0.254006,test_loss: 0.257457\n",
      "Epoch 1579/2000 train_loss: 0.254010,test_loss: 0.257439\n",
      "Epoch 1580/2000 train_loss: 0.253997,test_loss: 0.257528\n",
      "Epoch 1581/2000 train_loss: 0.254004,test_loss: 0.257415\n",
      "Epoch 1582/2000 train_loss: 0.253993,test_loss: 0.257474\n",
      "Epoch 1583/2000 train_loss: 0.253989,test_loss: 0.257450\n",
      "Epoch 1584/2000 train_loss: 0.253988,test_loss: 0.257447\n",
      "Epoch 1585/2000 train_loss: 0.253989,test_loss: 0.257382\n",
      "Epoch 1586/2000 train_loss: 0.253982,test_loss: 0.257357\n",
      "Epoch 1587/2000 train_loss: 0.253979,test_loss: 0.257438\n",
      "Epoch 1588/2000 train_loss: 0.253971,test_loss: 0.257490\n",
      "Epoch 1589/2000 train_loss: 0.253976,test_loss: 0.257449\n",
      "Epoch 1590/2000 train_loss: 0.253969,test_loss: 0.257429\n",
      "Epoch 1591/2000 train_loss: 0.253965,test_loss: 0.257443\n",
      "Epoch 1592/2000 train_loss: 0.253964,test_loss: 0.257395\n",
      "Epoch 1593/2000 train_loss: 0.253964,test_loss: 0.257459\n",
      "Epoch 1594/2000 train_loss: 0.253955,test_loss: 0.257423\n",
      "Epoch 1595/2000 train_loss: 0.253954,test_loss: 0.257416\n",
      "Epoch 1596/2000 train_loss: 0.253961,test_loss: 0.257386\n",
      "Epoch 1597/2000 train_loss: 0.253950,test_loss: 0.257414\n",
      "Epoch 1598/2000 train_loss: 0.253949,test_loss: 0.257445\n",
      "Epoch 1599/2000 train_loss: 0.253934,test_loss: 0.257351\n",
      "Epoch 1600/2000 train_loss: 0.253941,test_loss: 0.257429\n",
      "Epoch 1601/2000 train_loss: 0.253945,test_loss: 0.257467\n",
      "Epoch 1602/2000 train_loss: 0.253941,test_loss: 0.257448\n",
      "Epoch 1603/2000 train_loss: 0.253932,test_loss: 0.257364\n",
      "Epoch 1604/2000 train_loss: 0.253931,test_loss: 0.257428\n",
      "Epoch 1605/2000 train_loss: 0.253930,test_loss: 0.257382\n",
      "Epoch 1606/2000 train_loss: 0.253927,test_loss: 0.257375\n",
      "Epoch 1607/2000 train_loss: 0.253916,test_loss: 0.257422\n",
      "Epoch 1608/2000 train_loss: 0.253921,test_loss: 0.257455\n",
      "Epoch 1609/2000 train_loss: 0.253917,test_loss: 0.257362\n",
      "Epoch 1610/2000 train_loss: 0.253920,test_loss: 0.257373\n",
      "Epoch 1611/2000 train_loss: 0.253911,test_loss: 0.257362\n",
      "Epoch 1612/2000 train_loss: 0.253898,test_loss: 0.257458\n",
      "Epoch 1613/2000 train_loss: 0.253898,test_loss: 0.257425\n",
      "Epoch 1614/2000 train_loss: 0.253908,test_loss: 0.257390\n",
      "Epoch 1615/2000 train_loss: 0.253908,test_loss: 0.257390\n",
      "Epoch 1616/2000 train_loss: 0.253890,test_loss: 0.257361\n",
      "Epoch 1617/2000 train_loss: 0.253900,test_loss: 0.257337\n",
      "Epoch 1618/2000 train_loss: 0.253895,test_loss: 0.257359\n",
      "Epoch 1619/2000 train_loss: 0.253884,test_loss: 0.257402\n",
      "Epoch 1620/2000 train_loss: 0.253889,test_loss: 0.257337\n",
      "Epoch 1621/2000 train_loss: 0.253885,test_loss: 0.257348\n",
      "Epoch 1622/2000 train_loss: 0.253877,test_loss: 0.257416\n",
      "Epoch 1623/2000 train_loss: 0.253887,test_loss: 0.257364\n",
      "Epoch 1624/2000 train_loss: 0.253864,test_loss: 0.257402\n",
      "Epoch 1625/2000 train_loss: 0.253877,test_loss: 0.257387\n",
      "Epoch 1626/2000 train_loss: 0.253875,test_loss: 0.257440\n",
      "Epoch 1627/2000 train_loss: 0.253870,test_loss: 0.257384\n",
      "Epoch 1628/2000 train_loss: 0.253868,test_loss: 0.257441\n",
      "Epoch 1629/2000 train_loss: 0.253846,test_loss: 0.257414\n",
      "Epoch 1630/2000 train_loss: 0.253866,test_loss: 0.257381\n",
      "Epoch 1631/2000 train_loss: 0.253860,test_loss: 0.257322\n",
      "Epoch 1632/2000 train_loss: 0.253854,test_loss: 0.257362\n",
      "Epoch 1633/2000 train_loss: 0.253857,test_loss: 0.257392\n",
      "Epoch 1634/2000 train_loss: 0.253850,test_loss: 0.257376\n",
      "Epoch 1635/2000 train_loss: 0.253825,test_loss: 0.257454\n",
      "Epoch 1636/2000 train_loss: 0.253854,test_loss: 0.257376\n",
      "Epoch 1637/2000 train_loss: 0.253846,test_loss: 0.257387\n",
      "Epoch 1638/2000 train_loss: 0.253843,test_loss: 0.257322\n",
      "Epoch 1639/2000 train_loss: 0.253838,test_loss: 0.257369\n",
      "Epoch 1640/2000 train_loss: 0.253824,test_loss: 0.257311\n",
      "Epoch 1641/2000 train_loss: 0.253838,test_loss: 0.257439\n",
      "Epoch 1642/2000 train_loss: 0.253832,test_loss: 0.257363\n",
      "Epoch 1643/2000 train_loss: 0.253827,test_loss: 0.257381\n",
      "Epoch 1644/2000 train_loss: 0.253820,test_loss: 0.257275\n",
      "Epoch 1645/2000 train_loss: 0.253819,test_loss: 0.257391\n",
      "Epoch 1646/2000 train_loss: 0.253820,test_loss: 0.257314\n",
      "Epoch 1647/2000 train_loss: 0.253812,test_loss: 0.257345\n",
      "Epoch 1648/2000 train_loss: 0.253820,test_loss: 0.257268\n",
      "Epoch 1649/2000 train_loss: 0.253808,test_loss: 0.257350\n",
      "Epoch 1650/2000 train_loss: 0.253813,test_loss: 0.257352\n",
      "Epoch 1651/2000 train_loss: 0.253810,test_loss: 0.257349\n",
      "Epoch 1652/2000 train_loss: 0.253797,test_loss: 0.257426\n",
      "Epoch 1653/2000 train_loss: 0.253804,test_loss: 0.257344\n",
      "Epoch 1654/2000 train_loss: 0.253803,test_loss: 0.257319\n",
      "Epoch 1655/2000 train_loss: 0.253792,test_loss: 0.257318\n",
      "Epoch 1656/2000 train_loss: 0.253788,test_loss: 0.257338\n",
      "Epoch 1657/2000 train_loss: 0.253786,test_loss: 0.257331\n",
      "Epoch 1658/2000 train_loss: 0.253781,test_loss: 0.257300\n",
      "Epoch 1659/2000 train_loss: 0.253790,test_loss: 0.257350\n",
      "Epoch 1660/2000 train_loss: 0.253778,test_loss: 0.257335\n",
      "Epoch 1661/2000 train_loss: 0.253769,test_loss: 0.257285\n",
      "Epoch 1662/2000 train_loss: 0.253768,test_loss: 0.257402\n",
      "Epoch 1663/2000 train_loss: 0.253784,test_loss: 0.257345\n",
      "Epoch 1664/2000 train_loss: 0.253768,test_loss: 0.257268\n",
      "Epoch 1665/2000 train_loss: 0.253769,test_loss: 0.257272\n",
      "Epoch 1666/2000 train_loss: 0.253754,test_loss: 0.257311\n",
      "Epoch 1667/2000 train_loss: 0.253765,test_loss: 0.257305\n",
      "Epoch 1668/2000 train_loss: 0.253759,test_loss: 0.257255\n",
      "Epoch 1669/2000 train_loss: 0.253745,test_loss: 0.257303\n",
      "Epoch 1670/2000 train_loss: 0.253762,test_loss: 0.257310\n",
      "Epoch 1671/2000 train_loss: 0.253752,test_loss: 0.257355\n",
      "Epoch 1672/2000 train_loss: 0.253741,test_loss: 0.257257\n",
      "Epoch 1673/2000 train_loss: 0.253752,test_loss: 0.257273\n",
      "Epoch 1674/2000 train_loss: 0.253748,test_loss: 0.257314\n",
      "Epoch 1675/2000 train_loss: 0.253747,test_loss: 0.257319\n",
      "Epoch 1676/2000 train_loss: 0.253744,test_loss: 0.257277\n",
      "Epoch 1677/2000 train_loss: 0.253733,test_loss: 0.257258\n",
      "Epoch 1678/2000 train_loss: 0.253736,test_loss: 0.257280\n",
      "Epoch 1679/2000 train_loss: 0.253733,test_loss: 0.257295\n",
      "Epoch 1680/2000 train_loss: 0.253733,test_loss: 0.257246\n",
      "Epoch 1681/2000 train_loss: 0.253727,test_loss: 0.257302\n",
      "Epoch 1682/2000 train_loss: 0.253732,test_loss: 0.257281\n",
      "Epoch 1683/2000 train_loss: 0.253725,test_loss: 0.257222\n",
      "Epoch 1684/2000 train_loss: 0.253717,test_loss: 0.257371\n",
      "Epoch 1685/2000 train_loss: 0.253725,test_loss: 0.257343\n",
      "Epoch 1686/2000 train_loss: 0.253720,test_loss: 0.257284\n",
      "Epoch 1687/2000 train_loss: 0.253707,test_loss: 0.257291\n",
      "Epoch 1688/2000 train_loss: 0.253702,test_loss: 0.257281\n",
      "Epoch 1689/2000 train_loss: 0.253681,test_loss: 0.257278\n",
      "Epoch 1690/2000 train_loss: 0.253700,test_loss: 0.257339\n",
      "Epoch 1691/2000 train_loss: 0.253708,test_loss: 0.257244\n",
      "Epoch 1692/2000 train_loss: 0.253703,test_loss: 0.257295\n",
      "Epoch 1693/2000 train_loss: 0.253686,test_loss: 0.257293\n",
      "Epoch 1694/2000 train_loss: 0.253698,test_loss: 0.257255\n",
      "Epoch 1695/2000 train_loss: 0.253696,test_loss: 0.257264\n",
      "Epoch 1696/2000 train_loss: 0.253679,test_loss: 0.257241\n",
      "Epoch 1697/2000 train_loss: 0.253681,test_loss: 0.257282\n",
      "Epoch 1698/2000 train_loss: 0.253681,test_loss: 0.257259\n",
      "Epoch 1699/2000 train_loss: 0.253686,test_loss: 0.257290\n",
      "Epoch 1700/2000 train_loss: 0.253670,test_loss: 0.257270\n",
      "Epoch 1701/2000 train_loss: 0.253677,test_loss: 0.257313\n",
      "Epoch 1702/2000 train_loss: 0.253668,test_loss: 0.257184\n",
      "Epoch 1703/2000 train_loss: 0.253659,test_loss: 0.257223\n",
      "Epoch 1704/2000 train_loss: 0.253672,test_loss: 0.257274\n",
      "Epoch 1705/2000 train_loss: 0.253665,test_loss: 0.257318\n",
      "Epoch 1706/2000 train_loss: 0.253661,test_loss: 0.257255\n",
      "Epoch 1707/2000 train_loss: 0.253665,test_loss: 0.257339\n",
      "Epoch 1708/2000 train_loss: 0.253655,test_loss: 0.257221\n",
      "Epoch 1709/2000 train_loss: 0.253661,test_loss: 0.257236\n",
      "Epoch 1710/2000 train_loss: 0.253647,test_loss: 0.257311\n",
      "Epoch 1711/2000 train_loss: 0.253645,test_loss: 0.257246\n",
      "Epoch 1712/2000 train_loss: 0.253653,test_loss: 0.257262\n",
      "Epoch 1713/2000 train_loss: 0.253651,test_loss: 0.257240\n",
      "Epoch 1714/2000 train_loss: 0.253645,test_loss: 0.257237\n",
      "Epoch 1715/2000 train_loss: 0.253643,test_loss: 0.257230\n",
      "Epoch 1716/2000 train_loss: 0.253642,test_loss: 0.257252\n",
      "Epoch 1717/2000 train_loss: 0.253640,test_loss: 0.257246\n",
      "Epoch 1718/2000 train_loss: 0.253632,test_loss: 0.257330\n",
      "Epoch 1719/2000 train_loss: 0.253631,test_loss: 0.257254\n",
      "Epoch 1720/2000 train_loss: 0.253622,test_loss: 0.257261\n",
      "Epoch 1721/2000 train_loss: 0.253627,test_loss: 0.257175\n",
      "Epoch 1722/2000 train_loss: 0.253632,test_loss: 0.257277\n",
      "Epoch 1723/2000 train_loss: 0.253621,test_loss: 0.257239\n",
      "Epoch 1724/2000 train_loss: 0.253623,test_loss: 0.257221\n",
      "Epoch 1725/2000 train_loss: 0.253618,test_loss: 0.257271\n",
      "Epoch 1726/2000 train_loss: 0.253615,test_loss: 0.257229\n",
      "Epoch 1727/2000 train_loss: 0.253611,test_loss: 0.257217\n",
      "Epoch 1728/2000 train_loss: 0.253610,test_loss: 0.257169\n",
      "Epoch 1729/2000 train_loss: 0.253603,test_loss: 0.257200\n",
      "Epoch 1730/2000 train_loss: 0.253607,test_loss: 0.257223\n",
      "Epoch 1731/2000 train_loss: 0.253600,test_loss: 0.257278\n",
      "Epoch 1732/2000 train_loss: 0.253601,test_loss: 0.257237\n",
      "Epoch 1733/2000 train_loss: 0.253596,test_loss: 0.257201\n",
      "Epoch 1734/2000 train_loss: 0.253598,test_loss: 0.257200\n",
      "Epoch 1735/2000 train_loss: 0.253593,test_loss: 0.257241\n",
      "Epoch 1736/2000 train_loss: 0.253592,test_loss: 0.257241\n",
      "Epoch 1737/2000 train_loss: 0.253584,test_loss: 0.257192\n",
      "Epoch 1738/2000 train_loss: 0.253577,test_loss: 0.257266\n",
      "Epoch 1739/2000 train_loss: 0.253581,test_loss: 0.257209\n",
      "Epoch 1740/2000 train_loss: 0.253580,test_loss: 0.257146\n",
      "Epoch 1741/2000 train_loss: 0.253570,test_loss: 0.257164\n",
      "Epoch 1742/2000 train_loss: 0.253582,test_loss: 0.257257\n",
      "Epoch 1743/2000 train_loss: 0.253583,test_loss: 0.257187\n",
      "Epoch 1744/2000 train_loss: 0.253576,test_loss: 0.257210\n",
      "Epoch 1745/2000 train_loss: 0.253577,test_loss: 0.257227\n",
      "Epoch 1746/2000 train_loss: 0.253560,test_loss: 0.257223\n",
      "Epoch 1747/2000 train_loss: 0.253559,test_loss: 0.257199\n",
      "Epoch 1748/2000 train_loss: 0.253555,test_loss: 0.257269\n",
      "Epoch 1749/2000 train_loss: 0.253562,test_loss: 0.257179\n",
      "Epoch 1750/2000 train_loss: 0.253557,test_loss: 0.257149\n",
      "Epoch 1751/2000 train_loss: 0.253556,test_loss: 0.257273\n",
      "Epoch 1752/2000 train_loss: 0.253550,test_loss: 0.257189\n",
      "Epoch 1753/2000 train_loss: 0.253553,test_loss: 0.257191\n",
      "Epoch 1754/2000 train_loss: 0.253546,test_loss: 0.257304\n",
      "Epoch 1755/2000 train_loss: 0.253539,test_loss: 0.257168\n",
      "Epoch 1756/2000 train_loss: 0.253542,test_loss: 0.257206\n",
      "Epoch 1757/2000 train_loss: 0.253543,test_loss: 0.257180\n",
      "Epoch 1758/2000 train_loss: 0.253532,test_loss: 0.257211\n",
      "Epoch 1759/2000 train_loss: 0.253536,test_loss: 0.257186\n",
      "Epoch 1760/2000 train_loss: 0.253535,test_loss: 0.257172\n",
      "Epoch 1761/2000 train_loss: 0.253531,test_loss: 0.257206\n",
      "Epoch 1762/2000 train_loss: 0.253532,test_loss: 0.257179\n",
      "Epoch 1763/2000 train_loss: 0.253521,test_loss: 0.257179\n",
      "Epoch 1764/2000 train_loss: 0.253526,test_loss: 0.257167\n",
      "Epoch 1765/2000 train_loss: 0.253523,test_loss: 0.257155\n",
      "Epoch 1766/2000 train_loss: 0.253524,test_loss: 0.257149\n",
      "Epoch 1767/2000 train_loss: 0.253517,test_loss: 0.257158\n",
      "Epoch 1768/2000 train_loss: 0.253512,test_loss: 0.257167\n",
      "Epoch 1769/2000 train_loss: 0.253490,test_loss: 0.257238\n",
      "Epoch 1770/2000 train_loss: 0.253508,test_loss: 0.257116\n",
      "Epoch 1771/2000 train_loss: 0.253510,test_loss: 0.257205\n",
      "Epoch 1772/2000 train_loss: 0.253506,test_loss: 0.257146\n",
      "Epoch 1773/2000 train_loss: 0.253493,test_loss: 0.257172\n",
      "Epoch 1774/2000 train_loss: 0.253500,test_loss: 0.257212\n",
      "Epoch 1775/2000 train_loss: 0.253493,test_loss: 0.257204\n",
      "Epoch 1776/2000 train_loss: 0.253495,test_loss: 0.257058\n",
      "Epoch 1777/2000 train_loss: 0.253492,test_loss: 0.257173\n",
      "Epoch 1778/2000 train_loss: 0.253493,test_loss: 0.257155\n",
      "Epoch 1779/2000 train_loss: 0.253490,test_loss: 0.257153\n",
      "Epoch 1780/2000 train_loss: 0.253493,test_loss: 0.257115\n",
      "Epoch 1781/2000 train_loss: 0.253486,test_loss: 0.257130\n",
      "Epoch 1782/2000 train_loss: 0.253469,test_loss: 0.257171\n",
      "Epoch 1783/2000 train_loss: 0.253474,test_loss: 0.257115\n",
      "Epoch 1784/2000 train_loss: 0.253483,test_loss: 0.257119\n",
      "Epoch 1785/2000 train_loss: 0.253466,test_loss: 0.257241\n",
      "Epoch 1786/2000 train_loss: 0.253473,test_loss: 0.257185\n",
      "Epoch 1787/2000 train_loss: 0.253475,test_loss: 0.257132\n",
      "Epoch 1788/2000 train_loss: 0.253465,test_loss: 0.257108\n",
      "Epoch 1789/2000 train_loss: 0.253452,test_loss: 0.257236\n",
      "Epoch 1790/2000 train_loss: 0.253468,test_loss: 0.257213\n",
      "Epoch 1791/2000 train_loss: 0.253467,test_loss: 0.257133\n",
      "Epoch 1792/2000 train_loss: 0.253459,test_loss: 0.257141\n",
      "Epoch 1793/2000 train_loss: 0.253457,test_loss: 0.257127\n",
      "Epoch 1794/2000 train_loss: 0.253452,test_loss: 0.257202\n",
      "Epoch 1795/2000 train_loss: 0.253455,test_loss: 0.257153\n",
      "Epoch 1796/2000 train_loss: 0.253448,test_loss: 0.257132\n",
      "Epoch 1797/2000 train_loss: 0.253442,test_loss: 0.257115\n",
      "Epoch 1798/2000 train_loss: 0.253437,test_loss: 0.257108\n",
      "Epoch 1799/2000 train_loss: 0.253440,test_loss: 0.257109\n",
      "Epoch 1800/2000 train_loss: 0.253440,test_loss: 0.257130\n",
      "Epoch 1801/2000 train_loss: 0.253445,test_loss: 0.257105\n",
      "Epoch 1802/2000 train_loss: 0.253440,test_loss: 0.257173\n",
      "Epoch 1803/2000 train_loss: 0.253435,test_loss: 0.257115\n",
      "Epoch 1804/2000 train_loss: 0.253431,test_loss: 0.257134\n",
      "Epoch 1805/2000 train_loss: 0.253429,test_loss: 0.257145\n",
      "Epoch 1806/2000 train_loss: 0.253425,test_loss: 0.257088\n",
      "Epoch 1807/2000 train_loss: 0.253425,test_loss: 0.257128\n",
      "Epoch 1808/2000 train_loss: 0.253421,test_loss: 0.257232\n",
      "Epoch 1809/2000 train_loss: 0.253426,test_loss: 0.257093\n",
      "Epoch 1810/2000 train_loss: 0.253411,test_loss: 0.257189\n",
      "Epoch 1811/2000 train_loss: 0.253417,test_loss: 0.257148\n",
      "Epoch 1812/2000 train_loss: 0.253418,test_loss: 0.257098\n",
      "Epoch 1813/2000 train_loss: 0.253409,test_loss: 0.257076\n",
      "Epoch 1814/2000 train_loss: 0.253405,test_loss: 0.257095\n",
      "Epoch 1815/2000 train_loss: 0.253397,test_loss: 0.257165\n",
      "Epoch 1816/2000 train_loss: 0.253398,test_loss: 0.257087\n",
      "Epoch 1817/2000 train_loss: 0.253397,test_loss: 0.257126\n",
      "Epoch 1818/2000 train_loss: 0.253401,test_loss: 0.257102\n",
      "Epoch 1819/2000 train_loss: 0.253395,test_loss: 0.257107\n",
      "Epoch 1820/2000 train_loss: 0.253392,test_loss: 0.257097\n",
      "Epoch 1821/2000 train_loss: 0.253384,test_loss: 0.257079\n",
      "Epoch 1822/2000 train_loss: 0.253378,test_loss: 0.257157\n",
      "Epoch 1823/2000 train_loss: 0.253379,test_loss: 0.257130\n",
      "Epoch 1824/2000 train_loss: 0.253389,test_loss: 0.257093\n",
      "Epoch 1825/2000 train_loss: 0.253382,test_loss: 0.257118\n",
      "Epoch 1826/2000 train_loss: 0.253376,test_loss: 0.257046\n",
      "Epoch 1827/2000 train_loss: 0.253372,test_loss: 0.257107\n",
      "Epoch 1828/2000 train_loss: 0.253385,test_loss: 0.257063\n",
      "Epoch 1829/2000 train_loss: 0.253376,test_loss: 0.257068\n",
      "Epoch 1830/2000 train_loss: 0.253374,test_loss: 0.257076\n",
      "Epoch 1831/2000 train_loss: 0.253366,test_loss: 0.257090\n",
      "Epoch 1832/2000 train_loss: 0.253356,test_loss: 0.257026\n",
      "Epoch 1833/2000 train_loss: 0.253364,test_loss: 0.257090\n",
      "Epoch 1834/2000 train_loss: 0.253369,test_loss: 0.257098\n",
      "Epoch 1835/2000 train_loss: 0.253356,test_loss: 0.257124\n",
      "Epoch 1836/2000 train_loss: 0.253362,test_loss: 0.257172\n",
      "Epoch 1837/2000 train_loss: 0.253359,test_loss: 0.257052\n",
      "Epoch 1838/2000 train_loss: 0.253363,test_loss: 0.257006\n",
      "Epoch 1839/2000 train_loss: 0.253346,test_loss: 0.257152\n",
      "Epoch 1840/2000 train_loss: 0.253351,test_loss: 0.257143\n",
      "Epoch 1841/2000 train_loss: 0.253348,test_loss: 0.257084\n",
      "Epoch 1842/2000 train_loss: 0.253335,test_loss: 0.257149\n",
      "Epoch 1843/2000 train_loss: 0.253344,test_loss: 0.257070\n",
      "Epoch 1844/2000 train_loss: 0.253333,test_loss: 0.257067\n",
      "Epoch 1845/2000 train_loss: 0.253341,test_loss: 0.257029\n",
      "Epoch 1846/2000 train_loss: 0.253340,test_loss: 0.257092\n",
      "Epoch 1847/2000 train_loss: 0.253337,test_loss: 0.257066\n",
      "Epoch 1848/2000 train_loss: 0.253339,test_loss: 0.257035\n",
      "Epoch 1849/2000 train_loss: 0.253329,test_loss: 0.257029\n",
      "Epoch 1850/2000 train_loss: 0.253332,test_loss: 0.257017\n",
      "Epoch 1851/2000 train_loss: 0.253322,test_loss: 0.257079\n",
      "Epoch 1852/2000 train_loss: 0.253321,test_loss: 0.257079\n",
      "Epoch 1853/2000 train_loss: 0.253319,test_loss: 0.257037\n",
      "Epoch 1854/2000 train_loss: 0.253319,test_loss: 0.257071\n",
      "Epoch 1855/2000 train_loss: 0.253315,test_loss: 0.257064\n",
      "Epoch 1856/2000 train_loss: 0.253314,test_loss: 0.257108\n",
      "Epoch 1857/2000 train_loss: 0.253292,test_loss: 0.257100\n",
      "Epoch 1858/2000 train_loss: 0.253318,test_loss: 0.257097\n",
      "Epoch 1859/2000 train_loss: 0.253306,test_loss: 0.257081\n",
      "Epoch 1860/2000 train_loss: 0.253302,test_loss: 0.257089\n",
      "Epoch 1861/2000 train_loss: 0.253300,test_loss: 0.257006\n",
      "Epoch 1862/2000 train_loss: 0.253298,test_loss: 0.256980\n",
      "Epoch 1863/2000 train_loss: 0.253310,test_loss: 0.257048\n",
      "Epoch 1864/2000 train_loss: 0.253307,test_loss: 0.257043\n",
      "Epoch 1865/2000 train_loss: 0.253260,test_loss: 0.257095\n",
      "Epoch 1866/2000 train_loss: 0.253282,test_loss: 0.257025\n",
      "Epoch 1867/2000 train_loss: 0.253291,test_loss: 0.257043\n",
      "Epoch 1868/2000 train_loss: 0.253289,test_loss: 0.257044\n",
      "Epoch 1869/2000 train_loss: 0.253274,test_loss: 0.257036\n",
      "Epoch 1870/2000 train_loss: 0.253287,test_loss: 0.257005\n",
      "Epoch 1871/2000 train_loss: 0.253268,test_loss: 0.257130\n",
      "Epoch 1872/2000 train_loss: 0.253284,test_loss: 0.257082\n",
      "Epoch 1873/2000 train_loss: 0.253284,test_loss: 0.257018\n",
      "Epoch 1874/2000 train_loss: 0.253275,test_loss: 0.257020\n",
      "Epoch 1875/2000 train_loss: 0.253273,test_loss: 0.257041\n",
      "Epoch 1876/2000 train_loss: 0.253275,test_loss: 0.257034\n",
      "Epoch 1877/2000 train_loss: 0.253252,test_loss: 0.257053\n",
      "Epoch 1878/2000 train_loss: 0.253265,test_loss: 0.257019\n",
      "Epoch 1879/2000 train_loss: 0.253264,test_loss: 0.257063\n",
      "Epoch 1880/2000 train_loss: 0.253256,test_loss: 0.257039\n",
      "Epoch 1881/2000 train_loss: 0.253260,test_loss: 0.257032\n",
      "Epoch 1882/2000 train_loss: 0.253258,test_loss: 0.256979\n",
      "Epoch 1883/2000 train_loss: 0.253254,test_loss: 0.257002\n",
      "Epoch 1884/2000 train_loss: 0.253252,test_loss: 0.257057\n",
      "Epoch 1885/2000 train_loss: 0.253241,test_loss: 0.256962\n",
      "Epoch 1886/2000 train_loss: 0.253245,test_loss: 0.257082\n",
      "Epoch 1887/2000 train_loss: 0.253232,test_loss: 0.257133\n",
      "Epoch 1888/2000 train_loss: 0.253247,test_loss: 0.257067\n",
      "Epoch 1889/2000 train_loss: 0.253242,test_loss: 0.257014\n",
      "Epoch 1890/2000 train_loss: 0.253235,test_loss: 0.257011\n",
      "Epoch 1891/2000 train_loss: 0.253237,test_loss: 0.257029\n",
      "Epoch 1892/2000 train_loss: 0.253239,test_loss: 0.256970\n",
      "Epoch 1893/2000 train_loss: 0.253233,test_loss: 0.257000\n",
      "Epoch 1894/2000 train_loss: 0.253240,test_loss: 0.256984\n",
      "Epoch 1895/2000 train_loss: 0.253240,test_loss: 0.257011\n",
      "Epoch 1896/2000 train_loss: 0.253224,test_loss: 0.257039\n",
      "Epoch 1897/2000 train_loss: 0.253228,test_loss: 0.257029\n",
      "Epoch 1898/2000 train_loss: 0.253224,test_loss: 0.256971\n",
      "Epoch 1899/2000 train_loss: 0.253226,test_loss: 0.257005\n",
      "Epoch 1900/2000 train_loss: 0.253214,test_loss: 0.256991\n",
      "Epoch 1901/2000 train_loss: 0.253221,test_loss: 0.256946\n",
      "Epoch 1902/2000 train_loss: 0.253213,test_loss: 0.257000\n",
      "Epoch 1903/2000 train_loss: 0.253217,test_loss: 0.257074\n",
      "Epoch 1904/2000 train_loss: 0.253211,test_loss: 0.257024\n",
      "Epoch 1905/2000 train_loss: 0.253203,test_loss: 0.257026\n",
      "Epoch 1906/2000 train_loss: 0.253203,test_loss: 0.256967\n",
      "Epoch 1907/2000 train_loss: 0.253206,test_loss: 0.256936\n",
      "Epoch 1908/2000 train_loss: 0.253206,test_loss: 0.257000\n",
      "Epoch 1909/2000 train_loss: 0.253195,test_loss: 0.257040\n",
      "Epoch 1910/2000 train_loss: 0.253198,test_loss: 0.257044\n",
      "Epoch 1911/2000 train_loss: 0.253196,test_loss: 0.257015\n",
      "Epoch 1912/2000 train_loss: 0.253195,test_loss: 0.257008\n",
      "Epoch 1913/2000 train_loss: 0.253193,test_loss: 0.256978\n",
      "Epoch 1914/2000 train_loss: 0.253188,test_loss: 0.256965\n",
      "Epoch 1915/2000 train_loss: 0.253191,test_loss: 0.257009\n",
      "Epoch 1916/2000 train_loss: 0.253188,test_loss: 0.256995\n",
      "Epoch 1917/2000 train_loss: 0.253184,test_loss: 0.257066\n",
      "Epoch 1918/2000 train_loss: 0.253185,test_loss: 0.256964\n",
      "Epoch 1919/2000 train_loss: 0.253185,test_loss: 0.256997\n",
      "Epoch 1920/2000 train_loss: 0.253167,test_loss: 0.256972\n",
      "Epoch 1921/2000 train_loss: 0.253173,test_loss: 0.256995\n",
      "Epoch 1922/2000 train_loss: 0.253172,test_loss: 0.257012\n",
      "Epoch 1923/2000 train_loss: 0.253170,test_loss: 0.256996\n",
      "Epoch 1924/2000 train_loss: 0.253167,test_loss: 0.256971\n",
      "Epoch 1925/2000 train_loss: 0.253172,test_loss: 0.256956\n",
      "Epoch 1926/2000 train_loss: 0.253166,test_loss: 0.256960\n",
      "Epoch 1927/2000 train_loss: 0.253164,test_loss: 0.256975\n",
      "Epoch 1928/2000 train_loss: 0.253158,test_loss: 0.257004\n",
      "Epoch 1929/2000 train_loss: 0.253158,test_loss: 0.257008\n",
      "Epoch 1930/2000 train_loss: 0.253163,test_loss: 0.256993\n",
      "Epoch 1931/2000 train_loss: 0.253163,test_loss: 0.256990\n",
      "Epoch 1932/2000 train_loss: 0.253151,test_loss: 0.256961\n",
      "Epoch 1933/2000 train_loss: 0.253149,test_loss: 0.256921\n",
      "Epoch 1934/2000 train_loss: 0.253147,test_loss: 0.257081\n",
      "Epoch 1935/2000 train_loss: 0.253154,test_loss: 0.256988\n",
      "Epoch 1936/2000 train_loss: 0.253146,test_loss: 0.256946\n",
      "Epoch 1937/2000 train_loss: 0.253141,test_loss: 0.257032\n",
      "Epoch 1938/2000 train_loss: 0.253143,test_loss: 0.256927\n",
      "Epoch 1939/2000 train_loss: 0.253136,test_loss: 0.256940\n",
      "Epoch 1940/2000 train_loss: 0.253144,test_loss: 0.256952\n",
      "Epoch 1941/2000 train_loss: 0.253130,test_loss: 0.256980\n",
      "Epoch 1942/2000 train_loss: 0.253131,test_loss: 0.256972\n",
      "Epoch 1943/2000 train_loss: 0.253121,test_loss: 0.256948\n",
      "Epoch 1944/2000 train_loss: 0.253140,test_loss: 0.256940\n",
      "Epoch 1945/2000 train_loss: 0.253134,test_loss: 0.256975\n",
      "Epoch 1946/2000 train_loss: 0.253119,test_loss: 0.256975\n",
      "Epoch 1947/2000 train_loss: 0.253131,test_loss: 0.256972\n",
      "Epoch 1948/2000 train_loss: 0.253123,test_loss: 0.256995\n",
      "Epoch 1949/2000 train_loss: 0.253116,test_loss: 0.256939\n",
      "Epoch 1950/2000 train_loss: 0.253118,test_loss: 0.256941\n",
      "Epoch 1951/2000 train_loss: 0.253115,test_loss: 0.256916\n",
      "Epoch 1952/2000 train_loss: 0.253104,test_loss: 0.256921\n",
      "Epoch 1953/2000 train_loss: 0.253110,test_loss: 0.256971\n",
      "Epoch 1954/2000 train_loss: 0.253108,test_loss: 0.256933\n",
      "Epoch 1955/2000 train_loss: 0.253103,test_loss: 0.256874\n",
      "Epoch 1956/2000 train_loss: 0.253108,test_loss: 0.256928\n",
      "Epoch 1957/2000 train_loss: 0.253091,test_loss: 0.256999\n",
      "Epoch 1958/2000 train_loss: 0.253101,test_loss: 0.256877\n",
      "Epoch 1959/2000 train_loss: 0.253100,test_loss: 0.256941\n",
      "Epoch 1960/2000 train_loss: 0.253097,test_loss: 0.256947\n",
      "Epoch 1961/2000 train_loss: 0.253088,test_loss: 0.256949\n",
      "Epoch 1962/2000 train_loss: 0.253097,test_loss: 0.256934\n",
      "Epoch 1963/2000 train_loss: 0.253098,test_loss: 0.256982\n",
      "Epoch 1964/2000 train_loss: 0.253088,test_loss: 0.257016\n",
      "Epoch 1965/2000 train_loss: 0.253090,test_loss: 0.256891\n",
      "Epoch 1966/2000 train_loss: 0.253091,test_loss: 0.256921\n",
      "Epoch 1967/2000 train_loss: 0.253071,test_loss: 0.256934\n",
      "Epoch 1968/2000 train_loss: 0.253067,test_loss: 0.256923\n",
      "Epoch 1969/2000 train_loss: 0.253060,test_loss: 0.256906\n",
      "Epoch 1970/2000 train_loss: 0.253091,test_loss: 0.256924\n",
      "Epoch 1971/2000 train_loss: 0.253084,test_loss: 0.256984\n",
      "Epoch 1972/2000 train_loss: 0.253072,test_loss: 0.256947\n",
      "Epoch 1973/2000 train_loss: 0.253059,test_loss: 0.257001\n",
      "Epoch 1974/2000 train_loss: 0.253074,test_loss: 0.256961\n",
      "Epoch 1975/2000 train_loss: 0.253069,test_loss: 0.256926\n",
      "Epoch 1976/2000 train_loss: 0.253065,test_loss: 0.256991\n",
      "Epoch 1977/2000 train_loss: 0.253063,test_loss: 0.256935\n",
      "Epoch 1978/2000 train_loss: 0.253059,test_loss: 0.256901\n",
      "Epoch 1979/2000 train_loss: 0.253061,test_loss: 0.256972\n",
      "Epoch 1980/2000 train_loss: 0.253060,test_loss: 0.256990\n",
      "Epoch 1981/2000 train_loss: 0.253056,test_loss: 0.256937\n",
      "Epoch 1982/2000 train_loss: 0.253048,test_loss: 0.256885\n",
      "Epoch 1983/2000 train_loss: 0.253053,test_loss: 0.256971\n",
      "Epoch 1984/2000 train_loss: 0.253049,test_loss: 0.256935\n",
      "Epoch 1985/2000 train_loss: 0.253040,test_loss: 0.256955\n",
      "Epoch 1986/2000 train_loss: 0.253053,test_loss: 0.256884\n",
      "Epoch 1987/2000 train_loss: 0.253053,test_loss: 0.256915\n",
      "Epoch 1988/2000 train_loss: 0.253029,test_loss: 0.256976\n",
      "Epoch 1989/2000 train_loss: 0.253042,test_loss: 0.256990\n",
      "Epoch 1990/2000 train_loss: 0.253039,test_loss: 0.256947\n",
      "Epoch 1991/2000 train_loss: 0.253042,test_loss: 0.256926\n",
      "Epoch 1992/2000 train_loss: 0.253039,test_loss: 0.256900\n",
      "Epoch 1993/2000 train_loss: 0.253031,test_loss: 0.256939\n",
      "Epoch 1994/2000 train_loss: 0.253032,test_loss: 0.256936\n",
      "Epoch 1995/2000 train_loss: 0.253024,test_loss: 0.256992\n",
      "Epoch 1996/2000 train_loss: 0.253027,test_loss: 0.256794\n",
      "Epoch 1997/2000 train_loss: 0.253026,test_loss: 0.256877\n",
      "Epoch 1998/2000 train_loss: 0.253024,test_loss: 0.256887\n",
      "Epoch 1999/2000 train_loss: 0.253027,test_loss: 0.256980\n",
      "Epoch 2000/2000 train_loss: 0.253021,test_loss: 0.256904\n"
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    loss_sum = 0.0\n",
    "    test_sum = 0.0\n",
    "\n",
    "    for (train_features,train_labels),(test_features,test_labels) in zip(train_dataloader,test_dataloader) :\n",
    "\n",
    "        epoch_y = net(train_features)\n",
    "        loss = loss_func(epoch_y.to(torch.float32),train_labels.to(torch.float32))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        for m in net.modules():\n",
    "            if isinstance(m,nn.Linear):\n",
    "                writer.add_histogram('weight',m.weights.data,epoch + 1)\n",
    "\n",
    "        loss_sum += loss\n",
    "        \n",
    "        epoch_test_y = net(test_features)\n",
    "\n",
    "        loss_test = loss_func_test(epoch_test_y.to(torch.float32),test_labels.to(torch.float32))\n",
    "\n",
    "        test_sum += loss_test\n",
    "\n",
    "    loss = loss_sum / len(train_dataloader)\n",
    "    test = test_sum / len(test_dataloader)\n",
    "\n",
    "    print('Epoch {:4d}/{} train_loss: {:.6f},test_loss: {:.6f}'.format(\n",
    "        epoch+1, epochs, loss, test\n",
    "    ))\n",
    "\n",
    "    if loss < 0.1 :\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "tensor([0.1234], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "in_x, y = data_train[804]\n",
    "\n",
    "print(y)\n",
    "\n",
    "print(net(in_x.squeeze().reshape(-1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
