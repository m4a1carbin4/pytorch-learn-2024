{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torcheval.metrics import BinaryAccuracy\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "data_train = datasets.MNIST(root=\"./dataset\",train=True,download=True,transform=transforms.ToTensor())\n",
    "data_test = datasets.MNIST(root=\"./dataset\",train=False,download=True,transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "image, label = data_train[0]\n",
    "\n",
    "plt.imshow(image.squeeze().numpy(),cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "class UnkownNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UnkownNet,self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784,1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,input):\n",
    "\n",
    "        f2 = self.fc1(input)\n",
    "\n",
    "        output = self.activation(f2)\n",
    "\n",
    "        return output\n",
    "    \n",
    "net = UnkownNet()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(),lr=0.01)\n",
    "loss_func = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = []\n",
    "\n",
    "for x , y in data_train:\n",
    "\n",
    "    if y % 2 != 0:\n",
    "        y = np.array([1])\n",
    "    else :\n",
    "        y = np.array([0])\n",
    "\n",
    "    x = x.squeeze().reshape(-1).numpy()\n",
    "\n",
    "    train_dataset.append([x,y])\n",
    "\n",
    "test_dataset = []\n",
    "\n",
    "for x , y in data_test:\n",
    "\n",
    "    if y % 2 != 0:\n",
    "        y = np.array([1])\n",
    "    else :\n",
    "        y = np.array([0])\n",
    "\n",
    "    x = x.squeeze().reshape(-1).numpy()\n",
    "\n",
    "    test_dataset.append([x,y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import dataset, DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=600,shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=100,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataloader))\n",
    "print(len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_tensor(tensor):\n",
    "    \n",
    "    v_min, v_max = tensor.min(), tensor.max()\n",
    "    \n",
    "    new_min,new_max = 0,255\n",
    "    \n",
    "    v_p = (tensor - v_min)/(v_max - v_min)*(new_max - new_min) + new_min\n",
    "    \n",
    "    return v_p\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/2000 train_loss: 0.573621 acc: 0.765300\n",
      "Epoch    2/2000 train_loss: 0.466061 acc: 0.819800\n",
      "Epoch    3/2000 train_loss: 0.422429 acc: 0.830000\n",
      "Epoch    4/2000 train_loss: 0.398189 acc: 0.836800\n",
      "Epoch    5/2000 train_loss: 0.382448 acc: 0.841300\n",
      "Epoch    6/2000 train_loss: 0.371237 acc: 0.845700\n",
      "Epoch    7/2000 train_loss: 0.362762 acc: 0.847400\n",
      "Epoch    8/2000 train_loss: 0.356076 acc: 0.849500\n",
      "Epoch    9/2000 train_loss: 0.350634 acc: 0.852700\n",
      "Epoch   10/2000 train_loss: 0.346069 acc: 0.855500\n",
      "Epoch   11/2000 train_loss: 0.342185 acc: 0.857500\n",
      "Epoch   12/2000 train_loss: 0.338793 acc: 0.859100\n",
      "Epoch   13/2000 train_loss: 0.335831 acc: 0.860400\n",
      "Epoch   14/2000 train_loss: 0.333191 acc: 0.861600\n",
      "Epoch   15/2000 train_loss: 0.330814 acc: 0.862400\n",
      "Epoch   16/2000 train_loss: 0.328649 acc: 0.863500\n",
      "Epoch   17/2000 train_loss: 0.326698 acc: 0.864000\n",
      "Epoch   18/2000 train_loss: 0.324892 acc: 0.866000\n",
      "Epoch   19/2000 train_loss: 0.323199 acc: 0.866800\n",
      "Epoch   20/2000 train_loss: 0.321694 acc: 0.868400\n",
      "Epoch   21/2000 train_loss: 0.320233 acc: 0.868500\n",
      "Epoch   22/2000 train_loss: 0.318881 acc: 0.869500\n",
      "Epoch   23/2000 train_loss: 0.317611 acc: 0.869700\n",
      "Epoch   24/2000 train_loss: 0.316427 acc: 0.871100\n",
      "Epoch   25/2000 train_loss: 0.315278 acc: 0.872300\n",
      "Epoch   26/2000 train_loss: 0.314225 acc: 0.872500\n",
      "Epoch   27/2000 train_loss: 0.313202 acc: 0.872900\n",
      "Epoch   28/2000 train_loss: 0.312222 acc: 0.873300\n",
      "Epoch   29/2000 train_loss: 0.311308 acc: 0.873600\n",
      "Epoch   30/2000 train_loss: 0.310410 acc: 0.874300\n",
      "Epoch   31/2000 train_loss: 0.309573 acc: 0.874500\n",
      "Epoch   32/2000 train_loss: 0.308772 acc: 0.875000\n",
      "Epoch   33/2000 train_loss: 0.307981 acc: 0.875500\n",
      "Epoch   34/2000 train_loss: 0.307223 acc: 0.876000\n",
      "Epoch   35/2000 train_loss: 0.306518 acc: 0.876200\n",
      "Epoch   36/2000 train_loss: 0.305796 acc: 0.876000\n",
      "Epoch   37/2000 train_loss: 0.305135 acc: 0.876900\n",
      "Epoch   38/2000 train_loss: 0.304492 acc: 0.877500\n",
      "Epoch   39/2000 train_loss: 0.303856 acc: 0.876900\n",
      "Epoch   40/2000 train_loss: 0.303259 acc: 0.877900\n",
      "Epoch   41/2000 train_loss: 0.302660 acc: 0.877900\n",
      "Epoch   42/2000 train_loss: 0.302077 acc: 0.878000\n",
      "Epoch   43/2000 train_loss: 0.301512 acc: 0.878200\n",
      "Epoch   44/2000 train_loss: 0.300985 acc: 0.878900\n",
      "Epoch   45/2000 train_loss: 0.300455 acc: 0.878400\n",
      "Epoch   46/2000 train_loss: 0.299942 acc: 0.878300\n",
      "Epoch   47/2000 train_loss: 0.299450 acc: 0.879200\n",
      "Epoch   48/2000 train_loss: 0.298952 acc: 0.880200\n",
      "Epoch   49/2000 train_loss: 0.298478 acc: 0.880000\n",
      "Epoch   50/2000 train_loss: 0.298031 acc: 0.879700\n",
      "Epoch   51/2000 train_loss: 0.297567 acc: 0.880300\n",
      "Epoch   52/2000 train_loss: 0.297119 acc: 0.881400\n",
      "Epoch   53/2000 train_loss: 0.296709 acc: 0.880500\n",
      "Epoch   54/2000 train_loss: 0.296266 acc: 0.881500\n",
      "Epoch   55/2000 train_loss: 0.295865 acc: 0.881300\n",
      "Epoch   56/2000 train_loss: 0.295461 acc: 0.882000\n",
      "Epoch   57/2000 train_loss: 0.295054 acc: 0.882400\n",
      "Epoch   58/2000 train_loss: 0.294681 acc: 0.883200\n",
      "Epoch   59/2000 train_loss: 0.294277 acc: 0.883000\n",
      "Epoch   60/2000 train_loss: 0.293943 acc: 0.883800\n",
      "Epoch   61/2000 train_loss: 0.293566 acc: 0.884100\n",
      "Epoch   62/2000 train_loss: 0.293205 acc: 0.884200\n",
      "Epoch   63/2000 train_loss: 0.292862 acc: 0.884200\n",
      "Epoch   64/2000 train_loss: 0.292522 acc: 0.884500\n",
      "Epoch   65/2000 train_loss: 0.292184 acc: 0.884700\n",
      "Epoch   66/2000 train_loss: 0.291856 acc: 0.884700\n",
      "Epoch   67/2000 train_loss: 0.291534 acc: 0.885000\n",
      "Epoch   68/2000 train_loss: 0.291197 acc: 0.885100\n",
      "Epoch   69/2000 train_loss: 0.290885 acc: 0.885100\n",
      "Epoch   70/2000 train_loss: 0.290592 acc: 0.885400\n",
      "Epoch   71/2000 train_loss: 0.290276 acc: 0.885700\n",
      "Epoch   72/2000 train_loss: 0.289988 acc: 0.885400\n",
      "Epoch   73/2000 train_loss: 0.289700 acc: 0.885700\n",
      "Epoch   74/2000 train_loss: 0.289410 acc: 0.885600\n",
      "Epoch   75/2000 train_loss: 0.289131 acc: 0.886000\n",
      "Epoch   76/2000 train_loss: 0.288841 acc: 0.886200\n",
      "Epoch   77/2000 train_loss: 0.288573 acc: 0.885900\n",
      "Epoch   78/2000 train_loss: 0.288299 acc: 0.885900\n",
      "Epoch   79/2000 train_loss: 0.288043 acc: 0.885900\n",
      "Epoch   80/2000 train_loss: 0.287780 acc: 0.886100\n",
      "Epoch   81/2000 train_loss: 0.287524 acc: 0.886600\n",
      "Epoch   82/2000 train_loss: 0.287269 acc: 0.886000\n",
      "Epoch   83/2000 train_loss: 0.287031 acc: 0.885600\n",
      "Epoch   84/2000 train_loss: 0.286792 acc: 0.885800\n",
      "Epoch   85/2000 train_loss: 0.286531 acc: 0.886500\n",
      "Epoch   86/2000 train_loss: 0.286303 acc: 0.886600\n",
      "Epoch   87/2000 train_loss: 0.286066 acc: 0.886200\n",
      "Epoch   88/2000 train_loss: 0.285831 acc: 0.886400\n",
      "Epoch   89/2000 train_loss: 0.285598 acc: 0.886900\n",
      "Epoch   90/2000 train_loss: 0.285383 acc: 0.886500\n",
      "Epoch   91/2000 train_loss: 0.285164 acc: 0.887200\n",
      "Epoch   92/2000 train_loss: 0.284938 acc: 0.886900\n",
      "Epoch   93/2000 train_loss: 0.284724 acc: 0.887100\n",
      "Epoch   94/2000 train_loss: 0.284524 acc: 0.887100\n",
      "Epoch   95/2000 train_loss: 0.284299 acc: 0.887500\n",
      "Epoch   96/2000 train_loss: 0.284103 acc: 0.887600\n",
      "Epoch   97/2000 train_loss: 0.283892 acc: 0.887300\n",
      "Epoch   98/2000 train_loss: 0.283684 acc: 0.887700\n",
      "Epoch   99/2000 train_loss: 0.283493 acc: 0.887400\n",
      "Epoch  100/2000 train_loss: 0.283288 acc: 0.888100\n",
      "Epoch  101/2000 train_loss: 0.283103 acc: 0.888600\n",
      "Epoch  102/2000 train_loss: 0.282916 acc: 0.888700\n",
      "Epoch  103/2000 train_loss: 0.282721 acc: 0.888300\n",
      "Epoch  104/2000 train_loss: 0.282541 acc: 0.888400\n",
      "Epoch  105/2000 train_loss: 0.282347 acc: 0.888000\n",
      "Epoch  106/2000 train_loss: 0.282176 acc: 0.888300\n",
      "Epoch  107/2000 train_loss: 0.281985 acc: 0.888500\n",
      "Epoch  108/2000 train_loss: 0.281813 acc: 0.888400\n",
      "Epoch  109/2000 train_loss: 0.281628 acc: 0.888600\n",
      "Epoch  110/2000 train_loss: 0.281468 acc: 0.888600\n",
      "Epoch  111/2000 train_loss: 0.281291 acc: 0.888900\n",
      "Epoch  112/2000 train_loss: 0.281134 acc: 0.888700\n",
      "Epoch  113/2000 train_loss: 0.280970 acc: 0.888800\n",
      "Epoch  114/2000 train_loss: 0.280801 acc: 0.889100\n",
      "Epoch  115/2000 train_loss: 0.280638 acc: 0.889100\n",
      "Epoch  116/2000 train_loss: 0.280475 acc: 0.889100\n",
      "Epoch  117/2000 train_loss: 0.280305 acc: 0.889000\n",
      "Epoch  118/2000 train_loss: 0.280152 acc: 0.889600\n",
      "Epoch  119/2000 train_loss: 0.280009 acc: 0.889400\n",
      "Epoch  120/2000 train_loss: 0.279856 acc: 0.889300\n",
      "Epoch  121/2000 train_loss: 0.279698 acc: 0.889600\n",
      "Epoch  122/2000 train_loss: 0.279547 acc: 0.889200\n",
      "Epoch  123/2000 train_loss: 0.279403 acc: 0.889300\n",
      "Epoch  124/2000 train_loss: 0.279253 acc: 0.889700\n",
      "Epoch  125/2000 train_loss: 0.279109 acc: 0.889800\n",
      "Epoch  126/2000 train_loss: 0.278954 acc: 0.889900\n",
      "Epoch  127/2000 train_loss: 0.278822 acc: 0.890100\n",
      "Epoch  128/2000 train_loss: 0.278681 acc: 0.890100\n",
      "Epoch  129/2000 train_loss: 0.278531 acc: 0.890300\n",
      "Epoch  130/2000 train_loss: 0.278404 acc: 0.890400\n",
      "Epoch  131/2000 train_loss: 0.278273 acc: 0.890500\n",
      "Epoch  132/2000 train_loss: 0.278144 acc: 0.890800\n",
      "Epoch  133/2000 train_loss: 0.278007 acc: 0.890100\n",
      "Epoch  134/2000 train_loss: 0.277869 acc: 0.890400\n",
      "Epoch  135/2000 train_loss: 0.277736 acc: 0.890500\n",
      "Epoch  136/2000 train_loss: 0.277617 acc: 0.890600\n",
      "Epoch  137/2000 train_loss: 0.277492 acc: 0.890800\n",
      "Epoch  138/2000 train_loss: 0.277354 acc: 0.891200\n",
      "Epoch  139/2000 train_loss: 0.277234 acc: 0.891200\n",
      "Epoch  140/2000 train_loss: 0.277108 acc: 0.891200\n",
      "Epoch  141/2000 train_loss: 0.276989 acc: 0.891100\n",
      "Epoch  142/2000 train_loss: 0.276869 acc: 0.890800\n",
      "Epoch  143/2000 train_loss: 0.276738 acc: 0.891300\n",
      "Epoch  144/2000 train_loss: 0.276626 acc: 0.891000\n",
      "Epoch  145/2000 train_loss: 0.276500 acc: 0.891200\n",
      "Epoch  146/2000 train_loss: 0.276395 acc: 0.891600\n",
      "Epoch  147/2000 train_loss: 0.276283 acc: 0.890900\n",
      "Epoch  148/2000 train_loss: 0.276162 acc: 0.891300\n",
      "Epoch  149/2000 train_loss: 0.276041 acc: 0.890900\n",
      "Epoch  150/2000 train_loss: 0.275943 acc: 0.891100\n",
      "Epoch  151/2000 train_loss: 0.275823 acc: 0.891100\n",
      "Epoch  152/2000 train_loss: 0.275687 acc: 0.891100\n",
      "Epoch  153/2000 train_loss: 0.275622 acc: 0.892200\n",
      "Epoch  154/2000 train_loss: 0.275515 acc: 0.891400\n",
      "Epoch  155/2000 train_loss: 0.275397 acc: 0.891600\n",
      "Epoch  156/2000 train_loss: 0.275290 acc: 0.891800\n",
      "Epoch  157/2000 train_loss: 0.275192 acc: 0.891900\n",
      "Epoch  158/2000 train_loss: 0.275091 acc: 0.891700\n",
      "Epoch  159/2000 train_loss: 0.274981 acc: 0.891700\n",
      "Epoch  160/2000 train_loss: 0.274881 acc: 0.892200\n",
      "Epoch  161/2000 train_loss: 0.274767 acc: 0.892000\n",
      "Epoch  162/2000 train_loss: 0.274670 acc: 0.892000\n",
      "Epoch  163/2000 train_loss: 0.274571 acc: 0.891700\n",
      "Epoch  164/2000 train_loss: 0.274469 acc: 0.891900\n",
      "Epoch  165/2000 train_loss: 0.274376 acc: 0.891900\n",
      "Epoch  166/2000 train_loss: 0.274283 acc: 0.892300\n",
      "Epoch  167/2000 train_loss: 0.274199 acc: 0.892000\n",
      "Epoch  168/2000 train_loss: 0.274103 acc: 0.892000\n",
      "Epoch  169/2000 train_loss: 0.273999 acc: 0.892300\n",
      "Epoch  170/2000 train_loss: 0.273903 acc: 0.892300\n",
      "Epoch  171/2000 train_loss: 0.273832 acc: 0.892600\n",
      "Epoch  172/2000 train_loss: 0.273724 acc: 0.892000\n",
      "Epoch  173/2000 train_loss: 0.273639 acc: 0.892400\n",
      "Epoch  174/2000 train_loss: 0.273542 acc: 0.891800\n",
      "Epoch  175/2000 train_loss: 0.273455 acc: 0.892200\n",
      "Epoch  176/2000 train_loss: 0.273377 acc: 0.892100\n",
      "Epoch  177/2000 train_loss: 0.273285 acc: 0.892600\n",
      "Epoch  178/2000 train_loss: 0.273186 acc: 0.892500\n",
      "Epoch  179/2000 train_loss: 0.273108 acc: 0.892700\n",
      "Epoch  180/2000 train_loss: 0.273016 acc: 0.892700\n",
      "Epoch  181/2000 train_loss: 0.272941 acc: 0.892600\n",
      "Epoch  182/2000 train_loss: 0.272857 acc: 0.892300\n",
      "Epoch  183/2000 train_loss: 0.272780 acc: 0.892400\n",
      "Epoch  184/2000 train_loss: 0.272696 acc: 0.892600\n",
      "Epoch  185/2000 train_loss: 0.272611 acc: 0.893100\n",
      "Epoch  186/2000 train_loss: 0.272520 acc: 0.893000\n",
      "Epoch  187/2000 train_loss: 0.272447 acc: 0.892700\n",
      "Epoch  188/2000 train_loss: 0.272370 acc: 0.892500\n",
      "Epoch  189/2000 train_loss: 0.272290 acc: 0.893100\n",
      "Epoch  190/2000 train_loss: 0.272211 acc: 0.892600\n",
      "Epoch  191/2000 train_loss: 0.272142 acc: 0.892600\n",
      "Epoch  192/2000 train_loss: 0.272055 acc: 0.892900\n",
      "Epoch  193/2000 train_loss: 0.271966 acc: 0.892800\n",
      "Epoch  194/2000 train_loss: 0.271910 acc: 0.892800\n",
      "Epoch  195/2000 train_loss: 0.271832 acc: 0.893000\n",
      "Epoch  196/2000 train_loss: 0.271768 acc: 0.892900\n",
      "Epoch  197/2000 train_loss: 0.271684 acc: 0.892800\n",
      "Epoch  198/2000 train_loss: 0.271611 acc: 0.893100\n",
      "Epoch  199/2000 train_loss: 0.271533 acc: 0.892700\n",
      "Epoch  200/2000 train_loss: 0.271452 acc: 0.893000\n",
      "Epoch  201/2000 train_loss: 0.271386 acc: 0.892500\n",
      "Epoch  202/2000 train_loss: 0.271311 acc: 0.892900\n",
      "Epoch  203/2000 train_loss: 0.271249 acc: 0.892600\n",
      "Epoch  204/2000 train_loss: 0.271176 acc: 0.892400\n",
      "Epoch  205/2000 train_loss: 0.271100 acc: 0.892800\n",
      "Epoch  206/2000 train_loss: 0.271039 acc: 0.893100\n",
      "Epoch  207/2000 train_loss: 0.270976 acc: 0.892600\n",
      "Epoch  208/2000 train_loss: 0.270896 acc: 0.892700\n",
      "Epoch  209/2000 train_loss: 0.270811 acc: 0.892900\n",
      "Epoch  210/2000 train_loss: 0.270768 acc: 0.892900\n",
      "Epoch  211/2000 train_loss: 0.270696 acc: 0.892900\n",
      "Epoch  212/2000 train_loss: 0.270626 acc: 0.892600\n",
      "Epoch  213/2000 train_loss: 0.270543 acc: 0.892200\n",
      "Epoch  214/2000 train_loss: 0.270476 acc: 0.892300\n",
      "Epoch  215/2000 train_loss: 0.270439 acc: 0.893000\n",
      "Epoch  216/2000 train_loss: 0.270369 acc: 0.893100\n",
      "Epoch  217/2000 train_loss: 0.270314 acc: 0.893300\n",
      "Epoch  218/2000 train_loss: 0.270236 acc: 0.892800\n",
      "Epoch  219/2000 train_loss: 0.270183 acc: 0.893400\n",
      "Epoch  220/2000 train_loss: 0.270121 acc: 0.892600\n",
      "Epoch  221/2000 train_loss: 0.270053 acc: 0.893300\n",
      "Epoch  222/2000 train_loss: 0.269989 acc: 0.893400\n",
      "Epoch  223/2000 train_loss: 0.269923 acc: 0.893400\n",
      "Epoch  224/2000 train_loss: 0.269877 acc: 0.893100\n",
      "Epoch  225/2000 train_loss: 0.269793 acc: 0.893500\n",
      "Epoch  226/2000 train_loss: 0.269755 acc: 0.893200\n",
      "Epoch  227/2000 train_loss: 0.269686 acc: 0.893200\n",
      "Epoch  228/2000 train_loss: 0.269642 acc: 0.893400\n",
      "Epoch  229/2000 train_loss: 0.269581 acc: 0.893400\n",
      "Epoch  230/2000 train_loss: 0.269520 acc: 0.893000\n",
      "Epoch  231/2000 train_loss: 0.269452 acc: 0.893200\n",
      "Epoch  232/2000 train_loss: 0.269397 acc: 0.893100\n",
      "Epoch  233/2000 train_loss: 0.269343 acc: 0.893100\n",
      "Epoch  234/2000 train_loss: 0.269293 acc: 0.893700\n",
      "Epoch  235/2000 train_loss: 0.269235 acc: 0.893500\n",
      "Epoch  236/2000 train_loss: 0.269182 acc: 0.893500\n",
      "Epoch  237/2000 train_loss: 0.269126 acc: 0.893400\n",
      "Epoch  238/2000 train_loss: 0.269054 acc: 0.893100\n",
      "Epoch  239/2000 train_loss: 0.269013 acc: 0.893100\n",
      "Epoch  240/2000 train_loss: 0.268943 acc: 0.893300\n",
      "Epoch  241/2000 train_loss: 0.268902 acc: 0.893400\n",
      "Epoch  242/2000 train_loss: 0.268854 acc: 0.893700\n",
      "Epoch  243/2000 train_loss: 0.268788 acc: 0.893400\n",
      "Epoch  244/2000 train_loss: 0.268747 acc: 0.893200\n",
      "Epoch  245/2000 train_loss: 0.268693 acc: 0.893300\n",
      "Epoch  246/2000 train_loss: 0.268650 acc: 0.893200\n",
      "Epoch  247/2000 train_loss: 0.268581 acc: 0.893600\n",
      "Epoch  248/2000 train_loss: 0.268540 acc: 0.893200\n",
      "Epoch  249/2000 train_loss: 0.268479 acc: 0.893100\n",
      "Epoch  250/2000 train_loss: 0.268419 acc: 0.893100\n",
      "Epoch  251/2000 train_loss: 0.268385 acc: 0.893200\n",
      "Epoch  252/2000 train_loss: 0.268330 acc: 0.893200\n",
      "Epoch  253/2000 train_loss: 0.268282 acc: 0.893500\n",
      "Epoch  254/2000 train_loss: 0.268237 acc: 0.893400\n",
      "Epoch  255/2000 train_loss: 0.268184 acc: 0.893200\n",
      "Epoch  256/2000 train_loss: 0.268124 acc: 0.893200\n",
      "Epoch  257/2000 train_loss: 0.268088 acc: 0.893000\n",
      "Epoch  258/2000 train_loss: 0.268036 acc: 0.893200\n",
      "Epoch  259/2000 train_loss: 0.268000 acc: 0.893000\n",
      "Epoch  260/2000 train_loss: 0.267929 acc: 0.893200\n",
      "Epoch  261/2000 train_loss: 0.267881 acc: 0.893700\n",
      "Epoch  262/2000 train_loss: 0.267852 acc: 0.893500\n",
      "Epoch  263/2000 train_loss: 0.267803 acc: 0.893300\n",
      "Epoch  264/2000 train_loss: 0.267754 acc: 0.893400\n",
      "Epoch  265/2000 train_loss: 0.267683 acc: 0.893000\n",
      "Epoch  266/2000 train_loss: 0.267648 acc: 0.894000\n",
      "Epoch  267/2000 train_loss: 0.267612 acc: 0.893400\n",
      "Epoch  268/2000 train_loss: 0.267563 acc: 0.893800\n",
      "Epoch  269/2000 train_loss: 0.267519 acc: 0.893700\n",
      "Epoch  270/2000 train_loss: 0.267470 acc: 0.893400\n",
      "Epoch  271/2000 train_loss: 0.267428 acc: 0.893800\n",
      "Epoch  272/2000 train_loss: 0.267372 acc: 0.894100\n",
      "Epoch  273/2000 train_loss: 0.267351 acc: 0.894000\n",
      "Epoch  274/2000 train_loss: 0.267295 acc: 0.893700\n",
      "Epoch  275/2000 train_loss: 0.267247 acc: 0.894200\n",
      "Epoch  276/2000 train_loss: 0.267205 acc: 0.893700\n",
      "Epoch  277/2000 train_loss: 0.267169 acc: 0.894200\n",
      "Epoch  278/2000 train_loss: 0.267116 acc: 0.893700\n",
      "Epoch  279/2000 train_loss: 0.267072 acc: 0.893800\n",
      "Epoch  280/2000 train_loss: 0.267041 acc: 0.894100\n",
      "Epoch  281/2000 train_loss: 0.267000 acc: 0.894500\n",
      "Epoch  282/2000 train_loss: 0.266951 acc: 0.894400\n",
      "Epoch  283/2000 train_loss: 0.266917 acc: 0.894000\n",
      "Epoch  284/2000 train_loss: 0.266861 acc: 0.894500\n",
      "Epoch  285/2000 train_loss: 0.266828 acc: 0.894600\n",
      "Epoch  286/2000 train_loss: 0.266781 acc: 0.894300\n",
      "Epoch  287/2000 train_loss: 0.266750 acc: 0.894000\n",
      "Epoch  288/2000 train_loss: 0.266704 acc: 0.894500\n",
      "Epoch  289/2000 train_loss: 0.266656 acc: 0.894300\n",
      "Epoch  290/2000 train_loss: 0.266617 acc: 0.894400\n",
      "Epoch  291/2000 train_loss: 0.266592 acc: 0.894300\n",
      "Epoch  292/2000 train_loss: 0.266535 acc: 0.894200\n",
      "Epoch  293/2000 train_loss: 0.266492 acc: 0.894200\n",
      "Epoch  294/2000 train_loss: 0.266467 acc: 0.894100\n",
      "Epoch  295/2000 train_loss: 0.266425 acc: 0.894600\n",
      "Epoch  296/2000 train_loss: 0.266375 acc: 0.894100\n",
      "Epoch  297/2000 train_loss: 0.266340 acc: 0.894500\n",
      "Epoch  298/2000 train_loss: 0.266306 acc: 0.894700\n",
      "Epoch  299/2000 train_loss: 0.266264 acc: 0.894700\n",
      "Epoch  300/2000 train_loss: 0.266242 acc: 0.894600\n",
      "Epoch  301/2000 train_loss: 0.266198 acc: 0.894300\n",
      "Epoch  302/2000 train_loss: 0.266156 acc: 0.894800\n",
      "Epoch  303/2000 train_loss: 0.266122 acc: 0.894700\n",
      "Epoch  304/2000 train_loss: 0.266086 acc: 0.894300\n",
      "Epoch  305/2000 train_loss: 0.266034 acc: 0.893700\n",
      "Epoch  306/2000 train_loss: 0.266002 acc: 0.894500\n",
      "Epoch  307/2000 train_loss: 0.265971 acc: 0.894200\n",
      "Epoch  308/2000 train_loss: 0.265943 acc: 0.894600\n",
      "Epoch  309/2000 train_loss: 0.265893 acc: 0.894300\n",
      "Epoch  310/2000 train_loss: 0.265865 acc: 0.894600\n",
      "Epoch  311/2000 train_loss: 0.265823 acc: 0.894700\n",
      "Epoch  312/2000 train_loss: 0.265787 acc: 0.894500\n",
      "Epoch  313/2000 train_loss: 0.265758 acc: 0.894600\n",
      "Epoch  314/2000 train_loss: 0.265718 acc: 0.894300\n",
      "Epoch  315/2000 train_loss: 0.265689 acc: 0.894700\n",
      "Epoch  316/2000 train_loss: 0.265657 acc: 0.894500\n",
      "Epoch  317/2000 train_loss: 0.265612 acc: 0.894500\n",
      "Epoch  318/2000 train_loss: 0.265581 acc: 0.894900\n",
      "Epoch  319/2000 train_loss: 0.265541 acc: 0.894600\n",
      "Epoch  320/2000 train_loss: 0.265508 acc: 0.894300\n",
      "Epoch  321/2000 train_loss: 0.265460 acc: 0.894800\n",
      "Epoch  322/2000 train_loss: 0.265449 acc: 0.894500\n",
      "Epoch  323/2000 train_loss: 0.265406 acc: 0.894600\n",
      "Epoch  324/2000 train_loss: 0.265374 acc: 0.894900\n",
      "Epoch  325/2000 train_loss: 0.265348 acc: 0.895000\n",
      "Epoch  326/2000 train_loss: 0.265303 acc: 0.894700\n",
      "Epoch  327/2000 train_loss: 0.265270 acc: 0.894700\n",
      "Epoch  328/2000 train_loss: 0.265221 acc: 0.895100\n",
      "Epoch  329/2000 train_loss: 0.265220 acc: 0.894800\n",
      "Epoch  330/2000 train_loss: 0.265173 acc: 0.894600\n",
      "Epoch  331/2000 train_loss: 0.265139 acc: 0.894800\n",
      "Epoch  332/2000 train_loss: 0.265097 acc: 0.894400\n",
      "Epoch  333/2000 train_loss: 0.265084 acc: 0.894500\n",
      "Epoch  334/2000 train_loss: 0.265046 acc: 0.894900\n",
      "Epoch  335/2000 train_loss: 0.265003 acc: 0.894900\n",
      "Epoch  336/2000 train_loss: 0.264979 acc: 0.894900\n",
      "Epoch  337/2000 train_loss: 0.264956 acc: 0.895100\n",
      "Epoch  338/2000 train_loss: 0.264922 acc: 0.894800\n",
      "Epoch  339/2000 train_loss: 0.264884 acc: 0.895100\n",
      "Epoch  340/2000 train_loss: 0.264856 acc: 0.895500\n",
      "Epoch  341/2000 train_loss: 0.264821 acc: 0.894600\n",
      "Epoch  342/2000 train_loss: 0.264802 acc: 0.895200\n",
      "Epoch  343/2000 train_loss: 0.264769 acc: 0.895000\n",
      "Epoch  344/2000 train_loss: 0.264727 acc: 0.894800\n",
      "Epoch  345/2000 train_loss: 0.264690 acc: 0.895200\n",
      "Epoch  346/2000 train_loss: 0.264674 acc: 0.895000\n",
      "Epoch  347/2000 train_loss: 0.264633 acc: 0.895100\n",
      "Epoch  348/2000 train_loss: 0.264612 acc: 0.895100\n",
      "Epoch  349/2000 train_loss: 0.264580 acc: 0.895300\n",
      "Epoch  350/2000 train_loss: 0.264551 acc: 0.895100\n",
      "Epoch  351/2000 train_loss: 0.264511 acc: 0.895800\n",
      "Epoch  352/2000 train_loss: 0.264497 acc: 0.895500\n",
      "Epoch  353/2000 train_loss: 0.264463 acc: 0.895000\n",
      "Epoch  354/2000 train_loss: 0.264431 acc: 0.895300\n",
      "Epoch  355/2000 train_loss: 0.264400 acc: 0.895700\n",
      "Epoch  356/2000 train_loss: 0.264380 acc: 0.895500\n",
      "Epoch  357/2000 train_loss: 0.264347 acc: 0.895200\n",
      "Epoch  358/2000 train_loss: 0.264311 acc: 0.895700\n",
      "Epoch  359/2000 train_loss: 0.264295 acc: 0.895800\n",
      "Epoch  360/2000 train_loss: 0.264262 acc: 0.895600\n",
      "Epoch  361/2000 train_loss: 0.264238 acc: 0.895700\n",
      "Epoch  362/2000 train_loss: 0.264204 acc: 0.895500\n",
      "Epoch  363/2000 train_loss: 0.264173 acc: 0.895500\n",
      "Epoch  364/2000 train_loss: 0.264147 acc: 0.895700\n",
      "Epoch  365/2000 train_loss: 0.264110 acc: 0.895400\n",
      "Epoch  366/2000 train_loss: 0.264085 acc: 0.895700\n",
      "Epoch  367/2000 train_loss: 0.264063 acc: 0.895700\n",
      "Epoch  368/2000 train_loss: 0.264037 acc: 0.895900\n",
      "Epoch  369/2000 train_loss: 0.264012 acc: 0.895700\n",
      "Epoch  370/2000 train_loss: 0.263978 acc: 0.895500\n",
      "Epoch  371/2000 train_loss: 0.263963 acc: 0.895400\n",
      "Epoch  372/2000 train_loss: 0.263929 acc: 0.895800\n",
      "Epoch  373/2000 train_loss: 0.263902 acc: 0.895500\n",
      "Epoch  374/2000 train_loss: 0.263875 acc: 0.895200\n",
      "Epoch  375/2000 train_loss: 0.263827 acc: 0.895600\n",
      "Epoch  376/2000 train_loss: 0.263831 acc: 0.895700\n",
      "Epoch  377/2000 train_loss: 0.263800 acc: 0.895300\n",
      "Epoch  378/2000 train_loss: 0.263770 acc: 0.895900\n",
      "Epoch  379/2000 train_loss: 0.263745 acc: 0.895700\n",
      "Epoch  380/2000 train_loss: 0.263713 acc: 0.895600\n",
      "Epoch  381/2000 train_loss: 0.263690 acc: 0.895600\n",
      "Epoch  382/2000 train_loss: 0.263669 acc: 0.895600\n",
      "Epoch  383/2000 train_loss: 0.263647 acc: 0.895700\n",
      "Epoch  384/2000 train_loss: 0.263617 acc: 0.896000\n",
      "Epoch  385/2000 train_loss: 0.263582 acc: 0.895800\n",
      "Epoch  386/2000 train_loss: 0.263572 acc: 0.895600\n",
      "Epoch  387/2000 train_loss: 0.263540 acc: 0.895900\n",
      "Epoch  388/2000 train_loss: 0.263508 acc: 0.895500\n",
      "Epoch  389/2000 train_loss: 0.263484 acc: 0.895900\n",
      "Epoch  390/2000 train_loss: 0.263447 acc: 0.895300\n",
      "Epoch  391/2000 train_loss: 0.263440 acc: 0.895500\n",
      "Epoch  392/2000 train_loss: 0.263415 acc: 0.895800\n",
      "Epoch  393/2000 train_loss: 0.263387 acc: 0.895500\n",
      "Epoch  394/2000 train_loss: 0.263367 acc: 0.895700\n",
      "Epoch  395/2000 train_loss: 0.263329 acc: 0.895800\n",
      "Epoch  396/2000 train_loss: 0.263316 acc: 0.895600\n",
      "Epoch  397/2000 train_loss: 0.263293 acc: 0.895600\n",
      "Epoch  398/2000 train_loss: 0.263270 acc: 0.895800\n",
      "Epoch  399/2000 train_loss: 0.263216 acc: 0.895500\n",
      "Epoch  400/2000 train_loss: 0.263211 acc: 0.895800\n",
      "Epoch  401/2000 train_loss: 0.263199 acc: 0.896000\n",
      "Epoch  402/2000 train_loss: 0.263178 acc: 0.895900\n",
      "Epoch  403/2000 train_loss: 0.263142 acc: 0.895500\n",
      "Epoch  404/2000 train_loss: 0.263122 acc: 0.895900\n",
      "Epoch  405/2000 train_loss: 0.263101 acc: 0.895600\n",
      "Epoch  406/2000 train_loss: 0.263075 acc: 0.896200\n",
      "Epoch  407/2000 train_loss: 0.263044 acc: 0.896100\n",
      "Epoch  408/2000 train_loss: 0.263023 acc: 0.896100\n",
      "Epoch  409/2000 train_loss: 0.263012 acc: 0.895500\n",
      "Epoch  410/2000 train_loss: 0.262979 acc: 0.896000\n",
      "Epoch  411/2000 train_loss: 0.262963 acc: 0.896100\n",
      "Epoch  412/2000 train_loss: 0.262931 acc: 0.896200\n",
      "Epoch  413/2000 train_loss: 0.262914 acc: 0.895700\n",
      "Epoch  414/2000 train_loss: 0.262894 acc: 0.895900\n",
      "Epoch  415/2000 train_loss: 0.262873 acc: 0.895800\n",
      "Epoch  416/2000 train_loss: 0.262844 acc: 0.895900\n",
      "Epoch  417/2000 train_loss: 0.262820 acc: 0.895900\n",
      "Epoch  418/2000 train_loss: 0.262807 acc: 0.895600\n",
      "Epoch  419/2000 train_loss: 0.262776 acc: 0.895600\n",
      "Epoch  420/2000 train_loss: 0.262745 acc: 0.896400\n",
      "Epoch  421/2000 train_loss: 0.262747 acc: 0.896300\n",
      "Epoch  422/2000 train_loss: 0.262712 acc: 0.895700\n",
      "Epoch  423/2000 train_loss: 0.262687 acc: 0.895800\n",
      "Epoch  424/2000 train_loss: 0.262663 acc: 0.896100\n",
      "Epoch  425/2000 train_loss: 0.262644 acc: 0.896100\n",
      "Epoch  426/2000 train_loss: 0.262627 acc: 0.895800\n",
      "Epoch  427/2000 train_loss: 0.262595 acc: 0.895800\n",
      "Epoch  428/2000 train_loss: 0.262595 acc: 0.896300\n",
      "Epoch  429/2000 train_loss: 0.262536 acc: 0.896100\n",
      "Epoch  430/2000 train_loss: 0.262538 acc: 0.895800\n",
      "Epoch  431/2000 train_loss: 0.262520 acc: 0.896300\n",
      "Epoch  432/2000 train_loss: 0.262488 acc: 0.896300\n",
      "Epoch  433/2000 train_loss: 0.262478 acc: 0.896000\n",
      "Epoch  434/2000 train_loss: 0.262440 acc: 0.896400\n",
      "Epoch  435/2000 train_loss: 0.262436 acc: 0.896100\n",
      "Epoch  436/2000 train_loss: 0.262397 acc: 0.896200\n",
      "Epoch  437/2000 train_loss: 0.262379 acc: 0.895700\n",
      "Epoch  438/2000 train_loss: 0.262368 acc: 0.896000\n",
      "Epoch  439/2000 train_loss: 0.262347 acc: 0.895800\n",
      "Epoch  440/2000 train_loss: 0.262335 acc: 0.896000\n",
      "Epoch  441/2000 train_loss: 0.262308 acc: 0.895900\n",
      "Epoch  442/2000 train_loss: 0.262282 acc: 0.896100\n",
      "Epoch  443/2000 train_loss: 0.262269 acc: 0.895900\n",
      "Epoch  444/2000 train_loss: 0.262239 acc: 0.896200\n",
      "Epoch  445/2000 train_loss: 0.262219 acc: 0.896200\n",
      "Epoch  446/2000 train_loss: 0.262202 acc: 0.895800\n",
      "Epoch  447/2000 train_loss: 0.262182 acc: 0.895900\n",
      "Epoch  448/2000 train_loss: 0.262165 acc: 0.896000\n",
      "Epoch  449/2000 train_loss: 0.262143 acc: 0.895900\n",
      "Epoch  450/2000 train_loss: 0.262124 acc: 0.896000\n",
      "Epoch  451/2000 train_loss: 0.262103 acc: 0.896000\n",
      "Epoch  452/2000 train_loss: 0.262083 acc: 0.896300\n",
      "Epoch  453/2000 train_loss: 0.262053 acc: 0.896100\n",
      "Epoch  454/2000 train_loss: 0.262041 acc: 0.896200\n",
      "Epoch  455/2000 train_loss: 0.262020 acc: 0.895800\n",
      "Epoch  456/2000 train_loss: 0.262011 acc: 0.896100\n",
      "Epoch  457/2000 train_loss: 0.261981 acc: 0.896100\n",
      "Epoch  458/2000 train_loss: 0.261953 acc: 0.896400\n",
      "Epoch  459/2000 train_loss: 0.261953 acc: 0.895700\n",
      "Epoch  460/2000 train_loss: 0.261929 acc: 0.896100\n",
      "Epoch  461/2000 train_loss: 0.261909 acc: 0.896800\n",
      "Epoch  462/2000 train_loss: 0.261878 acc: 0.896100\n",
      "Epoch  463/2000 train_loss: 0.261871 acc: 0.896800\n",
      "Epoch  464/2000 train_loss: 0.261842 acc: 0.896700\n",
      "Epoch  465/2000 train_loss: 0.261818 acc: 0.896500\n",
      "Epoch  466/2000 train_loss: 0.261816 acc: 0.896500\n",
      "Epoch  467/2000 train_loss: 0.261797 acc: 0.896200\n",
      "Epoch  468/2000 train_loss: 0.261780 acc: 0.895900\n",
      "Epoch  469/2000 train_loss: 0.261756 acc: 0.896700\n",
      "Epoch  470/2000 train_loss: 0.261733 acc: 0.896100\n",
      "Epoch  471/2000 train_loss: 0.261700 acc: 0.896600\n",
      "Epoch  472/2000 train_loss: 0.261712 acc: 0.896200\n",
      "Epoch  473/2000 train_loss: 0.261684 acc: 0.896000\n",
      "Epoch  474/2000 train_loss: 0.261670 acc: 0.896200\n",
      "Epoch  475/2000 train_loss: 0.261628 acc: 0.896100\n",
      "Epoch  476/2000 train_loss: 0.261635 acc: 0.896400\n",
      "Epoch  477/2000 train_loss: 0.261615 acc: 0.895900\n",
      "Epoch  478/2000 train_loss: 0.261596 acc: 0.896900\n",
      "Epoch  479/2000 train_loss: 0.261570 acc: 0.896500\n",
      "Epoch  480/2000 train_loss: 0.261564 acc: 0.896200\n",
      "Epoch  481/2000 train_loss: 0.261542 acc: 0.896000\n",
      "Epoch  482/2000 train_loss: 0.261519 acc: 0.896200\n",
      "Epoch  483/2000 train_loss: 0.261493 acc: 0.896800\n",
      "Epoch  484/2000 train_loss: 0.261477 acc: 0.896100\n",
      "Epoch  485/2000 train_loss: 0.261468 acc: 0.896700\n",
      "Epoch  486/2000 train_loss: 0.261456 acc: 0.896400\n",
      "Epoch  487/2000 train_loss: 0.261425 acc: 0.896600\n",
      "Epoch  488/2000 train_loss: 0.261419 acc: 0.896000\n",
      "Epoch  489/2000 train_loss: 0.261389 acc: 0.896400\n",
      "Epoch  490/2000 train_loss: 0.261388 acc: 0.896500\n",
      "Epoch  491/2000 train_loss: 0.261361 acc: 0.896800\n",
      "Epoch  492/2000 train_loss: 0.261344 acc: 0.896700\n",
      "Epoch  493/2000 train_loss: 0.261328 acc: 0.896500\n",
      "Epoch  494/2000 train_loss: 0.261306 acc: 0.896800\n",
      "Epoch  495/2000 train_loss: 0.261294 acc: 0.896900\n",
      "Epoch  496/2000 train_loss: 0.261267 acc: 0.896600\n",
      "Epoch  497/2000 train_loss: 0.261244 acc: 0.896400\n",
      "Epoch  498/2000 train_loss: 0.261245 acc: 0.896800\n",
      "Epoch  499/2000 train_loss: 0.261200 acc: 0.896600\n",
      "Epoch  500/2000 train_loss: 0.261215 acc: 0.897200\n",
      "Epoch  501/2000 train_loss: 0.261193 acc: 0.897200\n",
      "Epoch  502/2000 train_loss: 0.261180 acc: 0.896900\n",
      "Epoch  503/2000 train_loss: 0.261163 acc: 0.896900\n",
      "Epoch  504/2000 train_loss: 0.261137 acc: 0.896900\n",
      "Epoch  505/2000 train_loss: 0.261116 acc: 0.896300\n",
      "Epoch  506/2000 train_loss: 0.261106 acc: 0.897500\n",
      "Epoch  507/2000 train_loss: 0.261089 acc: 0.896800\n",
      "Epoch  508/2000 train_loss: 0.261077 acc: 0.896300\n",
      "Epoch  509/2000 train_loss: 0.261061 acc: 0.897000\n",
      "Epoch  510/2000 train_loss: 0.261044 acc: 0.896700\n",
      "Epoch  511/2000 train_loss: 0.261030 acc: 0.896900\n",
      "Epoch  512/2000 train_loss: 0.261008 acc: 0.896700\n",
      "Epoch  513/2000 train_loss: 0.260989 acc: 0.896900\n",
      "Epoch  514/2000 train_loss: 0.260976 acc: 0.897100\n",
      "Epoch  515/2000 train_loss: 0.260952 acc: 0.896700\n",
      "Epoch  516/2000 train_loss: 0.260936 acc: 0.897100\n",
      "Epoch  517/2000 train_loss: 0.260929 acc: 0.896800\n",
      "Epoch  518/2000 train_loss: 0.260899 acc: 0.897300\n",
      "Epoch  519/2000 train_loss: 0.260904 acc: 0.897200\n",
      "Epoch  520/2000 train_loss: 0.260884 acc: 0.897100\n",
      "Epoch  521/2000 train_loss: 0.260859 acc: 0.897200\n",
      "Epoch  522/2000 train_loss: 0.260845 acc: 0.896900\n",
      "Epoch  523/2000 train_loss: 0.260833 acc: 0.896600\n",
      "Epoch  524/2000 train_loss: 0.260807 acc: 0.897200\n",
      "Epoch  525/2000 train_loss: 0.260790 acc: 0.897800\n",
      "Epoch  526/2000 train_loss: 0.260792 acc: 0.897000\n",
      "Epoch  527/2000 train_loss: 0.260772 acc: 0.897200\n",
      "Epoch  528/2000 train_loss: 0.260753 acc: 0.897100\n",
      "Epoch  529/2000 train_loss: 0.260728 acc: 0.897500\n",
      "Epoch  530/2000 train_loss: 0.260726 acc: 0.897600\n",
      "Epoch  531/2000 train_loss: 0.260706 acc: 0.897100\n",
      "Epoch  532/2000 train_loss: 0.260685 acc: 0.897200\n",
      "Epoch  533/2000 train_loss: 0.260671 acc: 0.897100\n",
      "Epoch  534/2000 train_loss: 0.260662 acc: 0.897500\n",
      "Epoch  535/2000 train_loss: 0.260651 acc: 0.897400\n",
      "Epoch  536/2000 train_loss: 0.260640 acc: 0.897100\n",
      "Epoch  537/2000 train_loss: 0.260610 acc: 0.897300\n",
      "Epoch  538/2000 train_loss: 0.260597 acc: 0.897400\n",
      "Epoch  539/2000 train_loss: 0.260587 acc: 0.897300\n",
      "Epoch  540/2000 train_loss: 0.260579 acc: 0.897600\n",
      "Epoch  541/2000 train_loss: 0.260550 acc: 0.897200\n",
      "Epoch  542/2000 train_loss: 0.260537 acc: 0.896700\n",
      "Epoch  543/2000 train_loss: 0.260522 acc: 0.897200\n",
      "Epoch  544/2000 train_loss: 0.260505 acc: 0.897300\n",
      "Epoch  545/2000 train_loss: 0.260492 acc: 0.897400\n",
      "Epoch  546/2000 train_loss: 0.260467 acc: 0.897600\n",
      "Epoch  547/2000 train_loss: 0.260476 acc: 0.897600\n",
      "Epoch  548/2000 train_loss: 0.260452 acc: 0.897700\n",
      "Epoch  549/2000 train_loss: 0.260443 acc: 0.897600\n",
      "Epoch  550/2000 train_loss: 0.260425 acc: 0.897400\n",
      "Epoch  551/2000 train_loss: 0.260406 acc: 0.897600\n",
      "Epoch  552/2000 train_loss: 0.260404 acc: 0.898000\n",
      "Epoch  553/2000 train_loss: 0.260384 acc: 0.897600\n",
      "Epoch  554/2000 train_loss: 0.260367 acc: 0.897800\n",
      "Epoch  555/2000 train_loss: 0.260339 acc: 0.897400\n",
      "Epoch  556/2000 train_loss: 0.260337 acc: 0.897400\n",
      "Epoch  557/2000 train_loss: 0.260317 acc: 0.897900\n",
      "Epoch  558/2000 train_loss: 0.260314 acc: 0.897300\n",
      "Epoch  559/2000 train_loss: 0.260298 acc: 0.897600\n",
      "Epoch  560/2000 train_loss: 0.260280 acc: 0.897500\n",
      "Epoch  561/2000 train_loss: 0.260270 acc: 0.897700\n",
      "Epoch  562/2000 train_loss: 0.260257 acc: 0.897700\n",
      "Epoch  563/2000 train_loss: 0.260235 acc: 0.897800\n",
      "Epoch  564/2000 train_loss: 0.260220 acc: 0.897900\n",
      "Epoch  565/2000 train_loss: 0.260216 acc: 0.897500\n",
      "Epoch  566/2000 train_loss: 0.260190 acc: 0.897200\n",
      "Epoch  567/2000 train_loss: 0.260193 acc: 0.897900\n",
      "Epoch  568/2000 train_loss: 0.260162 acc: 0.897800\n",
      "Epoch  569/2000 train_loss: 0.260156 acc: 0.897700\n",
      "Epoch  570/2000 train_loss: 0.260138 acc: 0.897600\n",
      "Epoch  571/2000 train_loss: 0.260125 acc: 0.897400\n",
      "Epoch  572/2000 train_loss: 0.260116 acc: 0.897600\n",
      "Epoch  573/2000 train_loss: 0.260101 acc: 0.898300\n",
      "Epoch  574/2000 train_loss: 0.260084 acc: 0.898000\n",
      "Epoch  575/2000 train_loss: 0.260066 acc: 0.897500\n",
      "Epoch  576/2000 train_loss: 0.260055 acc: 0.897400\n",
      "Epoch  577/2000 train_loss: 0.260052 acc: 0.897600\n",
      "Epoch  578/2000 train_loss: 0.260034 acc: 0.897900\n",
      "Epoch  579/2000 train_loss: 0.260023 acc: 0.897800\n",
      "Epoch  580/2000 train_loss: 0.260007 acc: 0.897800\n",
      "Epoch  581/2000 train_loss: 0.259995 acc: 0.898000\n",
      "Epoch  582/2000 train_loss: 0.259977 acc: 0.897700\n",
      "Epoch  583/2000 train_loss: 0.259963 acc: 0.897800\n",
      "Epoch  584/2000 train_loss: 0.259960 acc: 0.898300\n",
      "Epoch  585/2000 train_loss: 0.259947 acc: 0.897800\n",
      "Epoch  586/2000 train_loss: 0.259937 acc: 0.898100\n",
      "Epoch  587/2000 train_loss: 0.259917 acc: 0.897800\n",
      "Epoch  588/2000 train_loss: 0.259902 acc: 0.897900\n",
      "Epoch  589/2000 train_loss: 0.259889 acc: 0.897900\n",
      "Epoch  590/2000 train_loss: 0.259880 acc: 0.898200\n",
      "Epoch  591/2000 train_loss: 0.259860 acc: 0.897800\n",
      "Epoch  592/2000 train_loss: 0.259847 acc: 0.897800\n",
      "Epoch  593/2000 train_loss: 0.259836 acc: 0.898100\n",
      "Epoch  594/2000 train_loss: 0.259826 acc: 0.898000\n",
      "Epoch  595/2000 train_loss: 0.259817 acc: 0.898300\n",
      "Epoch  596/2000 train_loss: 0.259792 acc: 0.897800\n",
      "Epoch  597/2000 train_loss: 0.259791 acc: 0.898300\n",
      "Epoch  598/2000 train_loss: 0.259768 acc: 0.898100\n",
      "Epoch  599/2000 train_loss: 0.259755 acc: 0.898400\n",
      "Epoch  600/2000 train_loss: 0.259742 acc: 0.898100\n",
      "Epoch  601/2000 train_loss: 0.259723 acc: 0.898000\n",
      "Epoch  602/2000 train_loss: 0.259727 acc: 0.898100\n",
      "Epoch  603/2000 train_loss: 0.259705 acc: 0.898000\n",
      "Epoch  604/2000 train_loss: 0.259691 acc: 0.897700\n",
      "Epoch  605/2000 train_loss: 0.259686 acc: 0.898100\n",
      "Epoch  606/2000 train_loss: 0.259670 acc: 0.898100\n",
      "Epoch  607/2000 train_loss: 0.259652 acc: 0.898400\n",
      "Epoch  608/2000 train_loss: 0.259638 acc: 0.897800\n",
      "Epoch  609/2000 train_loss: 0.259627 acc: 0.898300\n",
      "Epoch  610/2000 train_loss: 0.259624 acc: 0.898100\n",
      "Epoch  611/2000 train_loss: 0.259609 acc: 0.897800\n",
      "Epoch  612/2000 train_loss: 0.259598 acc: 0.898400\n",
      "Epoch  613/2000 train_loss: 0.259578 acc: 0.898100\n",
      "Epoch  614/2000 train_loss: 0.259550 acc: 0.898100\n",
      "Epoch  615/2000 train_loss: 0.259559 acc: 0.897700\n",
      "Epoch  616/2000 train_loss: 0.259550 acc: 0.898000\n",
      "Epoch  617/2000 train_loss: 0.259535 acc: 0.898300\n",
      "Epoch  618/2000 train_loss: 0.259516 acc: 0.898400\n",
      "Epoch  619/2000 train_loss: 0.259510 acc: 0.898300\n",
      "Epoch  620/2000 train_loss: 0.259492 acc: 0.898900\n",
      "Epoch  621/2000 train_loss: 0.259481 acc: 0.898700\n",
      "Epoch  622/2000 train_loss: 0.259476 acc: 0.898600\n",
      "Epoch  623/2000 train_loss: 0.259461 acc: 0.898300\n",
      "Epoch  624/2000 train_loss: 0.259441 acc: 0.898100\n",
      "Epoch  625/2000 train_loss: 0.259443 acc: 0.898300\n",
      "Epoch  626/2000 train_loss: 0.259413 acc: 0.898000\n",
      "Epoch  627/2000 train_loss: 0.259419 acc: 0.898100\n",
      "Epoch  628/2000 train_loss: 0.259406 acc: 0.898300\n",
      "Epoch  629/2000 train_loss: 0.259392 acc: 0.898400\n",
      "Epoch  630/2000 train_loss: 0.259376 acc: 0.897900\n",
      "Epoch  631/2000 train_loss: 0.259369 acc: 0.897900\n",
      "Epoch  632/2000 train_loss: 0.259351 acc: 0.898600\n",
      "Epoch  633/2000 train_loss: 0.259348 acc: 0.898200\n",
      "Epoch  634/2000 train_loss: 0.259335 acc: 0.898100\n",
      "Epoch  635/2000 train_loss: 0.259313 acc: 0.898000\n",
      "Epoch  636/2000 train_loss: 0.259301 acc: 0.898600\n",
      "Epoch  637/2000 train_loss: 0.259295 acc: 0.898200\n",
      "Epoch  638/2000 train_loss: 0.259286 acc: 0.898500\n",
      "Epoch  639/2000 train_loss: 0.259272 acc: 0.898000\n",
      "Epoch  640/2000 train_loss: 0.259261 acc: 0.898300\n",
      "Epoch  641/2000 train_loss: 0.259248 acc: 0.898500\n",
      "Epoch  642/2000 train_loss: 0.259243 acc: 0.898100\n",
      "Epoch  643/2000 train_loss: 0.259231 acc: 0.898200\n",
      "Epoch  644/2000 train_loss: 0.259205 acc: 0.898000\n",
      "Epoch  645/2000 train_loss: 0.259196 acc: 0.898300\n",
      "Epoch  646/2000 train_loss: 0.259203 acc: 0.898200\n",
      "Epoch  647/2000 train_loss: 0.259184 acc: 0.898300\n",
      "Epoch  648/2000 train_loss: 0.259171 acc: 0.898200\n",
      "Epoch  649/2000 train_loss: 0.259149 acc: 0.898200\n",
      "Epoch  650/2000 train_loss: 0.259145 acc: 0.898400\n",
      "Epoch  651/2000 train_loss: 0.259134 acc: 0.898200\n",
      "Epoch  652/2000 train_loss: 0.259126 acc: 0.898300\n",
      "Epoch  653/2000 train_loss: 0.259114 acc: 0.898200\n",
      "Epoch  654/2000 train_loss: 0.259114 acc: 0.898200\n",
      "Epoch  655/2000 train_loss: 0.259080 acc: 0.898400\n",
      "Epoch  656/2000 train_loss: 0.259085 acc: 0.898300\n",
      "Epoch  657/2000 train_loss: 0.259075 acc: 0.898300\n",
      "Epoch  658/2000 train_loss: 0.259056 acc: 0.897900\n",
      "Epoch  659/2000 train_loss: 0.259049 acc: 0.898100\n",
      "Epoch  660/2000 train_loss: 0.259042 acc: 0.898300\n",
      "Epoch  661/2000 train_loss: 0.259027 acc: 0.898000\n",
      "Epoch  662/2000 train_loss: 0.259020 acc: 0.898000\n",
      "Epoch  663/2000 train_loss: 0.258997 acc: 0.898000\n",
      "Epoch  664/2000 train_loss: 0.258990 acc: 0.898100\n",
      "Epoch  665/2000 train_loss: 0.258981 acc: 0.898100\n",
      "Epoch  666/2000 train_loss: 0.258975 acc: 0.898100\n",
      "Epoch  667/2000 train_loss: 0.258958 acc: 0.898100\n",
      "Epoch  668/2000 train_loss: 0.258952 acc: 0.898200\n",
      "Epoch  669/2000 train_loss: 0.258933 acc: 0.898100\n",
      "Epoch  670/2000 train_loss: 0.258922 acc: 0.898100\n",
      "Epoch  671/2000 train_loss: 0.258924 acc: 0.898200\n",
      "Epoch  672/2000 train_loss: 0.258907 acc: 0.898000\n",
      "Epoch  673/2000 train_loss: 0.258892 acc: 0.898100\n",
      "Epoch  674/2000 train_loss: 0.258879 acc: 0.898000\n",
      "Epoch  675/2000 train_loss: 0.258879 acc: 0.898200\n",
      "Epoch  676/2000 train_loss: 0.258867 acc: 0.898000\n",
      "Epoch  677/2000 train_loss: 0.258858 acc: 0.898100\n",
      "Epoch  678/2000 train_loss: 0.258843 acc: 0.898000\n",
      "Epoch  679/2000 train_loss: 0.258830 acc: 0.898100\n",
      "Epoch  680/2000 train_loss: 0.258821 acc: 0.898000\n",
      "Epoch  681/2000 train_loss: 0.258822 acc: 0.898200\n",
      "Epoch  682/2000 train_loss: 0.258799 acc: 0.898300\n",
      "Epoch  683/2000 train_loss: 0.258793 acc: 0.898400\n",
      "Epoch  684/2000 train_loss: 0.258778 acc: 0.898300\n",
      "Epoch  685/2000 train_loss: 0.258770 acc: 0.898300\n",
      "Epoch  686/2000 train_loss: 0.258746 acc: 0.898300\n",
      "Epoch  687/2000 train_loss: 0.258751 acc: 0.897800\n",
      "Epoch  688/2000 train_loss: 0.258740 acc: 0.898200\n",
      "Epoch  689/2000 train_loss: 0.258715 acc: 0.898000\n",
      "Epoch  690/2000 train_loss: 0.258717 acc: 0.898200\n",
      "Epoch  691/2000 train_loss: 0.258709 acc: 0.897900\n",
      "Epoch  692/2000 train_loss: 0.258692 acc: 0.898500\n",
      "Epoch  693/2000 train_loss: 0.258687 acc: 0.898400\n",
      "Epoch  694/2000 train_loss: 0.258688 acc: 0.898400\n",
      "Epoch  695/2000 train_loss: 0.258659 acc: 0.898200\n",
      "Epoch  696/2000 train_loss: 0.258656 acc: 0.897800\n",
      "Epoch  697/2000 train_loss: 0.258653 acc: 0.898400\n",
      "Epoch  698/2000 train_loss: 0.258630 acc: 0.898300\n",
      "Epoch  699/2000 train_loss: 0.258618 acc: 0.898200\n",
      "Epoch  700/2000 train_loss: 0.258616 acc: 0.898100\n",
      "Epoch  701/2000 train_loss: 0.258606 acc: 0.898400\n",
      "Epoch  702/2000 train_loss: 0.258602 acc: 0.898400\n",
      "Epoch  703/2000 train_loss: 0.258588 acc: 0.898300\n",
      "Epoch  704/2000 train_loss: 0.258575 acc: 0.898100\n",
      "Epoch  705/2000 train_loss: 0.258562 acc: 0.898200\n",
      "Epoch  706/2000 train_loss: 0.258567 acc: 0.898400\n",
      "Epoch  707/2000 train_loss: 0.258545 acc: 0.898300\n",
      "Epoch  708/2000 train_loss: 0.258545 acc: 0.898400\n",
      "Epoch  709/2000 train_loss: 0.258512 acc: 0.898100\n",
      "Epoch  710/2000 train_loss: 0.258521 acc: 0.898300\n",
      "Epoch  711/2000 train_loss: 0.258497 acc: 0.898500\n",
      "Epoch  712/2000 train_loss: 0.258502 acc: 0.898400\n",
      "Epoch  713/2000 train_loss: 0.258483 acc: 0.898300\n",
      "Epoch  714/2000 train_loss: 0.258485 acc: 0.898400\n",
      "Epoch  715/2000 train_loss: 0.258452 acc: 0.899200\n",
      "Epoch  716/2000 train_loss: 0.258456 acc: 0.898800\n",
      "Epoch  717/2000 train_loss: 0.258451 acc: 0.898500\n",
      "Epoch  718/2000 train_loss: 0.258443 acc: 0.898500\n",
      "Epoch  719/2000 train_loss: 0.258433 acc: 0.898300\n",
      "Epoch  720/2000 train_loss: 0.258421 acc: 0.898400\n",
      "Epoch  721/2000 train_loss: 0.258410 acc: 0.898300\n",
      "Epoch  722/2000 train_loss: 0.258401 acc: 0.898600\n",
      "Epoch  723/2000 train_loss: 0.258392 acc: 0.898400\n",
      "Epoch  724/2000 train_loss: 0.258382 acc: 0.897900\n",
      "Epoch  725/2000 train_loss: 0.258368 acc: 0.898600\n",
      "Epoch  726/2000 train_loss: 0.258371 acc: 0.898400\n",
      "Epoch  727/2000 train_loss: 0.258343 acc: 0.898700\n",
      "Epoch  728/2000 train_loss: 0.258350 acc: 0.898200\n",
      "Epoch  729/2000 train_loss: 0.258334 acc: 0.898100\n",
      "Epoch  730/2000 train_loss: 0.258325 acc: 0.898200\n",
      "Epoch  731/2000 train_loss: 0.258311 acc: 0.898200\n",
      "Epoch  732/2000 train_loss: 0.258310 acc: 0.898400\n",
      "Epoch  733/2000 train_loss: 0.258289 acc: 0.898800\n",
      "Epoch  734/2000 train_loss: 0.258288 acc: 0.898400\n",
      "Epoch  735/2000 train_loss: 0.258284 acc: 0.898500\n",
      "Epoch  736/2000 train_loss: 0.258266 acc: 0.898000\n",
      "Epoch  737/2000 train_loss: 0.258264 acc: 0.898000\n",
      "Epoch  738/2000 train_loss: 0.258250 acc: 0.898100\n",
      "Epoch  739/2000 train_loss: 0.258221 acc: 0.898300\n",
      "Epoch  740/2000 train_loss: 0.258244 acc: 0.898300\n",
      "Epoch  741/2000 train_loss: 0.258226 acc: 0.897900\n",
      "Epoch  742/2000 train_loss: 0.258214 acc: 0.898600\n",
      "Epoch  743/2000 train_loss: 0.258207 acc: 0.898700\n",
      "Epoch  744/2000 train_loss: 0.258193 acc: 0.898600\n",
      "Epoch  745/2000 train_loss: 0.258190 acc: 0.898600\n",
      "Epoch  746/2000 train_loss: 0.258169 acc: 0.898700\n",
      "Epoch  747/2000 train_loss: 0.258176 acc: 0.899200\n",
      "Epoch  748/2000 train_loss: 0.258154 acc: 0.898600\n",
      "Epoch  749/2000 train_loss: 0.258148 acc: 0.898400\n",
      "Epoch  750/2000 train_loss: 0.258147 acc: 0.898500\n",
      "Epoch  751/2000 train_loss: 0.258135 acc: 0.898500\n",
      "Epoch  752/2000 train_loss: 0.258122 acc: 0.898400\n",
      "Epoch  753/2000 train_loss: 0.258115 acc: 0.898400\n",
      "Epoch  754/2000 train_loss: 0.258107 acc: 0.898600\n",
      "Epoch  755/2000 train_loss: 0.258101 acc: 0.898900\n",
      "Epoch  756/2000 train_loss: 0.258095 acc: 0.898600\n",
      "Epoch  757/2000 train_loss: 0.258074 acc: 0.898500\n",
      "Epoch  758/2000 train_loss: 0.258072 acc: 0.898600\n",
      "Epoch  759/2000 train_loss: 0.258067 acc: 0.898400\n",
      "Epoch  760/2000 train_loss: 0.258046 acc: 0.898200\n",
      "Epoch  761/2000 train_loss: 0.258046 acc: 0.898300\n",
      "Epoch  762/2000 train_loss: 0.258042 acc: 0.898600\n",
      "Epoch  763/2000 train_loss: 0.258019 acc: 0.898300\n",
      "Epoch  764/2000 train_loss: 0.258001 acc: 0.898600\n",
      "Epoch  765/2000 train_loss: 0.258014 acc: 0.898700\n",
      "Epoch  766/2000 train_loss: 0.258003 acc: 0.898700\n",
      "Epoch  767/2000 train_loss: 0.257985 acc: 0.898800\n",
      "Epoch  768/2000 train_loss: 0.257985 acc: 0.898600\n",
      "Epoch  769/2000 train_loss: 0.257966 acc: 0.898800\n",
      "Epoch  770/2000 train_loss: 0.257970 acc: 0.898100\n",
      "Epoch  771/2000 train_loss: 0.257951 acc: 0.898400\n",
      "Epoch  772/2000 train_loss: 0.257950 acc: 0.898700\n",
      "Epoch  773/2000 train_loss: 0.257945 acc: 0.898100\n",
      "Epoch  774/2000 train_loss: 0.257923 acc: 0.898900\n",
      "Epoch  775/2000 train_loss: 0.257925 acc: 0.898200\n",
      "Epoch  776/2000 train_loss: 0.257915 acc: 0.898700\n",
      "Epoch  777/2000 train_loss: 0.257902 acc: 0.898900\n",
      "Epoch  778/2000 train_loss: 0.257901 acc: 0.898700\n",
      "Epoch  779/2000 train_loss: 0.257866 acc: 0.898600\n",
      "Epoch  780/2000 train_loss: 0.257883 acc: 0.898300\n",
      "Epoch  781/2000 train_loss: 0.257858 acc: 0.898300\n",
      "Epoch  782/2000 train_loss: 0.257861 acc: 0.898600\n",
      "Epoch  783/2000 train_loss: 0.257858 acc: 0.898600\n",
      "Epoch  784/2000 train_loss: 0.257848 acc: 0.898700\n",
      "Epoch  785/2000 train_loss: 0.257837 acc: 0.898500\n",
      "Epoch  786/2000 train_loss: 0.257826 acc: 0.898700\n",
      "Epoch  787/2000 train_loss: 0.257827 acc: 0.898600\n",
      "Epoch  788/2000 train_loss: 0.257820 acc: 0.898900\n",
      "Epoch  789/2000 train_loss: 0.257805 acc: 0.898400\n",
      "Epoch  790/2000 train_loss: 0.257790 acc: 0.898400\n",
      "Epoch  791/2000 train_loss: 0.257784 acc: 0.898400\n",
      "Epoch  792/2000 train_loss: 0.257777 acc: 0.898600\n",
      "Epoch  793/2000 train_loss: 0.257770 acc: 0.899100\n",
      "Epoch  794/2000 train_loss: 0.257762 acc: 0.898900\n",
      "Epoch  795/2000 train_loss: 0.257752 acc: 0.899100\n",
      "Epoch  796/2000 train_loss: 0.257745 acc: 0.898800\n",
      "Epoch  797/2000 train_loss: 0.257736 acc: 0.898800\n",
      "Epoch  798/2000 train_loss: 0.257722 acc: 0.898700\n",
      "Epoch  799/2000 train_loss: 0.257719 acc: 0.898600\n",
      "Epoch  800/2000 train_loss: 0.257706 acc: 0.898900\n",
      "Epoch  801/2000 train_loss: 0.257704 acc: 0.898800\n",
      "Epoch  802/2000 train_loss: 0.257696 acc: 0.898500\n",
      "Epoch  803/2000 train_loss: 0.257691 acc: 0.898600\n",
      "Epoch  804/2000 train_loss: 0.257684 acc: 0.898600\n",
      "Epoch  805/2000 train_loss: 0.257671 acc: 0.898900\n",
      "Epoch  806/2000 train_loss: 0.257670 acc: 0.898700\n",
      "Epoch  807/2000 train_loss: 0.257650 acc: 0.898600\n",
      "Epoch  808/2000 train_loss: 0.257635 acc: 0.898800\n",
      "Epoch  809/2000 train_loss: 0.257640 acc: 0.898700\n",
      "Epoch  810/2000 train_loss: 0.257632 acc: 0.898800\n",
      "Epoch  811/2000 train_loss: 0.257621 acc: 0.898300\n",
      "Epoch  812/2000 train_loss: 0.257616 acc: 0.898700\n",
      "Epoch  813/2000 train_loss: 0.257608 acc: 0.898500\n",
      "Epoch  814/2000 train_loss: 0.257601 acc: 0.898700\n",
      "Epoch  815/2000 train_loss: 0.257588 acc: 0.898700\n",
      "Epoch  816/2000 train_loss: 0.257591 acc: 0.898700\n",
      "Epoch  817/2000 train_loss: 0.257575 acc: 0.898800\n",
      "Epoch  818/2000 train_loss: 0.257566 acc: 0.898600\n",
      "Epoch  819/2000 train_loss: 0.257562 acc: 0.898800\n",
      "Epoch  820/2000 train_loss: 0.257559 acc: 0.898700\n",
      "Epoch  821/2000 train_loss: 0.257539 acc: 0.898600\n",
      "Epoch  822/2000 train_loss: 0.257534 acc: 0.898300\n",
      "Epoch  823/2000 train_loss: 0.257531 acc: 0.898500\n",
      "Epoch  824/2000 train_loss: 0.257527 acc: 0.898600\n",
      "Epoch  825/2000 train_loss: 0.257512 acc: 0.898700\n",
      "Epoch  826/2000 train_loss: 0.257502 acc: 0.899100\n",
      "Epoch  827/2000 train_loss: 0.257490 acc: 0.898600\n",
      "Epoch  828/2000 train_loss: 0.257493 acc: 0.898700\n",
      "Epoch  829/2000 train_loss: 0.257479 acc: 0.898700\n",
      "Epoch  830/2000 train_loss: 0.257479 acc: 0.898300\n",
      "Epoch  831/2000 train_loss: 0.257466 acc: 0.898600\n",
      "Epoch  832/2000 train_loss: 0.257460 acc: 0.898000\n",
      "Epoch  833/2000 train_loss: 0.257448 acc: 0.898500\n",
      "Epoch  834/2000 train_loss: 0.257438 acc: 0.898500\n",
      "Epoch  835/2000 train_loss: 0.257448 acc: 0.898500\n",
      "Epoch  836/2000 train_loss: 0.257437 acc: 0.898400\n",
      "Epoch  837/2000 train_loss: 0.257417 acc: 0.898600\n",
      "Epoch  838/2000 train_loss: 0.257414 acc: 0.898800\n",
      "Epoch  839/2000 train_loss: 0.257403 acc: 0.899000\n",
      "Epoch  840/2000 train_loss: 0.257392 acc: 0.898500\n",
      "Epoch  841/2000 train_loss: 0.257394 acc: 0.898800\n",
      "Epoch  842/2000 train_loss: 0.257384 acc: 0.898300\n",
      "Epoch  843/2000 train_loss: 0.257379 acc: 0.898300\n",
      "Epoch  844/2000 train_loss: 0.257373 acc: 0.898500\n",
      "Epoch  845/2000 train_loss: 0.257362 acc: 0.898300\n",
      "Epoch  846/2000 train_loss: 0.257354 acc: 0.898300\n",
      "Epoch  847/2000 train_loss: 0.257350 acc: 0.898700\n",
      "Epoch  848/2000 train_loss: 0.257314 acc: 0.898700\n",
      "Epoch  849/2000 train_loss: 0.257330 acc: 0.898900\n",
      "Epoch  850/2000 train_loss: 0.257316 acc: 0.899100\n",
      "Epoch  851/2000 train_loss: 0.257320 acc: 0.898500\n",
      "Epoch  852/2000 train_loss: 0.257292 acc: 0.898700\n",
      "Epoch  853/2000 train_loss: 0.257307 acc: 0.898700\n",
      "Epoch  854/2000 train_loss: 0.257292 acc: 0.898400\n",
      "Epoch  855/2000 train_loss: 0.257292 acc: 0.898400\n",
      "Epoch  856/2000 train_loss: 0.257268 acc: 0.898600\n",
      "Epoch  857/2000 train_loss: 0.257277 acc: 0.898700\n",
      "Epoch  858/2000 train_loss: 0.257262 acc: 0.898700\n",
      "Epoch  859/2000 train_loss: 0.257254 acc: 0.898800\n",
      "Epoch  860/2000 train_loss: 0.257260 acc: 0.898600\n",
      "Epoch  861/2000 train_loss: 0.257237 acc: 0.898600\n",
      "Epoch  862/2000 train_loss: 0.257234 acc: 0.898800\n",
      "Epoch  863/2000 train_loss: 0.257233 acc: 0.898900\n",
      "Epoch  864/2000 train_loss: 0.257221 acc: 0.898900\n",
      "Epoch  865/2000 train_loss: 0.257209 acc: 0.898500\n",
      "Epoch  866/2000 train_loss: 0.257207 acc: 0.898600\n",
      "Epoch  867/2000 train_loss: 0.257199 acc: 0.898400\n",
      "Epoch  868/2000 train_loss: 0.257188 acc: 0.898600\n",
      "Epoch  869/2000 train_loss: 0.257190 acc: 0.898600\n",
      "Epoch  870/2000 train_loss: 0.257172 acc: 0.898700\n",
      "Epoch  871/2000 train_loss: 0.257170 acc: 0.898800\n",
      "Epoch  872/2000 train_loss: 0.257167 acc: 0.898900\n",
      "Epoch  873/2000 train_loss: 0.257160 acc: 0.898600\n",
      "Epoch  874/2000 train_loss: 0.257146 acc: 0.898400\n",
      "Epoch  875/2000 train_loss: 0.257142 acc: 0.898600\n",
      "Epoch  876/2000 train_loss: 0.257134 acc: 0.898500\n",
      "Epoch  877/2000 train_loss: 0.257124 acc: 0.899200\n",
      "Epoch  878/2000 train_loss: 0.257124 acc: 0.898600\n",
      "Epoch  879/2000 train_loss: 0.257121 acc: 0.899000\n",
      "Epoch  880/2000 train_loss: 0.257105 acc: 0.898800\n",
      "Epoch  881/2000 train_loss: 0.257097 acc: 0.899100\n",
      "Epoch  882/2000 train_loss: 0.257093 acc: 0.898700\n",
      "Epoch  883/2000 train_loss: 0.257079 acc: 0.898900\n",
      "Epoch  884/2000 train_loss: 0.257076 acc: 0.898600\n",
      "Epoch  885/2000 train_loss: 0.257070 acc: 0.898700\n",
      "Epoch  886/2000 train_loss: 0.257050 acc: 0.898800\n",
      "Epoch  887/2000 train_loss: 0.257063 acc: 0.898800\n",
      "Epoch  888/2000 train_loss: 0.257051 acc: 0.899000\n",
      "Epoch  889/2000 train_loss: 0.257041 acc: 0.898500\n",
      "Epoch  890/2000 train_loss: 0.257037 acc: 0.898600\n",
      "Epoch  891/2000 train_loss: 0.257019 acc: 0.898300\n",
      "Epoch  892/2000 train_loss: 0.257028 acc: 0.898900\n",
      "Epoch  893/2000 train_loss: 0.257011 acc: 0.898900\n",
      "Epoch  894/2000 train_loss: 0.257008 acc: 0.898500\n",
      "Epoch  895/2000 train_loss: 0.256991 acc: 0.898600\n",
      "Epoch  896/2000 train_loss: 0.256994 acc: 0.899000\n",
      "Epoch  897/2000 train_loss: 0.256987 acc: 0.898800\n",
      "Epoch  898/2000 train_loss: 0.256956 acc: 0.899100\n",
      "Epoch  899/2000 train_loss: 0.256985 acc: 0.899100\n",
      "Epoch  900/2000 train_loss: 0.256968 acc: 0.898400\n",
      "Epoch  901/2000 train_loss: 0.256951 acc: 0.898600\n",
      "Epoch  902/2000 train_loss: 0.256948 acc: 0.898900\n",
      "Epoch  903/2000 train_loss: 0.256942 acc: 0.898800\n",
      "Epoch  904/2000 train_loss: 0.256943 acc: 0.898900\n",
      "Epoch  905/2000 train_loss: 0.256936 acc: 0.898800\n",
      "Epoch  906/2000 train_loss: 0.256932 acc: 0.898800\n",
      "Epoch  907/2000 train_loss: 0.256899 acc: 0.898800\n",
      "Epoch  908/2000 train_loss: 0.256912 acc: 0.898700\n",
      "Epoch  909/2000 train_loss: 0.256896 acc: 0.898800\n",
      "Epoch  910/2000 train_loss: 0.256901 acc: 0.898700\n",
      "Epoch  911/2000 train_loss: 0.256893 acc: 0.898900\n",
      "Epoch  912/2000 train_loss: 0.256893 acc: 0.899000\n",
      "Epoch  913/2000 train_loss: 0.256870 acc: 0.898700\n",
      "Epoch  914/2000 train_loss: 0.256871 acc: 0.898800\n",
      "Epoch  915/2000 train_loss: 0.256872 acc: 0.899100\n",
      "Epoch  916/2000 train_loss: 0.256864 acc: 0.899100\n",
      "Epoch  917/2000 train_loss: 0.256860 acc: 0.898800\n",
      "Epoch  918/2000 train_loss: 0.256842 acc: 0.899200\n",
      "Epoch  919/2000 train_loss: 0.256842 acc: 0.898600\n",
      "Epoch  920/2000 train_loss: 0.256829 acc: 0.899400\n",
      "Epoch  921/2000 train_loss: 0.256827 acc: 0.898900\n",
      "Epoch  922/2000 train_loss: 0.256817 acc: 0.899000\n",
      "Epoch  923/2000 train_loss: 0.256815 acc: 0.899300\n",
      "Epoch  924/2000 train_loss: 0.256808 acc: 0.898700\n",
      "Epoch  925/2000 train_loss: 0.256797 acc: 0.899500\n",
      "Epoch  926/2000 train_loss: 0.256797 acc: 0.899100\n",
      "Epoch  927/2000 train_loss: 0.256780 acc: 0.898500\n",
      "Epoch  928/2000 train_loss: 0.256780 acc: 0.899300\n",
      "Epoch  929/2000 train_loss: 0.256773 acc: 0.898700\n",
      "Epoch  930/2000 train_loss: 0.256772 acc: 0.899000\n",
      "Epoch  931/2000 train_loss: 0.256755 acc: 0.899400\n",
      "Epoch  932/2000 train_loss: 0.256743 acc: 0.899500\n",
      "Epoch  933/2000 train_loss: 0.256731 acc: 0.899500\n",
      "Epoch  934/2000 train_loss: 0.256743 acc: 0.898700\n",
      "Epoch  935/2000 train_loss: 0.256738 acc: 0.899100\n",
      "Epoch  936/2000 train_loss: 0.256724 acc: 0.898900\n",
      "Epoch  937/2000 train_loss: 0.256733 acc: 0.898800\n",
      "Epoch  938/2000 train_loss: 0.256713 acc: 0.898900\n",
      "Epoch  939/2000 train_loss: 0.256710 acc: 0.898900\n",
      "Epoch  940/2000 train_loss: 0.256702 acc: 0.898700\n",
      "Epoch  941/2000 train_loss: 0.256696 acc: 0.898900\n",
      "Epoch  942/2000 train_loss: 0.256682 acc: 0.898700\n",
      "Epoch  943/2000 train_loss: 0.256682 acc: 0.898900\n",
      "Epoch  944/2000 train_loss: 0.256681 acc: 0.898900\n",
      "Epoch  945/2000 train_loss: 0.256681 acc: 0.898600\n",
      "Epoch  946/2000 train_loss: 0.256658 acc: 0.898500\n",
      "Epoch  947/2000 train_loss: 0.256661 acc: 0.899000\n",
      "Epoch  948/2000 train_loss: 0.256647 acc: 0.899000\n",
      "Epoch  949/2000 train_loss: 0.256648 acc: 0.899000\n",
      "Epoch  950/2000 train_loss: 0.256627 acc: 0.898600\n",
      "Epoch  951/2000 train_loss: 0.256640 acc: 0.898600\n",
      "Epoch  952/2000 train_loss: 0.256627 acc: 0.899000\n",
      "Epoch  953/2000 train_loss: 0.256624 acc: 0.899100\n",
      "Epoch  954/2000 train_loss: 0.256607 acc: 0.899400\n",
      "Epoch  955/2000 train_loss: 0.256613 acc: 0.898800\n",
      "Epoch  956/2000 train_loss: 0.256597 acc: 0.899200\n",
      "Epoch  957/2000 train_loss: 0.256566 acc: 0.899500\n",
      "Epoch  958/2000 train_loss: 0.256597 acc: 0.898600\n",
      "Epoch  959/2000 train_loss: 0.256577 acc: 0.899400\n",
      "Epoch  960/2000 train_loss: 0.256572 acc: 0.898800\n",
      "Epoch  961/2000 train_loss: 0.256571 acc: 0.899100\n",
      "Epoch  962/2000 train_loss: 0.256564 acc: 0.899100\n",
      "Epoch  963/2000 train_loss: 0.256563 acc: 0.899000\n",
      "Epoch  964/2000 train_loss: 0.256547 acc: 0.898700\n",
      "Epoch  965/2000 train_loss: 0.256551 acc: 0.899000\n",
      "Epoch  966/2000 train_loss: 0.256541 acc: 0.899400\n",
      "Epoch  967/2000 train_loss: 0.256532 acc: 0.899500\n",
      "Epoch  968/2000 train_loss: 0.256531 acc: 0.898800\n",
      "Epoch  969/2000 train_loss: 0.256524 acc: 0.899400\n",
      "Epoch  970/2000 train_loss: 0.256511 acc: 0.899200\n",
      "Epoch  971/2000 train_loss: 0.256517 acc: 0.898900\n",
      "Epoch  972/2000 train_loss: 0.256508 acc: 0.899000\n",
      "Epoch  973/2000 train_loss: 0.256502 acc: 0.898900\n",
      "Epoch  974/2000 train_loss: 0.256491 acc: 0.898900\n",
      "Epoch  975/2000 train_loss: 0.256483 acc: 0.899100\n",
      "Epoch  976/2000 train_loss: 0.256474 acc: 0.898900\n",
      "Epoch  977/2000 train_loss: 0.256474 acc: 0.898900\n",
      "Epoch  978/2000 train_loss: 0.256474 acc: 0.899400\n",
      "Epoch  979/2000 train_loss: 0.256463 acc: 0.898700\n",
      "Epoch  980/2000 train_loss: 0.256455 acc: 0.899000\n",
      "Epoch  981/2000 train_loss: 0.256455 acc: 0.899100\n",
      "Epoch  982/2000 train_loss: 0.256443 acc: 0.899000\n",
      "Epoch  983/2000 train_loss: 0.256438 acc: 0.899300\n",
      "Epoch  984/2000 train_loss: 0.256431 acc: 0.899000\n",
      "Epoch  985/2000 train_loss: 0.256427 acc: 0.899100\n",
      "Epoch  986/2000 train_loss: 0.256417 acc: 0.898800\n",
      "Epoch  987/2000 train_loss: 0.256421 acc: 0.899200\n",
      "Epoch  988/2000 train_loss: 0.256404 acc: 0.899400\n",
      "Epoch  989/2000 train_loss: 0.256403 acc: 0.899500\n",
      "Epoch  990/2000 train_loss: 0.256403 acc: 0.899100\n",
      "Epoch  991/2000 train_loss: 0.256391 acc: 0.899100\n",
      "Epoch  992/2000 train_loss: 0.256384 acc: 0.899400\n",
      "Epoch  993/2000 train_loss: 0.256376 acc: 0.898700\n",
      "Epoch  994/2000 train_loss: 0.256371 acc: 0.899400\n",
      "Epoch  995/2000 train_loss: 0.256360 acc: 0.899100\n",
      "Epoch  996/2000 train_loss: 0.256365 acc: 0.899600\n",
      "Epoch  997/2000 train_loss: 0.256352 acc: 0.899400\n",
      "Epoch  998/2000 train_loss: 0.256353 acc: 0.899200\n",
      "Epoch  999/2000 train_loss: 0.256343 acc: 0.899100\n",
      "Epoch 1000/2000 train_loss: 0.256340 acc: 0.899200\n",
      "Epoch 1001/2000 train_loss: 0.256332 acc: 0.899500\n",
      "Epoch 1002/2000 train_loss: 0.256330 acc: 0.898900\n",
      "Epoch 1003/2000 train_loss: 0.256319 acc: 0.899500\n",
      "Epoch 1004/2000 train_loss: 0.256320 acc: 0.899400\n",
      "Epoch 1005/2000 train_loss: 0.256305 acc: 0.899000\n",
      "Epoch 1006/2000 train_loss: 0.256298 acc: 0.899400\n",
      "Epoch 1007/2000 train_loss: 0.256299 acc: 0.899200\n",
      "Epoch 1008/2000 train_loss: 0.256275 acc: 0.899000\n",
      "Epoch 1009/2000 train_loss: 0.256287 acc: 0.899700\n",
      "Epoch 1010/2000 train_loss: 0.256274 acc: 0.898900\n",
      "Epoch 1011/2000 train_loss: 0.256273 acc: 0.899300\n",
      "Epoch 1012/2000 train_loss: 0.256261 acc: 0.899400\n",
      "Epoch 1013/2000 train_loss: 0.256257 acc: 0.899000\n",
      "Epoch 1014/2000 train_loss: 0.256259 acc: 0.899200\n",
      "Epoch 1015/2000 train_loss: 0.256246 acc: 0.899600\n",
      "Epoch 1016/2000 train_loss: 0.256244 acc: 0.899400\n",
      "Epoch 1017/2000 train_loss: 0.256241 acc: 0.899400\n",
      "Epoch 1018/2000 train_loss: 0.256243 acc: 0.899600\n",
      "Epoch 1019/2000 train_loss: 0.256232 acc: 0.899100\n",
      "Epoch 1020/2000 train_loss: 0.256224 acc: 0.899100\n",
      "Epoch 1021/2000 train_loss: 0.256219 acc: 0.899000\n",
      "Epoch 1022/2000 train_loss: 0.256209 acc: 0.898900\n",
      "Epoch 1023/2000 train_loss: 0.256208 acc: 0.899200\n",
      "Epoch 1024/2000 train_loss: 0.256192 acc: 0.899000\n",
      "Epoch 1025/2000 train_loss: 0.256202 acc: 0.899500\n",
      "Epoch 1026/2000 train_loss: 0.256197 acc: 0.899400\n",
      "Epoch 1027/2000 train_loss: 0.256185 acc: 0.899100\n",
      "Epoch 1028/2000 train_loss: 0.256186 acc: 0.899100\n",
      "Epoch 1029/2000 train_loss: 0.256171 acc: 0.899100\n",
      "Epoch 1030/2000 train_loss: 0.256168 acc: 0.899200\n",
      "Epoch 1031/2000 train_loss: 0.256161 acc: 0.899400\n",
      "Epoch 1032/2000 train_loss: 0.256153 acc: 0.899300\n",
      "Epoch 1033/2000 train_loss: 0.256153 acc: 0.900000\n",
      "Epoch 1034/2000 train_loss: 0.256135 acc: 0.899100\n",
      "Epoch 1035/2000 train_loss: 0.256134 acc: 0.898700\n",
      "Epoch 1036/2000 train_loss: 0.256145 acc: 0.899500\n",
      "Epoch 1037/2000 train_loss: 0.256130 acc: 0.899600\n",
      "Epoch 1038/2000 train_loss: 0.256123 acc: 0.899300\n",
      "Epoch 1039/2000 train_loss: 0.256113 acc: 0.899200\n",
      "Epoch 1040/2000 train_loss: 0.256114 acc: 0.899700\n",
      "Epoch 1041/2000 train_loss: 0.256113 acc: 0.899300\n",
      "Epoch 1042/2000 train_loss: 0.256103 acc: 0.899500\n",
      "Epoch 1043/2000 train_loss: 0.256092 acc: 0.899200\n",
      "Epoch 1044/2000 train_loss: 0.256094 acc: 0.899400\n",
      "Epoch 1045/2000 train_loss: 0.256091 acc: 0.899700\n",
      "Epoch 1046/2000 train_loss: 0.256085 acc: 0.899000\n",
      "Epoch 1047/2000 train_loss: 0.256056 acc: 0.899600\n",
      "Epoch 1048/2000 train_loss: 0.256076 acc: 0.899500\n",
      "Epoch 1049/2000 train_loss: 0.256060 acc: 0.899700\n",
      "Epoch 1050/2000 train_loss: 0.256059 acc: 0.899200\n",
      "Epoch 1051/2000 train_loss: 0.256052 acc: 0.899700\n",
      "Epoch 1052/2000 train_loss: 0.256034 acc: 0.899100\n",
      "Epoch 1053/2000 train_loss: 0.256051 acc: 0.899300\n",
      "Epoch 1054/2000 train_loss: 0.256022 acc: 0.898900\n",
      "Epoch 1055/2000 train_loss: 0.256034 acc: 0.899300\n",
      "Epoch 1056/2000 train_loss: 0.256018 acc: 0.899500\n",
      "Epoch 1057/2000 train_loss: 0.256034 acc: 0.899400\n",
      "Epoch 1058/2000 train_loss: 0.256016 acc: 0.899800\n",
      "Epoch 1059/2000 train_loss: 0.256007 acc: 0.899500\n",
      "Epoch 1060/2000 train_loss: 0.256005 acc: 0.899800\n",
      "Epoch 1061/2000 train_loss: 0.256001 acc: 0.899700\n",
      "Epoch 1062/2000 train_loss: 0.255997 acc: 0.899600\n",
      "Epoch 1063/2000 train_loss: 0.255997 acc: 0.899400\n",
      "Epoch 1064/2000 train_loss: 0.255988 acc: 0.899300\n",
      "Epoch 1065/2000 train_loss: 0.255982 acc: 0.899200\n",
      "Epoch 1066/2000 train_loss: 0.255977 acc: 0.899400\n",
      "Epoch 1067/2000 train_loss: 0.255967 acc: 0.899300\n",
      "Epoch 1068/2000 train_loss: 0.255956 acc: 0.899700\n",
      "Epoch 1069/2000 train_loss: 0.255957 acc: 0.899700\n",
      "Epoch 1070/2000 train_loss: 0.255954 acc: 0.899700\n",
      "Epoch 1071/2000 train_loss: 0.255950 acc: 0.899400\n",
      "Epoch 1072/2000 train_loss: 0.255942 acc: 0.899300\n",
      "Epoch 1073/2000 train_loss: 0.255934 acc: 0.899300\n",
      "Epoch 1074/2000 train_loss: 0.255936 acc: 0.900000\n",
      "Epoch 1075/2000 train_loss: 0.255905 acc: 0.898700\n",
      "Epoch 1076/2000 train_loss: 0.255932 acc: 0.899600\n",
      "Epoch 1077/2000 train_loss: 0.255917 acc: 0.899800\n",
      "Epoch 1078/2000 train_loss: 0.255913 acc: 0.899500\n",
      "Epoch 1079/2000 train_loss: 0.255910 acc: 0.899600\n",
      "Epoch 1080/2000 train_loss: 0.255902 acc: 0.899500\n",
      "Epoch 1081/2000 train_loss: 0.255902 acc: 0.899100\n",
      "Epoch 1082/2000 train_loss: 0.255895 acc: 0.899900\n",
      "Epoch 1083/2000 train_loss: 0.255886 acc: 0.899600\n",
      "Epoch 1084/2000 train_loss: 0.255878 acc: 0.899400\n",
      "Epoch 1085/2000 train_loss: 0.255866 acc: 0.899100\n",
      "Epoch 1086/2000 train_loss: 0.255866 acc: 0.899200\n",
      "Epoch 1087/2000 train_loss: 0.255866 acc: 0.899500\n",
      "Epoch 1088/2000 train_loss: 0.255867 acc: 0.899800\n",
      "Epoch 1089/2000 train_loss: 0.255862 acc: 0.899600\n",
      "Epoch 1090/2000 train_loss: 0.255841 acc: 0.899400\n",
      "Epoch 1091/2000 train_loss: 0.255851 acc: 0.899700\n",
      "Epoch 1092/2000 train_loss: 0.255833 acc: 0.899000\n",
      "Epoch 1093/2000 train_loss: 0.255832 acc: 0.899300\n",
      "Epoch 1094/2000 train_loss: 0.255832 acc: 0.899300\n",
      "Epoch 1095/2000 train_loss: 0.255820 acc: 0.899600\n",
      "Epoch 1096/2000 train_loss: 0.255821 acc: 0.899600\n",
      "Epoch 1097/2000 train_loss: 0.255814 acc: 0.899800\n",
      "Epoch 1098/2000 train_loss: 0.255806 acc: 0.899500\n",
      "Epoch 1099/2000 train_loss: 0.255808 acc: 0.899700\n",
      "Epoch 1100/2000 train_loss: 0.255801 acc: 0.899300\n",
      "Epoch 1101/2000 train_loss: 0.255799 acc: 0.899600\n",
      "Epoch 1102/2000 train_loss: 0.255790 acc: 0.899800\n",
      "Epoch 1103/2000 train_loss: 0.255789 acc: 0.899500\n",
      "Epoch 1104/2000 train_loss: 0.255775 acc: 0.900000\n",
      "Epoch 1105/2000 train_loss: 0.255773 acc: 0.899400\n",
      "Epoch 1106/2000 train_loss: 0.255766 acc: 0.899900\n",
      "Epoch 1107/2000 train_loss: 0.255761 acc: 0.899600\n",
      "Epoch 1108/2000 train_loss: 0.255758 acc: 0.899600\n",
      "Epoch 1109/2000 train_loss: 0.255753 acc: 0.899700\n",
      "Epoch 1110/2000 train_loss: 0.255749 acc: 0.900000\n",
      "Epoch 1111/2000 train_loss: 0.255737 acc: 0.899900\n",
      "Epoch 1112/2000 train_loss: 0.255740 acc: 0.899800\n",
      "Epoch 1113/2000 train_loss: 0.255740 acc: 0.899800\n",
      "Epoch 1114/2000 train_loss: 0.255725 acc: 0.899800\n",
      "Epoch 1115/2000 train_loss: 0.255730 acc: 0.899600\n",
      "Epoch 1116/2000 train_loss: 0.255721 acc: 0.899800\n",
      "Epoch 1117/2000 train_loss: 0.255712 acc: 0.899600\n",
      "Epoch 1118/2000 train_loss: 0.255716 acc: 0.899600\n",
      "Epoch 1119/2000 train_loss: 0.255708 acc: 0.899600\n",
      "Epoch 1120/2000 train_loss: 0.255696 acc: 0.899600\n",
      "Epoch 1121/2000 train_loss: 0.255694 acc: 0.899900\n",
      "Epoch 1122/2000 train_loss: 0.255692 acc: 0.899200\n",
      "Epoch 1123/2000 train_loss: 0.255687 acc: 0.900000\n",
      "Epoch 1124/2000 train_loss: 0.255678 acc: 0.899600\n",
      "Epoch 1125/2000 train_loss: 0.255674 acc: 0.899600\n",
      "Epoch 1126/2000 train_loss: 0.255671 acc: 0.899800\n",
      "Epoch 1127/2000 train_loss: 0.255666 acc: 0.899200\n",
      "Epoch 1128/2000 train_loss: 0.255657 acc: 0.899600\n",
      "Epoch 1129/2000 train_loss: 0.255651 acc: 0.899800\n",
      "Epoch 1130/2000 train_loss: 0.255643 acc: 0.898900\n",
      "Epoch 1131/2000 train_loss: 0.255654 acc: 0.899300\n",
      "Epoch 1132/2000 train_loss: 0.255644 acc: 0.899600\n",
      "Epoch 1133/2000 train_loss: 0.255614 acc: 0.899200\n",
      "Epoch 1134/2000 train_loss: 0.255634 acc: 0.899300\n",
      "Epoch 1135/2000 train_loss: 0.255619 acc: 0.899600\n",
      "Epoch 1136/2000 train_loss: 0.255622 acc: 0.899500\n",
      "Epoch 1137/2000 train_loss: 0.255619 acc: 0.900100\n",
      "Epoch 1138/2000 train_loss: 0.255616 acc: 0.899700\n",
      "Epoch 1139/2000 train_loss: 0.255610 acc: 0.899700\n",
      "Epoch 1140/2000 train_loss: 0.255604 acc: 0.899500\n",
      "Epoch 1141/2000 train_loss: 0.255596 acc: 0.899400\n",
      "Epoch 1142/2000 train_loss: 0.255591 acc: 0.899800\n",
      "Epoch 1143/2000 train_loss: 0.255600 acc: 0.899600\n",
      "Epoch 1144/2000 train_loss: 0.255584 acc: 0.899700\n",
      "Epoch 1145/2000 train_loss: 0.255575 acc: 0.899400\n",
      "Epoch 1146/2000 train_loss: 0.255580 acc: 0.899700\n",
      "Epoch 1147/2000 train_loss: 0.255572 acc: 0.899900\n",
      "Epoch 1148/2000 train_loss: 0.255568 acc: 0.899700\n",
      "Epoch 1149/2000 train_loss: 0.255560 acc: 0.899600\n",
      "Epoch 1150/2000 train_loss: 0.255563 acc: 0.899900\n",
      "Epoch 1151/2000 train_loss: 0.255551 acc: 0.899800\n",
      "Epoch 1152/2000 train_loss: 0.255538 acc: 0.899600\n",
      "Epoch 1153/2000 train_loss: 0.255545 acc: 0.899600\n",
      "Epoch 1154/2000 train_loss: 0.255537 acc: 0.899700\n",
      "Epoch 1155/2000 train_loss: 0.255544 acc: 0.899700\n",
      "Epoch 1156/2000 train_loss: 0.255524 acc: 0.899700\n",
      "Epoch 1157/2000 train_loss: 0.255520 acc: 0.899800\n",
      "Epoch 1158/2000 train_loss: 0.255525 acc: 0.899400\n",
      "Epoch 1159/2000 train_loss: 0.255504 acc: 0.899500\n",
      "Epoch 1160/2000 train_loss: 0.255512 acc: 0.899600\n",
      "Epoch 1161/2000 train_loss: 0.255496 acc: 0.899800\n",
      "Epoch 1162/2000 train_loss: 0.255505 acc: 0.899800\n",
      "Epoch 1163/2000 train_loss: 0.255499 acc: 0.899700\n",
      "Epoch 1164/2000 train_loss: 0.255490 acc: 0.899700\n",
      "Epoch 1165/2000 train_loss: 0.255486 acc: 0.899600\n",
      "Epoch 1166/2000 train_loss: 0.255478 acc: 0.899600\n",
      "Epoch 1167/2000 train_loss: 0.255475 acc: 0.899500\n",
      "Epoch 1168/2000 train_loss: 0.255482 acc: 0.900000\n",
      "Epoch 1169/2000 train_loss: 0.255468 acc: 0.899700\n",
      "Epoch 1170/2000 train_loss: 0.255469 acc: 0.899500\n",
      "Epoch 1171/2000 train_loss: 0.255452 acc: 0.900000\n",
      "Epoch 1172/2000 train_loss: 0.255462 acc: 0.899600\n",
      "Epoch 1173/2000 train_loss: 0.255451 acc: 0.899600\n",
      "Epoch 1174/2000 train_loss: 0.255449 acc: 0.899800\n",
      "Epoch 1175/2000 train_loss: 0.255443 acc: 0.899900\n",
      "Epoch 1176/2000 train_loss: 0.255435 acc: 0.899600\n",
      "Epoch 1177/2000 train_loss: 0.255431 acc: 0.899400\n",
      "Epoch 1178/2000 train_loss: 0.255429 acc: 0.900100\n",
      "Epoch 1179/2000 train_loss: 0.255411 acc: 0.899600\n",
      "Epoch 1180/2000 train_loss: 0.255424 acc: 0.899500\n",
      "Epoch 1181/2000 train_loss: 0.255415 acc: 0.899700\n",
      "Epoch 1182/2000 train_loss: 0.255408 acc: 0.899700\n",
      "Epoch 1183/2000 train_loss: 0.255405 acc: 0.899800\n",
      "Epoch 1184/2000 train_loss: 0.255407 acc: 0.899900\n",
      "Epoch 1185/2000 train_loss: 0.255402 acc: 0.899900\n",
      "Epoch 1186/2000 train_loss: 0.255402 acc: 0.899700\n",
      "Epoch 1187/2000 train_loss: 0.255386 acc: 0.899700\n",
      "Epoch 1188/2000 train_loss: 0.255379 acc: 0.899800\n",
      "Epoch 1189/2000 train_loss: 0.255381 acc: 0.899800\n",
      "Epoch 1190/2000 train_loss: 0.255369 acc: 0.899800\n",
      "Epoch 1191/2000 train_loss: 0.255370 acc: 0.899900\n",
      "Epoch 1192/2000 train_loss: 0.255359 acc: 0.900100\n",
      "Epoch 1193/2000 train_loss: 0.255359 acc: 0.899400\n",
      "Epoch 1194/2000 train_loss: 0.255358 acc: 0.899700\n",
      "Epoch 1195/2000 train_loss: 0.255354 acc: 0.900000\n",
      "Epoch 1196/2000 train_loss: 0.255351 acc: 0.899600\n",
      "Epoch 1197/2000 train_loss: 0.255342 acc: 0.900200\n",
      "Epoch 1198/2000 train_loss: 0.255340 acc: 0.899700\n",
      "Epoch 1199/2000 train_loss: 0.255328 acc: 0.899400\n",
      "Epoch 1200/2000 train_loss: 0.255331 acc: 0.899800\n",
      "Epoch 1201/2000 train_loss: 0.255330 acc: 0.899600\n",
      "Epoch 1202/2000 train_loss: 0.255318 acc: 0.899600\n",
      "Epoch 1203/2000 train_loss: 0.255316 acc: 0.899500\n",
      "Epoch 1204/2000 train_loss: 0.255299 acc: 0.899700\n",
      "Epoch 1205/2000 train_loss: 0.255308 acc: 0.900000\n",
      "Epoch 1206/2000 train_loss: 0.255309 acc: 0.899700\n",
      "Epoch 1207/2000 train_loss: 0.255303 acc: 0.899500\n",
      "Epoch 1208/2000 train_loss: 0.255298 acc: 0.899800\n",
      "Epoch 1209/2000 train_loss: 0.255288 acc: 0.899800\n",
      "Epoch 1210/2000 train_loss: 0.255280 acc: 0.899800\n",
      "Epoch 1211/2000 train_loss: 0.255284 acc: 0.899900\n",
      "Epoch 1212/2000 train_loss: 0.255261 acc: 0.899400\n",
      "Epoch 1213/2000 train_loss: 0.255276 acc: 0.899800\n",
      "Epoch 1214/2000 train_loss: 0.255265 acc: 0.899900\n",
      "Epoch 1215/2000 train_loss: 0.255265 acc: 0.899900\n",
      "Epoch 1216/2000 train_loss: 0.255244 acc: 0.899600\n",
      "Epoch 1217/2000 train_loss: 0.255269 acc: 0.899800\n",
      "Epoch 1218/2000 train_loss: 0.255252 acc: 0.899900\n",
      "Epoch 1219/2000 train_loss: 0.255251 acc: 0.899800\n",
      "Epoch 1220/2000 train_loss: 0.255244 acc: 0.900000\n",
      "Epoch 1221/2000 train_loss: 0.255241 acc: 0.899900\n",
      "Epoch 1222/2000 train_loss: 0.255237 acc: 0.900000\n",
      "Epoch 1223/2000 train_loss: 0.255232 acc: 0.899600\n",
      "Epoch 1224/2000 train_loss: 0.255228 acc: 0.899600\n",
      "Epoch 1225/2000 train_loss: 0.255224 acc: 0.899800\n",
      "Epoch 1226/2000 train_loss: 0.255216 acc: 0.899900\n",
      "Epoch 1227/2000 train_loss: 0.255216 acc: 0.899700\n",
      "Epoch 1228/2000 train_loss: 0.255217 acc: 0.899900\n",
      "Epoch 1229/2000 train_loss: 0.255205 acc: 0.899700\n",
      "Epoch 1230/2000 train_loss: 0.255201 acc: 0.899700\n",
      "Epoch 1231/2000 train_loss: 0.255200 acc: 0.900000\n",
      "Epoch 1232/2000 train_loss: 0.255182 acc: 0.899900\n",
      "Epoch 1233/2000 train_loss: 0.255196 acc: 0.899800\n",
      "Epoch 1234/2000 train_loss: 0.255185 acc: 0.899800\n",
      "Epoch 1235/2000 train_loss: 0.255176 acc: 0.899700\n",
      "Epoch 1236/2000 train_loss: 0.255180 acc: 0.899300\n",
      "Epoch 1237/2000 train_loss: 0.255171 acc: 0.899900\n",
      "Epoch 1238/2000 train_loss: 0.255171 acc: 0.900000\n",
      "Epoch 1239/2000 train_loss: 0.255165 acc: 0.899700\n",
      "Epoch 1240/2000 train_loss: 0.255163 acc: 0.900000\n",
      "Epoch 1241/2000 train_loss: 0.255156 acc: 0.899500\n",
      "Epoch 1242/2000 train_loss: 0.255150 acc: 0.899100\n",
      "Epoch 1243/2000 train_loss: 0.255148 acc: 0.899900\n",
      "Epoch 1244/2000 train_loss: 0.255148 acc: 0.899600\n",
      "Epoch 1245/2000 train_loss: 0.255138 acc: 0.899600\n",
      "Epoch 1246/2000 train_loss: 0.255134 acc: 0.899900\n",
      "Epoch 1247/2000 train_loss: 0.255137 acc: 0.899400\n",
      "Epoch 1248/2000 train_loss: 0.255129 acc: 0.900000\n",
      "Epoch 1249/2000 train_loss: 0.255127 acc: 0.899600\n",
      "Epoch 1250/2000 train_loss: 0.255119 acc: 0.899800\n",
      "Epoch 1251/2000 train_loss: 0.255121 acc: 0.899800\n",
      "Epoch 1252/2000 train_loss: 0.255116 acc: 0.899900\n",
      "Epoch 1253/2000 train_loss: 0.255106 acc: 0.899900\n",
      "Epoch 1254/2000 train_loss: 0.255102 acc: 0.899500\n",
      "Epoch 1255/2000 train_loss: 0.255101 acc: 0.899600\n",
      "Epoch 1256/2000 train_loss: 0.255096 acc: 0.899500\n",
      "Epoch 1257/2000 train_loss: 0.255094 acc: 0.899900\n",
      "Epoch 1258/2000 train_loss: 0.255069 acc: 0.899700\n",
      "Epoch 1259/2000 train_loss: 0.255089 acc: 0.899600\n",
      "Epoch 1260/2000 train_loss: 0.255073 acc: 0.899500\n",
      "Epoch 1261/2000 train_loss: 0.255078 acc: 0.899600\n",
      "Epoch 1262/2000 train_loss: 0.255070 acc: 0.899600\n",
      "Epoch 1263/2000 train_loss: 0.255068 acc: 0.899900\n",
      "Epoch 1264/2000 train_loss: 0.255066 acc: 0.899600\n",
      "Epoch 1265/2000 train_loss: 0.255047 acc: 0.899500\n",
      "Epoch 1266/2000 train_loss: 0.255056 acc: 0.899800\n",
      "Epoch 1267/2000 train_loss: 0.255048 acc: 0.899800\n",
      "Epoch 1268/2000 train_loss: 0.255043 acc: 0.899400\n",
      "Epoch 1269/2000 train_loss: 0.255044 acc: 0.899900\n",
      "Epoch 1270/2000 train_loss: 0.255042 acc: 0.899700\n",
      "Epoch 1271/2000 train_loss: 0.255035 acc: 0.899900\n",
      "Epoch 1272/2000 train_loss: 0.255012 acc: 0.899500\n",
      "Epoch 1273/2000 train_loss: 0.255035 acc: 0.900100\n",
      "Epoch 1274/2000 train_loss: 0.255023 acc: 0.899900\n",
      "Epoch 1275/2000 train_loss: 0.255010 acc: 0.899500\n",
      "Epoch 1276/2000 train_loss: 0.255008 acc: 0.899600\n",
      "Epoch 1277/2000 train_loss: 0.255011 acc: 0.899300\n",
      "Epoch 1278/2000 train_loss: 0.255011 acc: 0.899800\n",
      "Epoch 1279/2000 train_loss: 0.255001 acc: 0.899900\n",
      "Epoch 1280/2000 train_loss: 0.254993 acc: 0.900300\n",
      "Epoch 1281/2000 train_loss: 0.254992 acc: 0.900000\n",
      "Epoch 1282/2000 train_loss: 0.254982 acc: 0.899300\n",
      "Epoch 1283/2000 train_loss: 0.254983 acc: 0.899500\n",
      "Epoch 1284/2000 train_loss: 0.254986 acc: 0.899700\n",
      "Epoch 1285/2000 train_loss: 0.254980 acc: 0.899900\n",
      "Epoch 1286/2000 train_loss: 0.254972 acc: 0.899800\n",
      "Epoch 1287/2000 train_loss: 0.254971 acc: 0.899700\n",
      "Epoch 1288/2000 train_loss: 0.254970 acc: 0.899500\n",
      "Epoch 1289/2000 train_loss: 0.254958 acc: 0.899900\n",
      "Epoch 1290/2000 train_loss: 0.254966 acc: 0.899800\n",
      "Epoch 1291/2000 train_loss: 0.254954 acc: 0.899500\n",
      "Epoch 1292/2000 train_loss: 0.254941 acc: 0.899700\n",
      "Epoch 1293/2000 train_loss: 0.254959 acc: 0.900000\n",
      "Epoch 1294/2000 train_loss: 0.254944 acc: 0.899700\n",
      "Epoch 1295/2000 train_loss: 0.254939 acc: 0.899500\n",
      "Epoch 1296/2000 train_loss: 0.254944 acc: 0.899600\n",
      "Epoch 1297/2000 train_loss: 0.254934 acc: 0.899800\n",
      "Epoch 1298/2000 train_loss: 0.254929 acc: 0.899900\n",
      "Epoch 1299/2000 train_loss: 0.254914 acc: 0.899800\n",
      "Epoch 1300/2000 train_loss: 0.254928 acc: 0.899900\n",
      "Epoch 1301/2000 train_loss: 0.254914 acc: 0.899800\n",
      "Epoch 1302/2000 train_loss: 0.254896 acc: 0.899700\n",
      "Epoch 1303/2000 train_loss: 0.254910 acc: 0.899400\n",
      "Epoch 1304/2000 train_loss: 0.254905 acc: 0.899700\n",
      "Epoch 1305/2000 train_loss: 0.254901 acc: 0.899900\n",
      "Epoch 1306/2000 train_loss: 0.254890 acc: 0.899800\n",
      "Epoch 1307/2000 train_loss: 0.254895 acc: 0.899800\n",
      "Epoch 1308/2000 train_loss: 0.254872 acc: 0.899700\n",
      "Epoch 1309/2000 train_loss: 0.254890 acc: 0.899700\n",
      "Epoch 1310/2000 train_loss: 0.254888 acc: 0.899800\n",
      "Epoch 1311/2000 train_loss: 0.254861 acc: 0.899600\n",
      "Epoch 1312/2000 train_loss: 0.254877 acc: 0.899900\n",
      "Epoch 1313/2000 train_loss: 0.254875 acc: 0.899800\n",
      "Epoch 1314/2000 train_loss: 0.254862 acc: 0.900000\n",
      "Epoch 1315/2000 train_loss: 0.254869 acc: 0.899500\n",
      "Epoch 1316/2000 train_loss: 0.254866 acc: 0.899500\n",
      "Epoch 1317/2000 train_loss: 0.254854 acc: 0.899700\n",
      "Epoch 1318/2000 train_loss: 0.254856 acc: 0.900000\n",
      "Epoch 1319/2000 train_loss: 0.254849 acc: 0.899800\n",
      "Epoch 1320/2000 train_loss: 0.254842 acc: 0.899900\n",
      "Epoch 1321/2000 train_loss: 0.254844 acc: 0.899300\n",
      "Epoch 1322/2000 train_loss: 0.254825 acc: 0.899800\n",
      "Epoch 1323/2000 train_loss: 0.254835 acc: 0.900100\n",
      "Epoch 1324/2000 train_loss: 0.254831 acc: 0.899900\n",
      "Epoch 1325/2000 train_loss: 0.254830 acc: 0.899800\n",
      "Epoch 1326/2000 train_loss: 0.254818 acc: 0.899600\n",
      "Epoch 1327/2000 train_loss: 0.254809 acc: 0.899600\n",
      "Epoch 1328/2000 train_loss: 0.254805 acc: 0.900000\n",
      "Epoch 1329/2000 train_loss: 0.254815 acc: 0.899600\n",
      "Epoch 1330/2000 train_loss: 0.254803 acc: 0.899500\n",
      "Epoch 1331/2000 train_loss: 0.254800 acc: 0.899500\n",
      "Epoch 1332/2000 train_loss: 0.254803 acc: 0.899900\n",
      "Epoch 1333/2000 train_loss: 0.254790 acc: 0.899700\n",
      "Epoch 1334/2000 train_loss: 0.254791 acc: 0.899400\n",
      "Epoch 1335/2000 train_loss: 0.254784 acc: 0.899900\n",
      "Epoch 1336/2000 train_loss: 0.254782 acc: 0.899400\n",
      "Epoch 1337/2000 train_loss: 0.254781 acc: 0.899500\n",
      "Epoch 1338/2000 train_loss: 0.254787 acc: 0.899500\n",
      "Epoch 1339/2000 train_loss: 0.254772 acc: 0.899800\n",
      "Epoch 1340/2000 train_loss: 0.254764 acc: 0.899800\n",
      "Epoch 1341/2000 train_loss: 0.254764 acc: 0.899700\n",
      "Epoch 1342/2000 train_loss: 0.254759 acc: 0.899600\n",
      "Epoch 1343/2000 train_loss: 0.254754 acc: 0.900100\n",
      "Epoch 1344/2000 train_loss: 0.254763 acc: 0.899500\n",
      "Epoch 1345/2000 train_loss: 0.254751 acc: 0.900000\n",
      "Epoch 1346/2000 train_loss: 0.254747 acc: 0.899700\n",
      "Epoch 1347/2000 train_loss: 0.254749 acc: 0.899600\n",
      "Epoch 1348/2000 train_loss: 0.254748 acc: 0.899300\n",
      "Epoch 1349/2000 train_loss: 0.254732 acc: 0.899900\n",
      "Epoch 1350/2000 train_loss: 0.254741 acc: 0.899800\n",
      "Epoch 1351/2000 train_loss: 0.254727 acc: 0.899700\n",
      "Epoch 1352/2000 train_loss: 0.254725 acc: 0.899700\n",
      "Epoch 1353/2000 train_loss: 0.254713 acc: 0.899700\n",
      "Epoch 1354/2000 train_loss: 0.254712 acc: 0.899900\n",
      "Epoch 1355/2000 train_loss: 0.254720 acc: 0.899500\n",
      "Epoch 1356/2000 train_loss: 0.254705 acc: 0.899300\n",
      "Epoch 1357/2000 train_loss: 0.254709 acc: 0.899700\n",
      "Epoch 1358/2000 train_loss: 0.254708 acc: 0.899900\n",
      "Epoch 1359/2000 train_loss: 0.254708 acc: 0.899700\n",
      "Epoch 1360/2000 train_loss: 0.254660 acc: 0.899800\n",
      "Epoch 1361/2000 train_loss: 0.254705 acc: 0.899500\n",
      "Epoch 1362/2000 train_loss: 0.254691 acc: 0.899400\n",
      "Epoch 1363/2000 train_loss: 0.254689 acc: 0.899700\n",
      "Epoch 1364/2000 train_loss: 0.254689 acc: 0.899500\n",
      "Epoch 1365/2000 train_loss: 0.254680 acc: 0.899500\n",
      "Epoch 1366/2000 train_loss: 0.254678 acc: 0.899800\n",
      "Epoch 1367/2000 train_loss: 0.254671 acc: 0.899900\n",
      "Epoch 1368/2000 train_loss: 0.254679 acc: 0.899600\n",
      "Epoch 1369/2000 train_loss: 0.254661 acc: 0.899600\n",
      "Epoch 1370/2000 train_loss: 0.254656 acc: 0.899500\n",
      "Epoch 1371/2000 train_loss: 0.254660 acc: 0.899800\n",
      "Epoch 1372/2000 train_loss: 0.254654 acc: 0.899600\n",
      "Epoch 1373/2000 train_loss: 0.254649 acc: 0.899400\n",
      "Epoch 1374/2000 train_loss: 0.254654 acc: 0.899700\n",
      "Epoch 1375/2000 train_loss: 0.254645 acc: 0.899500\n",
      "Epoch 1376/2000 train_loss: 0.254645 acc: 0.899900\n",
      "Epoch 1377/2000 train_loss: 0.254640 acc: 0.899300\n",
      "Epoch 1378/2000 train_loss: 0.254638 acc: 0.899500\n",
      "Epoch 1379/2000 train_loss: 0.254628 acc: 0.899800\n",
      "Epoch 1380/2000 train_loss: 0.254635 acc: 0.899500\n",
      "Epoch 1381/2000 train_loss: 0.254629 acc: 0.899900\n",
      "Epoch 1382/2000 train_loss: 0.254617 acc: 0.900100\n",
      "Epoch 1383/2000 train_loss: 0.254614 acc: 0.899700\n",
      "Epoch 1384/2000 train_loss: 0.254594 acc: 0.899700\n",
      "Epoch 1385/2000 train_loss: 0.254613 acc: 0.899400\n",
      "Epoch 1386/2000 train_loss: 0.254608 acc: 0.899700\n",
      "Epoch 1387/2000 train_loss: 0.254595 acc: 0.900100\n",
      "Epoch 1388/2000 train_loss: 0.254600 acc: 0.899500\n",
      "Epoch 1389/2000 train_loss: 0.254587 acc: 0.900000\n",
      "Epoch 1390/2000 train_loss: 0.254594 acc: 0.899400\n",
      "Epoch 1391/2000 train_loss: 0.254572 acc: 0.899800\n",
      "Epoch 1392/2000 train_loss: 0.254589 acc: 0.899500\n",
      "Epoch 1393/2000 train_loss: 0.254578 acc: 0.899700\n",
      "Epoch 1394/2000 train_loss: 0.254571 acc: 0.899900\n",
      "Epoch 1395/2000 train_loss: 0.254559 acc: 0.899800\n",
      "Epoch 1396/2000 train_loss: 0.254572 acc: 0.899700\n",
      "Epoch 1397/2000 train_loss: 0.254560 acc: 0.899700\n",
      "Epoch 1398/2000 train_loss: 0.254567 acc: 0.899700\n",
      "Epoch 1399/2000 train_loss: 0.254568 acc: 0.899900\n",
      "Epoch 1400/2000 train_loss: 0.254559 acc: 0.899700\n",
      "Epoch 1401/2000 train_loss: 0.254553 acc: 0.899400\n",
      "Epoch 1402/2000 train_loss: 0.254544 acc: 0.899600\n",
      "Epoch 1403/2000 train_loss: 0.254549 acc: 0.899600\n",
      "Epoch 1404/2000 train_loss: 0.254533 acc: 0.900200\n",
      "Epoch 1405/2000 train_loss: 0.254544 acc: 0.899900\n",
      "Epoch 1406/2000 train_loss: 0.254539 acc: 0.899700\n",
      "Epoch 1407/2000 train_loss: 0.254535 acc: 0.899400\n",
      "Epoch 1408/2000 train_loss: 0.254533 acc: 0.899800\n",
      "Epoch 1409/2000 train_loss: 0.254526 acc: 0.899300\n",
      "Epoch 1410/2000 train_loss: 0.254525 acc: 0.899400\n",
      "Epoch 1411/2000 train_loss: 0.254519 acc: 0.899700\n",
      "Epoch 1412/2000 train_loss: 0.254522 acc: 0.899500\n",
      "Epoch 1413/2000 train_loss: 0.254507 acc: 0.900100\n",
      "Epoch 1414/2000 train_loss: 0.254508 acc: 0.899700\n",
      "Epoch 1415/2000 train_loss: 0.254511 acc: 0.899900\n",
      "Epoch 1416/2000 train_loss: 0.254501 acc: 0.899700\n",
      "Epoch 1417/2000 train_loss: 0.254501 acc: 0.900000\n",
      "Epoch 1418/2000 train_loss: 0.254496 acc: 0.899700\n",
      "Epoch 1419/2000 train_loss: 0.254489 acc: 0.899900\n",
      "Epoch 1420/2000 train_loss: 0.254477 acc: 0.899800\n",
      "Epoch 1421/2000 train_loss: 0.254488 acc: 0.899400\n",
      "Epoch 1422/2000 train_loss: 0.254489 acc: 0.899600\n",
      "Epoch 1423/2000 train_loss: 0.254479 acc: 0.899700\n",
      "Epoch 1424/2000 train_loss: 0.254474 acc: 0.899400\n",
      "Epoch 1425/2000 train_loss: 0.254480 acc: 0.899700\n",
      "Epoch 1426/2000 train_loss: 0.254457 acc: 0.900000\n",
      "Epoch 1427/2000 train_loss: 0.254465 acc: 0.899900\n",
      "Epoch 1428/2000 train_loss: 0.254462 acc: 0.899700\n",
      "Epoch 1429/2000 train_loss: 0.254457 acc: 0.899800\n",
      "Epoch 1430/2000 train_loss: 0.254460 acc: 0.900000\n",
      "Epoch 1431/2000 train_loss: 0.254451 acc: 0.899900\n",
      "Epoch 1432/2000 train_loss: 0.254455 acc: 0.899600\n",
      "Epoch 1433/2000 train_loss: 0.254448 acc: 0.899700\n",
      "Epoch 1434/2000 train_loss: 0.254440 acc: 0.899600\n",
      "Epoch 1435/2000 train_loss: 0.254441 acc: 0.899900\n",
      "Epoch 1436/2000 train_loss: 0.254443 acc: 0.899800\n",
      "Epoch 1437/2000 train_loss: 0.254435 acc: 0.899700\n",
      "Epoch 1438/2000 train_loss: 0.254424 acc: 0.900100\n",
      "Epoch 1439/2000 train_loss: 0.254411 acc: 0.899600\n",
      "Epoch 1440/2000 train_loss: 0.254420 acc: 0.899500\n",
      "Epoch 1441/2000 train_loss: 0.254413 acc: 0.899900\n",
      "Epoch 1442/2000 train_loss: 0.254417 acc: 0.899400\n",
      "Epoch 1443/2000 train_loss: 0.254418 acc: 0.899900\n",
      "Epoch 1444/2000 train_loss: 0.254417 acc: 0.899600\n",
      "Epoch 1445/2000 train_loss: 0.254394 acc: 0.899700\n",
      "Epoch 1446/2000 train_loss: 0.254407 acc: 0.899800\n",
      "Epoch 1447/2000 train_loss: 0.254399 acc: 0.899400\n",
      "Epoch 1448/2000 train_loss: 0.254398 acc: 0.899700\n",
      "Epoch 1449/2000 train_loss: 0.254395 acc: 0.899500\n",
      "Epoch 1450/2000 train_loss: 0.254389 acc: 0.900000\n",
      "Epoch 1451/2000 train_loss: 0.254389 acc: 0.899600\n",
      "Epoch 1452/2000 train_loss: 0.254381 acc: 0.899800\n",
      "Epoch 1453/2000 train_loss: 0.254387 acc: 0.899800\n",
      "Epoch 1454/2000 train_loss: 0.254372 acc: 0.899800\n",
      "Epoch 1455/2000 train_loss: 0.254369 acc: 0.899900\n",
      "Epoch 1456/2000 train_loss: 0.254371 acc: 0.899600\n",
      "Epoch 1457/2000 train_loss: 0.254373 acc: 0.899900\n",
      "Epoch 1458/2000 train_loss: 0.254367 acc: 0.899800\n",
      "Epoch 1459/2000 train_loss: 0.254366 acc: 0.899700\n",
      "Epoch 1460/2000 train_loss: 0.254356 acc: 0.899700\n",
      "Epoch 1461/2000 train_loss: 0.254354 acc: 0.899300\n",
      "Epoch 1462/2000 train_loss: 0.254353 acc: 0.899900\n",
      "Epoch 1463/2000 train_loss: 0.254332 acc: 0.899500\n",
      "Epoch 1464/2000 train_loss: 0.254347 acc: 0.899900\n",
      "Epoch 1465/2000 train_loss: 0.254343 acc: 0.899600\n",
      "Epoch 1466/2000 train_loss: 0.254343 acc: 0.899500\n",
      "Epoch 1467/2000 train_loss: 0.254339 acc: 0.899400\n",
      "Epoch 1468/2000 train_loss: 0.254337 acc: 0.899400\n",
      "Epoch 1469/2000 train_loss: 0.254331 acc: 0.899600\n",
      "Epoch 1470/2000 train_loss: 0.254330 acc: 0.900100\n",
      "Epoch 1471/2000 train_loss: 0.254320 acc: 0.899700\n",
      "Epoch 1472/2000 train_loss: 0.254319 acc: 0.899300\n",
      "Epoch 1473/2000 train_loss: 0.254323 acc: 0.899600\n",
      "Epoch 1474/2000 train_loss: 0.254313 acc: 0.899500\n",
      "Epoch 1475/2000 train_loss: 0.254315 acc: 0.899600\n",
      "Epoch 1476/2000 train_loss: 0.254304 acc: 0.900200\n",
      "Epoch 1477/2000 train_loss: 0.254308 acc: 0.899700\n",
      "Epoch 1478/2000 train_loss: 0.254289 acc: 0.899700\n",
      "Epoch 1479/2000 train_loss: 0.254296 acc: 0.899700\n",
      "Epoch 1480/2000 train_loss: 0.254285 acc: 0.899800\n",
      "Epoch 1481/2000 train_loss: 0.254295 acc: 0.899700\n",
      "Epoch 1482/2000 train_loss: 0.254283 acc: 0.899600\n",
      "Epoch 1483/2000 train_loss: 0.254290 acc: 0.899500\n",
      "Epoch 1484/2000 train_loss: 0.254286 acc: 0.899800\n",
      "Epoch 1485/2000 train_loss: 0.254286 acc: 0.899700\n",
      "Epoch 1486/2000 train_loss: 0.254281 acc: 0.899900\n",
      "Epoch 1487/2000 train_loss: 0.254277 acc: 0.899400\n",
      "Epoch 1488/2000 train_loss: 0.254269 acc: 0.899900\n",
      "Epoch 1489/2000 train_loss: 0.254260 acc: 0.899900\n",
      "Epoch 1490/2000 train_loss: 0.254252 acc: 0.899700\n",
      "Epoch 1491/2000 train_loss: 0.254233 acc: 0.899600\n",
      "Epoch 1492/2000 train_loss: 0.254257 acc: 0.899700\n",
      "Epoch 1493/2000 train_loss: 0.254259 acc: 0.899700\n",
      "Epoch 1494/2000 train_loss: 0.254254 acc: 0.899700\n",
      "Epoch 1495/2000 train_loss: 0.254247 acc: 0.899700\n",
      "Epoch 1496/2000 train_loss: 0.254245 acc: 0.899900\n",
      "Epoch 1497/2000 train_loss: 0.254238 acc: 0.899800\n",
      "Epoch 1498/2000 train_loss: 0.254242 acc: 0.899700\n",
      "Epoch 1499/2000 train_loss: 0.254237 acc: 0.899400\n",
      "Epoch 1500/2000 train_loss: 0.254222 acc: 0.899800\n",
      "Epoch 1501/2000 train_loss: 0.254232 acc: 0.899100\n",
      "Epoch 1502/2000 train_loss: 0.254215 acc: 0.900000\n",
      "Epoch 1503/2000 train_loss: 0.254228 acc: 0.899900\n",
      "Epoch 1504/2000 train_loss: 0.254216 acc: 0.899900\n",
      "Epoch 1505/2000 train_loss: 0.254214 acc: 0.899600\n",
      "Epoch 1506/2000 train_loss: 0.254216 acc: 0.900000\n",
      "Epoch 1507/2000 train_loss: 0.254197 acc: 0.899500\n",
      "Epoch 1508/2000 train_loss: 0.254219 acc: 0.899900\n",
      "Epoch 1509/2000 train_loss: 0.254209 acc: 0.900000\n",
      "Epoch 1510/2000 train_loss: 0.254202 acc: 0.899600\n",
      "Epoch 1511/2000 train_loss: 0.254194 acc: 0.899800\n",
      "Epoch 1512/2000 train_loss: 0.254204 acc: 0.899700\n",
      "Epoch 1513/2000 train_loss: 0.254195 acc: 0.900000\n",
      "Epoch 1514/2000 train_loss: 0.254193 acc: 0.899700\n",
      "Epoch 1515/2000 train_loss: 0.254186 acc: 0.900000\n",
      "Epoch 1516/2000 train_loss: 0.254183 acc: 0.900000\n",
      "Epoch 1517/2000 train_loss: 0.254183 acc: 0.899600\n",
      "Epoch 1518/2000 train_loss: 0.254176 acc: 0.899600\n",
      "Epoch 1519/2000 train_loss: 0.254174 acc: 0.899700\n",
      "Epoch 1520/2000 train_loss: 0.254169 acc: 0.899900\n",
      "Epoch 1521/2000 train_loss: 0.254157 acc: 0.899800\n",
      "Epoch 1522/2000 train_loss: 0.254160 acc: 0.899900\n",
      "Epoch 1523/2000 train_loss: 0.254165 acc: 0.899500\n",
      "Epoch 1524/2000 train_loss: 0.254161 acc: 0.899800\n",
      "Epoch 1525/2000 train_loss: 0.254155 acc: 0.899300\n",
      "Epoch 1526/2000 train_loss: 0.254152 acc: 0.899600\n",
      "Epoch 1527/2000 train_loss: 0.254153 acc: 0.900000\n",
      "Epoch 1528/2000 train_loss: 0.254147 acc: 0.900000\n",
      "Epoch 1529/2000 train_loss: 0.254136 acc: 0.899100\n",
      "Epoch 1530/2000 train_loss: 0.254143 acc: 0.900100\n",
      "Epoch 1531/2000 train_loss: 0.254139 acc: 0.899100\n",
      "Epoch 1532/2000 train_loss: 0.254130 acc: 0.899400\n",
      "Epoch 1533/2000 train_loss: 0.254133 acc: 0.899900\n",
      "Epoch 1534/2000 train_loss: 0.254130 acc: 0.899600\n",
      "Epoch 1535/2000 train_loss: 0.254113 acc: 0.899800\n",
      "Epoch 1536/2000 train_loss: 0.254129 acc: 0.899800\n",
      "Epoch 1537/2000 train_loss: 0.254120 acc: 0.900000\n",
      "Epoch 1538/2000 train_loss: 0.254111 acc: 0.899900\n",
      "Epoch 1539/2000 train_loss: 0.254119 acc: 0.900200\n",
      "Epoch 1540/2000 train_loss: 0.254110 acc: 0.899500\n",
      "Epoch 1541/2000 train_loss: 0.254101 acc: 0.899800\n",
      "Epoch 1542/2000 train_loss: 0.254105 acc: 0.899800\n",
      "Epoch 1543/2000 train_loss: 0.254103 acc: 0.899700\n",
      "Epoch 1544/2000 train_loss: 0.254092 acc: 0.899800\n",
      "Epoch 1545/2000 train_loss: 0.254099 acc: 0.899900\n",
      "Epoch 1546/2000 train_loss: 0.254102 acc: 0.900100\n",
      "Epoch 1547/2000 train_loss: 0.254093 acc: 0.899600\n",
      "Epoch 1548/2000 train_loss: 0.254091 acc: 0.899700\n",
      "Epoch 1549/2000 train_loss: 0.254066 acc: 0.899400\n",
      "Epoch 1550/2000 train_loss: 0.254081 acc: 0.899600\n",
      "Epoch 1551/2000 train_loss: 0.254059 acc: 0.899600\n",
      "Epoch 1552/2000 train_loss: 0.254089 acc: 0.899700\n",
      "Epoch 1553/2000 train_loss: 0.254082 acc: 0.900100\n",
      "Epoch 1554/2000 train_loss: 0.254075 acc: 0.899600\n",
      "Epoch 1555/2000 train_loss: 0.254073 acc: 0.899900\n",
      "Epoch 1556/2000 train_loss: 0.254063 acc: 0.899800\n",
      "Epoch 1557/2000 train_loss: 0.254055 acc: 0.899700\n",
      "Epoch 1558/2000 train_loss: 0.254055 acc: 0.900000\n",
      "Epoch 1559/2000 train_loss: 0.254059 acc: 0.899800\n",
      "Epoch 1560/2000 train_loss: 0.254058 acc: 0.899900\n",
      "Epoch 1561/2000 train_loss: 0.254050 acc: 0.899800\n",
      "Epoch 1562/2000 train_loss: 0.254046 acc: 0.900000\n",
      "Epoch 1563/2000 train_loss: 0.254054 acc: 0.899700\n",
      "Epoch 1564/2000 train_loss: 0.254044 acc: 0.899900\n",
      "Epoch 1565/2000 train_loss: 0.254036 acc: 0.899700\n",
      "Epoch 1566/2000 train_loss: 0.254043 acc: 0.899800\n",
      "Epoch 1567/2000 train_loss: 0.254043 acc: 0.900200\n",
      "Epoch 1568/2000 train_loss: 0.254030 acc: 0.899300\n",
      "Epoch 1569/2000 train_loss: 0.254032 acc: 0.900000\n",
      "Epoch 1570/2000 train_loss: 0.254022 acc: 0.899700\n",
      "Epoch 1571/2000 train_loss: 0.254029 acc: 0.899700\n",
      "Epoch 1572/2000 train_loss: 0.254019 acc: 0.900000\n",
      "Epoch 1573/2000 train_loss: 0.254007 acc: 0.900100\n",
      "Epoch 1574/2000 train_loss: 0.254018 acc: 0.900000\n",
      "Epoch 1575/2000 train_loss: 0.254013 acc: 0.899800\n",
      "Epoch 1576/2000 train_loss: 0.254010 acc: 0.899900\n",
      "Epoch 1577/2000 train_loss: 0.254001 acc: 0.900100\n",
      "Epoch 1578/2000 train_loss: 0.254003 acc: 0.899600\n",
      "Epoch 1579/2000 train_loss: 0.254005 acc: 0.899800\n",
      "Epoch 1580/2000 train_loss: 0.253995 acc: 0.899800\n",
      "Epoch 1581/2000 train_loss: 0.253994 acc: 0.900000\n",
      "Epoch 1582/2000 train_loss: 0.253987 acc: 0.900000\n",
      "Epoch 1583/2000 train_loss: 0.253981 acc: 0.899700\n",
      "Epoch 1584/2000 train_loss: 0.253994 acc: 0.900100\n",
      "Epoch 1585/2000 train_loss: 0.253981 acc: 0.899700\n",
      "Epoch 1586/2000 train_loss: 0.253977 acc: 0.899800\n",
      "Epoch 1587/2000 train_loss: 0.253983 acc: 0.899800\n",
      "Epoch 1588/2000 train_loss: 0.253970 acc: 0.899800\n",
      "Epoch 1589/2000 train_loss: 0.253973 acc: 0.899800\n",
      "Epoch 1590/2000 train_loss: 0.253973 acc: 0.899900\n",
      "Epoch 1591/2000 train_loss: 0.253967 acc: 0.899500\n",
      "Epoch 1592/2000 train_loss: 0.253958 acc: 0.899500\n",
      "Epoch 1593/2000 train_loss: 0.253962 acc: 0.899900\n",
      "Epoch 1594/2000 train_loss: 0.253954 acc: 0.900000\n",
      "Epoch 1595/2000 train_loss: 0.253953 acc: 0.900100\n",
      "Epoch 1596/2000 train_loss: 0.253957 acc: 0.899800\n",
      "Epoch 1597/2000 train_loss: 0.253951 acc: 0.900000\n",
      "Epoch 1598/2000 train_loss: 0.253953 acc: 0.899800\n",
      "Epoch 1599/2000 train_loss: 0.253937 acc: 0.899800\n",
      "Epoch 1600/2000 train_loss: 0.253934 acc: 0.899700\n",
      "Epoch 1601/2000 train_loss: 0.253937 acc: 0.899500\n",
      "Epoch 1602/2000 train_loss: 0.253935 acc: 0.899700\n",
      "Epoch 1603/2000 train_loss: 0.253936 acc: 0.900000\n",
      "Epoch 1604/2000 train_loss: 0.253923 acc: 0.899700\n",
      "Epoch 1605/2000 train_loss: 0.253924 acc: 0.899800\n",
      "Epoch 1606/2000 train_loss: 0.253925 acc: 0.899700\n",
      "Epoch 1607/2000 train_loss: 0.253917 acc: 0.899800\n",
      "Epoch 1608/2000 train_loss: 0.253919 acc: 0.899700\n",
      "Epoch 1609/2000 train_loss: 0.253917 acc: 0.900100\n",
      "Epoch 1610/2000 train_loss: 0.253912 acc: 0.899600\n",
      "Epoch 1611/2000 train_loss: 0.253908 acc: 0.899800\n",
      "Epoch 1612/2000 train_loss: 0.253913 acc: 0.899500\n",
      "Epoch 1613/2000 train_loss: 0.253909 acc: 0.899700\n",
      "Epoch 1614/2000 train_loss: 0.253904 acc: 0.900000\n",
      "Epoch 1615/2000 train_loss: 0.253896 acc: 0.899500\n",
      "Epoch 1616/2000 train_loss: 0.253886 acc: 0.899600\n",
      "Epoch 1617/2000 train_loss: 0.253891 acc: 0.899700\n",
      "Epoch 1618/2000 train_loss: 0.253890 acc: 0.899700\n",
      "Epoch 1619/2000 train_loss: 0.253887 acc: 0.899800\n",
      "Epoch 1620/2000 train_loss: 0.253879 acc: 0.899600\n",
      "Epoch 1621/2000 train_loss: 0.253882 acc: 0.899600\n",
      "Epoch 1622/2000 train_loss: 0.253890 acc: 0.899900\n",
      "Epoch 1623/2000 train_loss: 0.253875 acc: 0.900300\n",
      "Epoch 1624/2000 train_loss: 0.253877 acc: 0.899500\n",
      "Epoch 1625/2000 train_loss: 0.253879 acc: 0.899700\n",
      "Epoch 1626/2000 train_loss: 0.253860 acc: 0.899400\n",
      "Epoch 1627/2000 train_loss: 0.253873 acc: 0.899900\n",
      "Epoch 1628/2000 train_loss: 0.253866 acc: 0.899900\n",
      "Epoch 1629/2000 train_loss: 0.253863 acc: 0.899800\n",
      "Epoch 1630/2000 train_loss: 0.253860 acc: 0.899800\n",
      "Epoch 1631/2000 train_loss: 0.253853 acc: 0.899800\n",
      "Epoch 1632/2000 train_loss: 0.253855 acc: 0.899700\n",
      "Epoch 1633/2000 train_loss: 0.253851 acc: 0.899900\n",
      "Epoch 1634/2000 train_loss: 0.253847 acc: 0.900300\n",
      "Epoch 1635/2000 train_loss: 0.253839 acc: 0.899600\n",
      "Epoch 1636/2000 train_loss: 0.253841 acc: 0.899900\n",
      "Epoch 1637/2000 train_loss: 0.253832 acc: 0.899600\n",
      "Epoch 1638/2000 train_loss: 0.253849 acc: 0.899900\n",
      "Epoch 1639/2000 train_loss: 0.253835 acc: 0.900100\n",
      "Epoch 1640/2000 train_loss: 0.253839 acc: 0.899900\n",
      "Epoch 1641/2000 train_loss: 0.253809 acc: 0.899700\n",
      "Epoch 1642/2000 train_loss: 0.253831 acc: 0.899800\n",
      "Epoch 1643/2000 train_loss: 0.253819 acc: 0.899700\n",
      "Epoch 1644/2000 train_loss: 0.253827 acc: 0.899900\n",
      "Epoch 1645/2000 train_loss: 0.253819 acc: 0.899600\n",
      "Epoch 1646/2000 train_loss: 0.253816 acc: 0.899600\n",
      "Epoch 1647/2000 train_loss: 0.253816 acc: 0.900200\n",
      "Epoch 1648/2000 train_loss: 0.253815 acc: 0.899800\n",
      "Epoch 1649/2000 train_loss: 0.253808 acc: 0.899700\n",
      "Epoch 1650/2000 train_loss: 0.253808 acc: 0.899700\n",
      "Epoch 1651/2000 train_loss: 0.253805 acc: 0.899800\n",
      "Epoch 1652/2000 train_loss: 0.253800 acc: 0.899700\n",
      "Epoch 1653/2000 train_loss: 0.253798 acc: 0.899800\n",
      "Epoch 1654/2000 train_loss: 0.253793 acc: 0.899800\n",
      "Epoch 1655/2000 train_loss: 0.253795 acc: 0.900000\n",
      "Epoch 1656/2000 train_loss: 0.253796 acc: 0.900000\n",
      "Epoch 1657/2000 train_loss: 0.253779 acc: 0.899900\n",
      "Epoch 1658/2000 train_loss: 0.253778 acc: 0.899700\n",
      "Epoch 1659/2000 train_loss: 0.253786 acc: 0.899700\n",
      "Epoch 1660/2000 train_loss: 0.253781 acc: 0.899800\n",
      "Epoch 1661/2000 train_loss: 0.253771 acc: 0.899900\n",
      "Epoch 1662/2000 train_loss: 0.253772 acc: 0.900100\n",
      "Epoch 1663/2000 train_loss: 0.253749 acc: 0.900400\n",
      "Epoch 1664/2000 train_loss: 0.253778 acc: 0.899500\n",
      "Epoch 1665/2000 train_loss: 0.253764 acc: 0.899500\n",
      "Epoch 1666/2000 train_loss: 0.253762 acc: 0.899700\n",
      "Epoch 1667/2000 train_loss: 0.253759 acc: 0.899600\n",
      "Epoch 1668/2000 train_loss: 0.253759 acc: 0.899800\n",
      "Epoch 1669/2000 train_loss: 0.253761 acc: 0.899700\n",
      "Epoch 1670/2000 train_loss: 0.253747 acc: 0.900000\n",
      "Epoch 1671/2000 train_loss: 0.253750 acc: 0.900000\n",
      "Epoch 1672/2000 train_loss: 0.253753 acc: 0.899600\n",
      "Epoch 1673/2000 train_loss: 0.253739 acc: 0.899900\n",
      "Epoch 1674/2000 train_loss: 0.253744 acc: 0.899600\n",
      "Epoch 1675/2000 train_loss: 0.253735 acc: 0.899800\n",
      "Epoch 1676/2000 train_loss: 0.253735 acc: 0.900200\n",
      "Epoch 1677/2000 train_loss: 0.253734 acc: 0.899400\n",
      "Epoch 1678/2000 train_loss: 0.253726 acc: 0.899700\n",
      "Epoch 1679/2000 train_loss: 0.253710 acc: 0.899900\n",
      "Epoch 1680/2000 train_loss: 0.253734 acc: 0.900100\n",
      "Epoch 1681/2000 train_loss: 0.253728 acc: 0.899500\n",
      "Epoch 1682/2000 train_loss: 0.253709 acc: 0.899900\n",
      "Epoch 1683/2000 train_loss: 0.253716 acc: 0.899900\n",
      "Epoch 1684/2000 train_loss: 0.253727 acc: 0.899200\n",
      "Epoch 1685/2000 train_loss: 0.253705 acc: 0.900200\n",
      "Epoch 1686/2000 train_loss: 0.253712 acc: 0.899300\n",
      "Epoch 1687/2000 train_loss: 0.253708 acc: 0.899900\n",
      "Epoch 1688/2000 train_loss: 0.253713 acc: 0.899500\n",
      "Epoch 1689/2000 train_loss: 0.253708 acc: 0.899800\n",
      "Epoch 1690/2000 train_loss: 0.253702 acc: 0.899600\n",
      "Epoch 1691/2000 train_loss: 0.253696 acc: 0.899600\n",
      "Epoch 1692/2000 train_loss: 0.253694 acc: 0.899900\n",
      "Epoch 1693/2000 train_loss: 0.253689 acc: 0.899600\n",
      "Epoch 1694/2000 train_loss: 0.253684 acc: 0.899600\n",
      "Epoch 1695/2000 train_loss: 0.253691 acc: 0.899700\n",
      "Epoch 1696/2000 train_loss: 0.253694 acc: 0.899800\n",
      "Epoch 1697/2000 train_loss: 0.253676 acc: 0.899900\n",
      "Epoch 1698/2000 train_loss: 0.253694 acc: 0.899800\n",
      "Epoch 1699/2000 train_loss: 0.253673 acc: 0.900100\n",
      "Epoch 1700/2000 train_loss: 0.253683 acc: 0.899900\n",
      "Epoch 1701/2000 train_loss: 0.253673 acc: 0.899500\n",
      "Epoch 1702/2000 train_loss: 0.253667 acc: 0.899800\n",
      "Epoch 1703/2000 train_loss: 0.253670 acc: 0.899800\n",
      "Epoch 1704/2000 train_loss: 0.253667 acc: 0.899900\n",
      "Epoch 1705/2000 train_loss: 0.253665 acc: 0.899800\n",
      "Epoch 1706/2000 train_loss: 0.253658 acc: 0.900000\n",
      "Epoch 1707/2000 train_loss: 0.253668 acc: 0.900200\n",
      "Epoch 1708/2000 train_loss: 0.253654 acc: 0.899700\n",
      "Epoch 1709/2000 train_loss: 0.253653 acc: 0.899700\n",
      "Epoch 1710/2000 train_loss: 0.253629 acc: 0.899600\n",
      "Epoch 1711/2000 train_loss: 0.253657 acc: 0.899700\n",
      "Epoch 1712/2000 train_loss: 0.253637 acc: 0.899400\n",
      "Epoch 1713/2000 train_loss: 0.253651 acc: 0.899900\n",
      "Epoch 1714/2000 train_loss: 0.253639 acc: 0.899500\n",
      "Epoch 1715/2000 train_loss: 0.253643 acc: 0.900000\n",
      "Epoch 1716/2000 train_loss: 0.253635 acc: 0.899800\n",
      "Epoch 1717/2000 train_loss: 0.253637 acc: 0.900100\n",
      "Epoch 1718/2000 train_loss: 0.253631 acc: 0.899600\n",
      "Epoch 1719/2000 train_loss: 0.253593 acc: 0.900100\n",
      "Epoch 1720/2000 train_loss: 0.253636 acc: 0.899400\n",
      "Epoch 1721/2000 train_loss: 0.253624 acc: 0.899500\n",
      "Epoch 1722/2000 train_loss: 0.253623 acc: 0.899600\n",
      "Epoch 1723/2000 train_loss: 0.253617 acc: 0.900000\n",
      "Epoch 1724/2000 train_loss: 0.253621 acc: 0.899700\n",
      "Epoch 1725/2000 train_loss: 0.253614 acc: 0.899900\n",
      "Epoch 1726/2000 train_loss: 0.253606 acc: 0.900000\n",
      "Epoch 1727/2000 train_loss: 0.253609 acc: 0.899400\n",
      "Epoch 1728/2000 train_loss: 0.253609 acc: 0.899800\n",
      "Epoch 1729/2000 train_loss: 0.253611 acc: 0.899500\n",
      "Epoch 1730/2000 train_loss: 0.253605 acc: 0.899600\n",
      "Epoch 1731/2000 train_loss: 0.253601 acc: 0.899900\n",
      "Epoch 1732/2000 train_loss: 0.253594 acc: 0.899600\n",
      "Epoch 1733/2000 train_loss: 0.253582 acc: 0.899400\n",
      "Epoch 1734/2000 train_loss: 0.253587 acc: 0.899100\n",
      "Epoch 1735/2000 train_loss: 0.253588 acc: 0.900000\n",
      "Epoch 1736/2000 train_loss: 0.253584 acc: 0.899900\n",
      "Epoch 1737/2000 train_loss: 0.253582 acc: 0.899400\n",
      "Epoch 1738/2000 train_loss: 0.253580 acc: 0.899800\n",
      "Epoch 1739/2000 train_loss: 0.253584 acc: 0.899500\n",
      "Epoch 1740/2000 train_loss: 0.253579 acc: 0.899900\n",
      "Epoch 1741/2000 train_loss: 0.253571 acc: 0.899800\n",
      "Epoch 1742/2000 train_loss: 0.253566 acc: 0.899900\n",
      "Epoch 1743/2000 train_loss: 0.253569 acc: 0.899800\n",
      "Epoch 1744/2000 train_loss: 0.253568 acc: 0.899500\n",
      "Epoch 1745/2000 train_loss: 0.253575 acc: 0.899900\n",
      "Epoch 1746/2000 train_loss: 0.253555 acc: 0.900000\n",
      "Epoch 1747/2000 train_loss: 0.253558 acc: 0.899600\n",
      "Epoch 1748/2000 train_loss: 0.253559 acc: 0.899700\n",
      "Epoch 1749/2000 train_loss: 0.253547 acc: 0.899900\n",
      "Epoch 1750/2000 train_loss: 0.253552 acc: 0.899700\n",
      "Epoch 1751/2000 train_loss: 0.253554 acc: 0.899600\n",
      "Epoch 1752/2000 train_loss: 0.253545 acc: 0.899700\n",
      "Epoch 1753/2000 train_loss: 0.253542 acc: 0.900100\n",
      "Epoch 1754/2000 train_loss: 0.253547 acc: 0.899700\n",
      "Epoch 1755/2000 train_loss: 0.253550 acc: 0.899700\n",
      "Epoch 1756/2000 train_loss: 0.253540 acc: 0.899800\n",
      "Epoch 1757/2000 train_loss: 0.253526 acc: 0.899400\n",
      "Epoch 1758/2000 train_loss: 0.253538 acc: 0.900000\n",
      "Epoch 1759/2000 train_loss: 0.253533 acc: 0.899500\n",
      "Epoch 1760/2000 train_loss: 0.253531 acc: 0.899900\n",
      "Epoch 1761/2000 train_loss: 0.253520 acc: 0.899900\n",
      "Epoch 1762/2000 train_loss: 0.253522 acc: 0.899800\n",
      "Epoch 1763/2000 train_loss: 0.253518 acc: 0.899800\n",
      "Epoch 1764/2000 train_loss: 0.253491 acc: 0.899500\n",
      "Epoch 1765/2000 train_loss: 0.253529 acc: 0.899700\n",
      "Epoch 1766/2000 train_loss: 0.253516 acc: 0.899500\n",
      "Epoch 1767/2000 train_loss: 0.253511 acc: 0.899800\n",
      "Epoch 1768/2000 train_loss: 0.253509 acc: 0.899700\n",
      "Epoch 1769/2000 train_loss: 0.253505 acc: 0.899600\n",
      "Epoch 1770/2000 train_loss: 0.253504 acc: 0.899800\n",
      "Epoch 1771/2000 train_loss: 0.253506 acc: 0.899600\n",
      "Epoch 1772/2000 train_loss: 0.253503 acc: 0.899800\n",
      "Epoch 1773/2000 train_loss: 0.253500 acc: 0.899600\n",
      "Epoch 1774/2000 train_loss: 0.253494 acc: 0.899500\n",
      "Epoch 1775/2000 train_loss: 0.253490 acc: 0.899800\n",
      "Epoch 1776/2000 train_loss: 0.253479 acc: 0.899500\n",
      "Epoch 1777/2000 train_loss: 0.253474 acc: 0.899500\n",
      "Epoch 1778/2000 train_loss: 0.253492 acc: 0.899600\n",
      "Epoch 1779/2000 train_loss: 0.253498 acc: 0.899700\n",
      "Epoch 1780/2000 train_loss: 0.253479 acc: 0.899600\n",
      "Epoch 1781/2000 train_loss: 0.253491 acc: 0.899700\n",
      "Epoch 1782/2000 train_loss: 0.253477 acc: 0.899500\n",
      "Epoch 1783/2000 train_loss: 0.253469 acc: 0.899300\n",
      "Epoch 1784/2000 train_loss: 0.253475 acc: 0.899800\n",
      "Epoch 1785/2000 train_loss: 0.253473 acc: 0.899500\n",
      "Epoch 1786/2000 train_loss: 0.253437 acc: 0.899800\n",
      "Epoch 1787/2000 train_loss: 0.253481 acc: 0.899700\n",
      "Epoch 1788/2000 train_loss: 0.253465 acc: 0.899400\n",
      "Epoch 1789/2000 train_loss: 0.253466 acc: 0.899700\n",
      "Epoch 1790/2000 train_loss: 0.253448 acc: 0.899400\n",
      "Epoch 1791/2000 train_loss: 0.253449 acc: 0.899400\n",
      "Epoch 1792/2000 train_loss: 0.253455 acc: 0.899900\n",
      "Epoch 1793/2000 train_loss: 0.253452 acc: 0.899200\n",
      "Epoch 1794/2000 train_loss: 0.253450 acc: 0.899600\n",
      "Epoch 1795/2000 train_loss: 0.253451 acc: 0.899500\n",
      "Epoch 1796/2000 train_loss: 0.253445 acc: 0.899900\n",
      "Epoch 1797/2000 train_loss: 0.253445 acc: 0.899700\n",
      "Epoch 1798/2000 train_loss: 0.253441 acc: 0.899800\n",
      "Epoch 1799/2000 train_loss: 0.253442 acc: 0.899700\n",
      "Epoch 1800/2000 train_loss: 0.253434 acc: 0.899900\n",
      "Epoch 1801/2000 train_loss: 0.253437 acc: 0.899700\n",
      "Epoch 1802/2000 train_loss: 0.253432 acc: 0.899300\n",
      "Epoch 1803/2000 train_loss: 0.253416 acc: 0.899900\n",
      "Epoch 1804/2000 train_loss: 0.253436 acc: 0.899900\n",
      "Epoch 1805/2000 train_loss: 0.253419 acc: 0.900000\n",
      "Epoch 1806/2000 train_loss: 0.253427 acc: 0.900100\n",
      "Epoch 1807/2000 train_loss: 0.253412 acc: 0.899900\n",
      "Epoch 1808/2000 train_loss: 0.253419 acc: 0.899600\n",
      "Epoch 1809/2000 train_loss: 0.253422 acc: 0.899600\n",
      "Epoch 1810/2000 train_loss: 0.253414 acc: 0.899600\n",
      "Epoch 1811/2000 train_loss: 0.253418 acc: 0.899400\n",
      "Epoch 1812/2000 train_loss: 0.253404 acc: 0.899700\n",
      "Epoch 1813/2000 train_loss: 0.253407 acc: 0.899500\n",
      "Epoch 1814/2000 train_loss: 0.253396 acc: 0.899400\n",
      "Epoch 1815/2000 train_loss: 0.253402 acc: 0.899700\n",
      "Epoch 1816/2000 train_loss: 0.253402 acc: 0.899700\n",
      "Epoch 1817/2000 train_loss: 0.253393 acc: 0.899600\n",
      "Epoch 1818/2000 train_loss: 0.253399 acc: 0.899600\n",
      "Epoch 1819/2000 train_loss: 0.253389 acc: 0.899400\n",
      "Epoch 1820/2000 train_loss: 0.253392 acc: 0.899500\n",
      "Epoch 1821/2000 train_loss: 0.253382 acc: 0.899900\n",
      "Epoch 1822/2000 train_loss: 0.253392 acc: 0.899800\n",
      "Epoch 1823/2000 train_loss: 0.253393 acc: 0.899500\n",
      "Epoch 1824/2000 train_loss: 0.253387 acc: 0.899400\n",
      "Epoch 1825/2000 train_loss: 0.253389 acc: 0.899500\n",
      "Epoch 1826/2000 train_loss: 0.253374 acc: 0.899400\n",
      "Epoch 1827/2000 train_loss: 0.253380 acc: 0.899400\n",
      "Epoch 1828/2000 train_loss: 0.253372 acc: 0.899700\n",
      "Epoch 1829/2000 train_loss: 0.253374 acc: 0.899600\n",
      "Epoch 1830/2000 train_loss: 0.253360 acc: 0.899500\n",
      "Epoch 1831/2000 train_loss: 0.253365 acc: 0.899900\n",
      "Epoch 1832/2000 train_loss: 0.253366 acc: 0.899900\n",
      "Epoch 1833/2000 train_loss: 0.253369 acc: 0.899500\n",
      "Epoch 1834/2000 train_loss: 0.253358 acc: 0.899700\n",
      "Epoch 1835/2000 train_loss: 0.253354 acc: 0.899800\n",
      "Epoch 1836/2000 train_loss: 0.253348 acc: 0.899800\n",
      "Epoch 1837/2000 train_loss: 0.253357 acc: 0.899700\n",
      "Epoch 1838/2000 train_loss: 0.253350 acc: 0.899400\n",
      "Epoch 1839/2000 train_loss: 0.253338 acc: 0.899400\n",
      "Epoch 1840/2000 train_loss: 0.253341 acc: 0.899200\n",
      "Epoch 1841/2000 train_loss: 0.253346 acc: 0.899400\n",
      "Epoch 1842/2000 train_loss: 0.253339 acc: 0.899500\n",
      "Epoch 1843/2000 train_loss: 0.253339 acc: 0.899700\n",
      "Epoch 1844/2000 train_loss: 0.253336 acc: 0.899700\n",
      "Epoch 1845/2000 train_loss: 0.253335 acc: 0.899900\n",
      "Epoch 1846/2000 train_loss: 0.253333 acc: 0.899300\n",
      "Epoch 1847/2000 train_loss: 0.253335 acc: 0.899400\n",
      "Epoch 1848/2000 train_loss: 0.253329 acc: 0.899900\n",
      "Epoch 1849/2000 train_loss: 0.253330 acc: 0.899600\n",
      "Epoch 1850/2000 train_loss: 0.253326 acc: 0.899700\n",
      "Epoch 1851/2000 train_loss: 0.253322 acc: 0.899500\n",
      "Epoch 1852/2000 train_loss: 0.253319 acc: 0.899500\n",
      "Epoch 1853/2000 train_loss: 0.253312 acc: 0.899500\n",
      "Epoch 1854/2000 train_loss: 0.253313 acc: 0.899700\n",
      "Epoch 1855/2000 train_loss: 0.253310 acc: 0.899700\n",
      "Epoch 1856/2000 train_loss: 0.253308 acc: 0.899800\n",
      "Epoch 1857/2000 train_loss: 0.253302 acc: 0.899300\n",
      "Epoch 1858/2000 train_loss: 0.253312 acc: 0.899500\n",
      "Epoch 1859/2000 train_loss: 0.253306 acc: 0.899500\n",
      "Epoch 1860/2000 train_loss: 0.253306 acc: 0.899700\n",
      "Epoch 1861/2000 train_loss: 0.253297 acc: 0.899700\n",
      "Epoch 1862/2000 train_loss: 0.253296 acc: 0.899700\n",
      "Epoch 1863/2000 train_loss: 0.253291 acc: 0.899400\n",
      "Epoch 1864/2000 train_loss: 0.253301 acc: 0.899500\n",
      "Epoch 1865/2000 train_loss: 0.253292 acc: 0.899500\n",
      "Epoch 1866/2000 train_loss: 0.253279 acc: 0.899500\n",
      "Epoch 1867/2000 train_loss: 0.253283 acc: 0.899500\n",
      "Epoch 1868/2000 train_loss: 0.253285 acc: 0.899800\n",
      "Epoch 1869/2000 train_loss: 0.253272 acc: 0.899500\n",
      "Epoch 1870/2000 train_loss: 0.253286 acc: 0.899500\n",
      "Epoch 1871/2000 train_loss: 0.253278 acc: 0.899400\n",
      "Epoch 1872/2000 train_loss: 0.253265 acc: 0.899500\n",
      "Epoch 1873/2000 train_loss: 0.253261 acc: 0.899900\n",
      "Epoch 1874/2000 train_loss: 0.253278 acc: 0.899700\n",
      "Epoch 1875/2000 train_loss: 0.253257 acc: 0.899200\n",
      "Epoch 1876/2000 train_loss: 0.253266 acc: 0.899500\n",
      "Epoch 1877/2000 train_loss: 0.253268 acc: 0.899600\n",
      "Epoch 1878/2000 train_loss: 0.253262 acc: 0.899400\n",
      "Epoch 1879/2000 train_loss: 0.253264 acc: 0.899500\n",
      "Epoch 1880/2000 train_loss: 0.253259 acc: 0.899400\n",
      "Epoch 1881/2000 train_loss: 0.253257 acc: 0.899700\n",
      "Epoch 1882/2000 train_loss: 0.253246 acc: 0.899300\n",
      "Epoch 1883/2000 train_loss: 0.253241 acc: 0.899500\n",
      "Epoch 1884/2000 train_loss: 0.253253 acc: 0.899200\n",
      "Epoch 1885/2000 train_loss: 0.253254 acc: 0.899700\n",
      "Epoch 1886/2000 train_loss: 0.253247 acc: 0.899500\n",
      "Epoch 1887/2000 train_loss: 0.253248 acc: 0.899700\n",
      "Epoch 1888/2000 train_loss: 0.253242 acc: 0.899800\n",
      "Epoch 1889/2000 train_loss: 0.253228 acc: 0.899600\n",
      "Epoch 1890/2000 train_loss: 0.253247 acc: 0.899400\n",
      "Epoch 1891/2000 train_loss: 0.253235 acc: 0.899600\n",
      "Epoch 1892/2000 train_loss: 0.253234 acc: 0.899400\n",
      "Epoch 1893/2000 train_loss: 0.253235 acc: 0.899800\n",
      "Epoch 1894/2000 train_loss: 0.253233 acc: 0.899300\n",
      "Epoch 1895/2000 train_loss: 0.253223 acc: 0.899400\n",
      "Epoch 1896/2000 train_loss: 0.253214 acc: 0.899300\n",
      "Epoch 1897/2000 train_loss: 0.253227 acc: 0.899400\n",
      "Epoch 1898/2000 train_loss: 0.253218 acc: 0.899200\n",
      "Epoch 1899/2000 train_loss: 0.253213 acc: 0.899700\n",
      "Epoch 1900/2000 train_loss: 0.253212 acc: 0.900000\n",
      "Epoch 1901/2000 train_loss: 0.253214 acc: 0.899400\n",
      "Epoch 1902/2000 train_loss: 0.253212 acc: 0.899600\n",
      "Epoch 1903/2000 train_loss: 0.253196 acc: 0.899800\n",
      "Epoch 1904/2000 train_loss: 0.253220 acc: 0.899300\n",
      "Epoch 1905/2000 train_loss: 0.253201 acc: 0.899500\n",
      "Epoch 1906/2000 train_loss: 0.253200 acc: 0.899700\n",
      "Epoch 1907/2000 train_loss: 0.253197 acc: 0.899500\n",
      "Epoch 1908/2000 train_loss: 0.253197 acc: 0.899400\n",
      "Epoch 1909/2000 train_loss: 0.253201 acc: 0.899400\n",
      "Epoch 1910/2000 train_loss: 0.253203 acc: 0.899400\n",
      "Epoch 1911/2000 train_loss: 0.253192 acc: 0.899200\n",
      "Epoch 1912/2000 train_loss: 0.253196 acc: 0.899700\n",
      "Epoch 1913/2000 train_loss: 0.253193 acc: 0.899700\n",
      "Epoch 1914/2000 train_loss: 0.253185 acc: 0.899500\n",
      "Epoch 1915/2000 train_loss: 0.253185 acc: 0.899400\n",
      "Epoch 1916/2000 train_loss: 0.253180 acc: 0.899700\n",
      "Epoch 1917/2000 train_loss: 0.253168 acc: 0.899100\n",
      "Epoch 1918/2000 train_loss: 0.253184 acc: 0.899300\n",
      "Epoch 1919/2000 train_loss: 0.253177 acc: 0.899500\n",
      "Epoch 1920/2000 train_loss: 0.253176 acc: 0.899600\n",
      "Epoch 1921/2000 train_loss: 0.253170 acc: 0.899400\n",
      "Epoch 1922/2000 train_loss: 0.253174 acc: 0.899500\n",
      "Epoch 1923/2000 train_loss: 0.253173 acc: 0.899500\n",
      "Epoch 1924/2000 train_loss: 0.253159 acc: 0.899800\n",
      "Epoch 1925/2000 train_loss: 0.253161 acc: 0.899400\n",
      "Epoch 1926/2000 train_loss: 0.253163 acc: 0.899400\n",
      "Epoch 1927/2000 train_loss: 0.253150 acc: 0.899600\n",
      "Epoch 1928/2000 train_loss: 0.253157 acc: 0.899800\n",
      "Epoch 1929/2000 train_loss: 0.253159 acc: 0.899500\n",
      "Epoch 1930/2000 train_loss: 0.253157 acc: 0.899600\n",
      "Epoch 1931/2000 train_loss: 0.253153 acc: 0.899600\n",
      "Epoch 1932/2000 train_loss: 0.253153 acc: 0.899400\n",
      "Epoch 1933/2000 train_loss: 0.253151 acc: 0.899900\n",
      "Epoch 1934/2000 train_loss: 0.253148 acc: 0.899700\n",
      "Epoch 1935/2000 train_loss: 0.253146 acc: 0.899500\n",
      "Epoch 1936/2000 train_loss: 0.253141 acc: 0.899500\n",
      "Epoch 1937/2000 train_loss: 0.253142 acc: 0.899500\n",
      "Epoch 1938/2000 train_loss: 0.253143 acc: 0.899500\n",
      "Epoch 1939/2000 train_loss: 0.253135 acc: 0.899400\n",
      "Epoch 1940/2000 train_loss: 0.253137 acc: 0.899500\n",
      "Epoch 1941/2000 train_loss: 0.253127 acc: 0.899600\n",
      "Epoch 1942/2000 train_loss: 0.253130 acc: 0.900000\n",
      "Epoch 1943/2000 train_loss: 0.253137 acc: 0.899500\n",
      "Epoch 1944/2000 train_loss: 0.253127 acc: 0.899700\n",
      "Epoch 1945/2000 train_loss: 0.253124 acc: 0.899700\n",
      "Epoch 1946/2000 train_loss: 0.253121 acc: 0.899700\n",
      "Epoch 1947/2000 train_loss: 0.253123 acc: 0.899700\n",
      "Epoch 1948/2000 train_loss: 0.253117 acc: 0.899600\n",
      "Epoch 1949/2000 train_loss: 0.253118 acc: 0.899500\n",
      "Epoch 1950/2000 train_loss: 0.253112 acc: 0.899400\n",
      "Epoch 1951/2000 train_loss: 0.253110 acc: 0.899600\n",
      "Epoch 1952/2000 train_loss: 0.253115 acc: 0.899500\n",
      "Epoch 1953/2000 train_loss: 0.253100 acc: 0.899100\n",
      "Epoch 1954/2000 train_loss: 0.253110 acc: 0.899300\n",
      "Epoch 1955/2000 train_loss: 0.253096 acc: 0.899600\n",
      "Epoch 1956/2000 train_loss: 0.253107 acc: 0.899900\n",
      "Epoch 1957/2000 train_loss: 0.253103 acc: 0.899300\n",
      "Epoch 1958/2000 train_loss: 0.253098 acc: 0.899400\n",
      "Epoch 1959/2000 train_loss: 0.253099 acc: 0.899300\n",
      "Epoch 1960/2000 train_loss: 0.253096 acc: 0.899800\n",
      "Epoch 1961/2000 train_loss: 0.253084 acc: 0.899500\n",
      "Epoch 1962/2000 train_loss: 0.253084 acc: 0.899100\n",
      "Epoch 1963/2000 train_loss: 0.253085 acc: 0.899500\n",
      "Epoch 1964/2000 train_loss: 0.253090 acc: 0.899700\n",
      "Epoch 1965/2000 train_loss: 0.253079 acc: 0.899300\n",
      "Epoch 1966/2000 train_loss: 0.253085 acc: 0.899600\n",
      "Epoch 1967/2000 train_loss: 0.253087 acc: 0.899300\n",
      "Epoch 1968/2000 train_loss: 0.253068 acc: 0.899400\n",
      "Epoch 1969/2000 train_loss: 0.253075 acc: 0.899500\n",
      "Epoch 1970/2000 train_loss: 0.253077 acc: 0.899400\n",
      "Epoch 1971/2000 train_loss: 0.253073 acc: 0.899200\n",
      "Epoch 1972/2000 train_loss: 0.253075 acc: 0.899500\n",
      "Epoch 1973/2000 train_loss: 0.253062 acc: 0.899600\n",
      "Epoch 1974/2000 train_loss: 0.253062 acc: 0.899300\n",
      "Epoch 1975/2000 train_loss: 0.253063 acc: 0.899500\n",
      "Epoch 1976/2000 train_loss: 0.253067 acc: 0.899400\n",
      "Epoch 1977/2000 train_loss: 0.253056 acc: 0.899700\n",
      "Epoch 1978/2000 train_loss: 0.253057 acc: 0.899500\n",
      "Epoch 1979/2000 train_loss: 0.253035 acc: 0.899700\n",
      "Epoch 1980/2000 train_loss: 0.253056 acc: 0.899700\n",
      "Epoch 1981/2000 train_loss: 0.253051 acc: 0.899600\n",
      "Epoch 1982/2000 train_loss: 0.253053 acc: 0.899500\n",
      "Epoch 1983/2000 train_loss: 0.253043 acc: 0.899600\n",
      "Epoch 1984/2000 train_loss: 0.253054 acc: 0.899600\n",
      "Epoch 1985/2000 train_loss: 0.253049 acc: 0.899800\n",
      "Epoch 1986/2000 train_loss: 0.253049 acc: 0.899400\n",
      "Epoch 1987/2000 train_loss: 0.253042 acc: 0.899500\n",
      "Epoch 1988/2000 train_loss: 0.253034 acc: 0.899600\n",
      "Epoch 1989/2000 train_loss: 0.253035 acc: 0.899500\n",
      "Epoch 1990/2000 train_loss: 0.253033 acc: 0.899400\n",
      "Epoch 1991/2000 train_loss: 0.253023 acc: 0.899600\n",
      "Epoch 1992/2000 train_loss: 0.253028 acc: 0.899400\n",
      "Epoch 1993/2000 train_loss: 0.253021 acc: 0.899100\n",
      "Epoch 1994/2000 train_loss: 0.253031 acc: 0.899800\n",
      "Epoch 1995/2000 train_loss: 0.253028 acc: 0.899100\n",
      "Epoch 1996/2000 train_loss: 0.253019 acc: 0.899600\n",
      "Epoch 1997/2000 train_loss: 0.253024 acc: 0.899500\n",
      "Epoch 1998/2000 train_loss: 0.253016 acc: 0.899500\n",
      "Epoch 1999/2000 train_loss: 0.253009 acc: 0.899600\n",
      "Epoch 2000/2000 train_loss: 0.253004 acc: 0.899200\n"
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    loss_sum = 0.0\n",
    "    test_sum = 0.0\n",
    "\n",
    "    metric = BinaryAccuracy()\n",
    "\n",
    "    for (train_features,train_labels),(test_features,test_labels) in zip(train_dataloader,test_dataloader) :\n",
    "\n",
    "        epoch_y = net(train_features)\n",
    "        loss = loss_func(epoch_y.to(torch.float32),train_labels.to(torch.float32))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 100 == 0 :\n",
    "            for m in net.modules():\n",
    "                if isinstance(m,nn.Linear):\n",
    "                    \n",
    "                    img_tensor = m.weight.clone().detach()\n",
    "                    img = resize_tensor(img_tensor.squeeze())\n",
    "                    \n",
    "                    img = img.reshape(28,28)\n",
    "                    np_arr = np.array(img,dtype=np.uint8)\n",
    "                                        \n",
    "                    writer.add_image('weight_to_img',np_arr,epoch + 1,dataformats='WH')\n",
    "\n",
    "        loss_sum += loss\n",
    "        \n",
    "        epoch_test_y = net(test_features)\n",
    "        \n",
    "        result_set = []\n",
    "        \n",
    "        for result in epoch_test_y:\n",
    "            pre = result.item()\n",
    "            \n",
    "            if pre <= 0.5:\n",
    "                result_set.append(np.array([0]))\n",
    "            else:\n",
    "                result_set.append(np.array([1]))\n",
    "                \n",
    "        result_set = np.array(result_set)\n",
    "        result_set = torch.tensor(result_set)\n",
    "        \n",
    "        metric.update(result_set.squeeze(),test_labels.squeeze())\n",
    "    acc = metric.compute()\n",
    "\n",
    "    loss = loss_sum / len(train_dataloader)\n",
    "\n",
    "    print('Epoch {:4d}/{} train_loss: {:.6f} acc: {:.6f}'.format(\n",
    "        epoch+1, epochs, loss, acc\n",
    "    ))\n",
    "\n",
    "    if loss < 0.1 :\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27525/3745704701.py:21: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  test_origin = torch.tensor(test_y)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.8994)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result = []\n",
    "\n",
    "test_y = []\n",
    "\n",
    "final_metric = BinaryAccuracy()\n",
    "\n",
    "for x ,y in test_dataset:\n",
    "    pre = net(torch.tensor(x))\n",
    "    \n",
    "    if pre <= 0.5:\n",
    "        test_result.append(np.array([0]))\n",
    "    else:\n",
    "        test_result.append(np.array([1]))\n",
    "        \n",
    "    test_y.append(y)\n",
    "    \n",
    "test_result = np.array(test_result)\n",
    "test_result = torch.tensor(test_result)\n",
    "\n",
    "test_origin = np.array(test_y)\n",
    "test_origin = torch.tensor(test_y)\n",
    "\n",
    "final_metric.update(test_result.squeeze(),test_origin.squeeze())\n",
    "acc = final_metric.compute()\n",
    "\n",
    "acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
