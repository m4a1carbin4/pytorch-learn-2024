{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torcheval.metrics import BinaryAccuracy\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "data_train = datasets.MNIST(root=\"./dataset\",train=True,download=True,transform=transforms.ToTensor())\n",
    "data_test = datasets.MNIST(root=\"./dataset\",train=False,download=True,transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "image, label = data_train[0]\n",
    "\n",
    "plt.imshow(image.squeeze().numpy(),cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = []\n",
    "\n",
    "for x , y in data_train:\n",
    "\n",
    "    list_temp = [0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "    list_temp[y] = 1\n",
    "    \n",
    "    y = np.array(list_temp)\n",
    "\n",
    "    x = x.squeeze().reshape(-1).numpy()\n",
    "\n",
    "    train_dataset.append([x,y])\n",
    "\n",
    "test_dataset = []\n",
    "\n",
    "for x , y in data_test:\n",
    "\n",
    "    list_temp = [0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "    list_temp[y] = 1\n",
    "    \n",
    "    y = np.array(list_temp)\n",
    "\n",
    "    x = x.squeeze().reshape(-1).numpy()\n",
    "\n",
    "    test_dataset.append([x,y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import dataset, DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=600,shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=100,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class HiddenSoftMaxNet(nn.Module):\n",
    "    \n",
    "    def __init__(self,hidden_layer:list=[]):\n",
    "        super(HiddenSoftMaxNet, self).__init__()\n",
    "        \n",
    "        self.fc = []\n",
    "        self.af = []\n",
    "        \n",
    "        if len(hidden_layer) == 0:\n",
    "            self.fc.append(nn.Linear(784,10))\n",
    "        else :\n",
    "            input_current = 784\n",
    "            \n",
    "            for num_class in hidden_layer :\n",
    "                self.fc.append(nn.Linear(input_current,num_class))\n",
    "                self.af.append(nn.ReLU())\n",
    "                input_current = num_class\n",
    "                \n",
    "            self.fc.append(nn.Linear(input_current,10))\n",
    "            self.af.append(nn.Softmax(1))\n",
    "            \n",
    "        self.fc = nn.ParameterList(self.fc)\n",
    "        self.af = nn.ParameterList(self.af)\n",
    "        \n",
    "    def forward(self,input):\n",
    "        \n",
    "        rfc = input\n",
    "        \n",
    "        for func,afunc in zip(self.fc,self.af):\n",
    "            rfc = func(rfc)\n",
    "            rfc = afunc(rfc) \n",
    "            \n",
    "        output = rfc\n",
    "        \n",
    "        return output\n",
    "    \n",
    "net = HiddenSoftMaxNet([128,64,32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(),lr=0.01)\n",
    "\n",
    "loss_fc = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_tensor(tensor):\n",
    "    \n",
    "    v_min, v_max = tensor.min(), tensor.max()\n",
    "    \n",
    "    new_min,new_max = 0,255\n",
    "    \n",
    "    v_p = (tensor - v_min)/(v_max - v_min)*(new_max - new_min) + new_min\n",
    "    \n",
    "    return v_p\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheval.metrics import MulticlassAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/2000 train_loss: 2.302369 acc: 0.113500\n",
      "Epoch    2/2000 train_loss: 2.302296 acc: 0.113500\n",
      "Epoch    3/2000 train_loss: 2.302223 acc: 0.113500\n",
      "Epoch    4/2000 train_loss: 2.302149 acc: 0.113500\n",
      "Epoch    5/2000 train_loss: 2.302073 acc: 0.113500\n",
      "Epoch    6/2000 train_loss: 2.301997 acc: 0.113500\n",
      "Epoch    7/2000 train_loss: 2.301919 acc: 0.113500\n",
      "Epoch    8/2000 train_loss: 2.301839 acc: 0.113500\n",
      "Epoch    9/2000 train_loss: 2.301758 acc: 0.113500\n",
      "Epoch   10/2000 train_loss: 2.301674 acc: 0.113500\n",
      "Epoch   11/2000 train_loss: 2.301589 acc: 0.113500\n",
      "Epoch   12/2000 train_loss: 2.301500 acc: 0.113500\n",
      "Epoch   13/2000 train_loss: 2.301408 acc: 0.113500\n",
      "Epoch   14/2000 train_loss: 2.301313 acc: 0.113500\n",
      "Epoch   15/2000 train_loss: 2.301215 acc: 0.113500\n",
      "Epoch   16/2000 train_loss: 2.301110 acc: 0.113500\n",
      "Epoch   17/2000 train_loss: 2.301003 acc: 0.113500\n",
      "Epoch   18/2000 train_loss: 2.300889 acc: 0.113500\n",
      "Epoch   19/2000 train_loss: 2.300770 acc: 0.113500\n",
      "Epoch   20/2000 train_loss: 2.300644 acc: 0.113500\n",
      "Epoch   21/2000 train_loss: 2.300512 acc: 0.113500\n",
      "Epoch   22/2000 train_loss: 2.300373 acc: 0.113500\n",
      "Epoch   23/2000 train_loss: 2.300228 acc: 0.113700\n",
      "Epoch   24/2000 train_loss: 2.300076 acc: 0.113800\n",
      "Epoch   25/2000 train_loss: 2.299916 acc: 0.113900\n",
      "Epoch   26/2000 train_loss: 2.299750 acc: 0.114600\n",
      "Epoch   27/2000 train_loss: 2.299577 acc: 0.115000\n",
      "Epoch   28/2000 train_loss: 2.299395 acc: 0.117700\n",
      "Epoch   29/2000 train_loss: 2.299205 acc: 0.123000\n",
      "Epoch   30/2000 train_loss: 2.299007 acc: 0.127600\n",
      "Epoch   31/2000 train_loss: 2.298798 acc: 0.134800\n",
      "Epoch   32/2000 train_loss: 2.298578 acc: 0.142900\n",
      "Epoch   33/2000 train_loss: 2.298347 acc: 0.150700\n",
      "Epoch   34/2000 train_loss: 2.298102 acc: 0.159900\n",
      "Epoch   35/2000 train_loss: 2.297843 acc: 0.167500\n",
      "Epoch   36/2000 train_loss: 2.297566 acc: 0.177400\n",
      "Epoch   37/2000 train_loss: 2.297272 acc: 0.187300\n",
      "Epoch   38/2000 train_loss: 2.296957 acc: 0.195000\n",
      "Epoch   39/2000 train_loss: 2.296619 acc: 0.203700\n",
      "Epoch   40/2000 train_loss: 2.296253 acc: 0.211100\n",
      "Epoch   41/2000 train_loss: 2.295857 acc: 0.218000\n",
      "Epoch   42/2000 train_loss: 2.295426 acc: 0.223500\n",
      "Epoch   43/2000 train_loss: 2.294954 acc: 0.230000\n",
      "Epoch   44/2000 train_loss: 2.294434 acc: 0.236000\n",
      "Epoch   45/2000 train_loss: 2.293857 acc: 0.240400\n",
      "Epoch   46/2000 train_loss: 2.293212 acc: 0.245100\n",
      "Epoch   47/2000 train_loss: 2.292486 acc: 0.250200\n",
      "Epoch   48/2000 train_loss: 2.291656 acc: 0.253400\n",
      "Epoch   49/2000 train_loss: 2.290698 acc: 0.257600\n",
      "Epoch   50/2000 train_loss: 2.289572 acc: 0.258800\n",
      "Epoch   51/2000 train_loss: 2.288220 acc: 0.260200\n",
      "Epoch   52/2000 train_loss: 2.286551 acc: 0.257900\n",
      "Epoch   53/2000 train_loss: 2.284427 acc: 0.254500\n",
      "Epoch   54/2000 train_loss: 2.281612 acc: 0.252300\n",
      "Epoch   55/2000 train_loss: 2.277727 acc: 0.243300\n",
      "Epoch   56/2000 train_loss: 2.272253 acc: 0.233000\n",
      "Epoch   57/2000 train_loss: 2.264923 acc: 0.223500\n",
      "Epoch   58/2000 train_loss: 2.256671 acc: 0.219000\n",
      "Epoch   59/2000 train_loss: 2.249144 acc: 0.219300\n",
      "Epoch   60/2000 train_loss: 2.242857 acc: 0.222700\n",
      "Epoch   61/2000 train_loss: 2.237470 acc: 0.227600\n",
      "Epoch   62/2000 train_loss: 2.232624 acc: 0.233300\n",
      "Epoch   63/2000 train_loss: 2.228056 acc: 0.237500\n",
      "Epoch   64/2000 train_loss: 2.223533 acc: 0.240900\n",
      "Epoch   65/2000 train_loss: 2.218823 acc: 0.244900\n",
      "Epoch   66/2000 train_loss: 2.213670 acc: 0.251600\n",
      "Epoch   67/2000 train_loss: 2.207796 acc: 0.257300\n",
      "Epoch   68/2000 train_loss: 2.200954 acc: 0.263900\n",
      "Epoch   69/2000 train_loss: 2.192996 acc: 0.270400\n",
      "Epoch   70/2000 train_loss: 2.183964 acc: 0.278800\n",
      "Epoch   71/2000 train_loss: 2.174094 acc: 0.288000\n",
      "Epoch   72/2000 train_loss: 2.163752 acc: 0.299500\n",
      "Epoch   73/2000 train_loss: 2.153340 acc: 0.311700\n",
      "Epoch   74/2000 train_loss: 2.143244 acc: 0.323900\n",
      "Epoch   75/2000 train_loss: 2.133626 acc: 0.336400\n",
      "Epoch   76/2000 train_loss: 2.124472 acc: 0.345700\n",
      "Epoch   77/2000 train_loss: 2.115628 acc: 0.357600\n",
      "Epoch   78/2000 train_loss: 2.106865 acc: 0.366300\n",
      "Epoch   79/2000 train_loss: 2.097926 acc: 0.375400\n",
      "Epoch   80/2000 train_loss: 2.088597 acc: 0.381800\n",
      "Epoch   81/2000 train_loss: 2.078825 acc: 0.386400\n",
      "Epoch   82/2000 train_loss: 2.068706 acc: 0.391400\n",
      "Epoch   83/2000 train_loss: 2.058371 acc: 0.397300\n",
      "Epoch   84/2000 train_loss: 2.047570 acc: 0.408000\n",
      "Epoch   85/2000 train_loss: 2.035724 acc: 0.434500\n",
      "Epoch   86/2000 train_loss: 2.022251 acc: 0.476500\n",
      "Epoch   87/2000 train_loss: 2.007455 acc: 0.502400\n",
      "Epoch   88/2000 train_loss: 1.992781 acc: 0.513600\n",
      "Epoch   89/2000 train_loss: 1.978988 acc: 0.523200\n",
      "Epoch   90/2000 train_loss: 1.965664 acc: 0.531200\n",
      "Epoch   91/2000 train_loss: 1.952247 acc: 0.545300\n",
      "Epoch   92/2000 train_loss: 1.938425 acc: 0.577800\n",
      "Epoch   93/2000 train_loss: 1.924268 acc: 0.611400\n",
      "Epoch   94/2000 train_loss: 1.910308 acc: 0.630200\n",
      "Epoch   95/2000 train_loss: 1.896875 acc: 0.643700\n",
      "Epoch   96/2000 train_loss: 1.884266 acc: 0.652300\n",
      "Epoch   97/2000 train_loss: 1.872631 acc: 0.659700\n",
      "Epoch   98/2000 train_loss: 1.862043 acc: 0.665000\n",
      "Epoch   99/2000 train_loss: 1.852529 acc: 0.668600\n",
      "Epoch  100/2000 train_loss: 1.843953 acc: 0.672500\n",
      "Epoch  101/2000 train_loss: 1.836303 acc: 0.675700\n",
      "Epoch  102/2000 train_loss: 1.829369 acc: 0.677100\n",
      "Epoch  103/2000 train_loss: 1.823101 acc: 0.679700\n",
      "Epoch  104/2000 train_loss: 1.817332 acc: 0.682700\n",
      "Epoch  105/2000 train_loss: 1.811978 acc: 0.684600\n",
      "Epoch  106/2000 train_loss: 1.806858 acc: 0.688200\n",
      "Epoch  107/2000 train_loss: 1.801910 acc: 0.691700\n",
      "Epoch  108/2000 train_loss: 1.797045 acc: 0.695700\n",
      "Epoch  109/2000 train_loss: 1.792236 acc: 0.702700\n",
      "Epoch  110/2000 train_loss: 1.787324 acc: 0.710600\n",
      "Epoch  111/2000 train_loss: 1.782323 acc: 0.718300\n",
      "Epoch  112/2000 train_loss: 1.777152 acc: 0.723000\n",
      "Epoch  113/2000 train_loss: 1.771728 acc: 0.729500\n",
      "Epoch  114/2000 train_loss: 1.766131 acc: 0.737500\n",
      "Epoch  115/2000 train_loss: 1.760343 acc: 0.744900\n",
      "Epoch  116/2000 train_loss: 1.754449 acc: 0.750600\n",
      "Epoch  117/2000 train_loss: 1.748526 acc: 0.755700\n",
      "Epoch  118/2000 train_loss: 1.742685 acc: 0.761100\n",
      "Epoch  119/2000 train_loss: 1.737035 acc: 0.764600\n",
      "Epoch  120/2000 train_loss: 1.731679 acc: 0.768500\n",
      "Epoch  121/2000 train_loss: 1.726651 acc: 0.772300\n",
      "Epoch  122/2000 train_loss: 1.722052 acc: 0.776400\n",
      "Epoch  123/2000 train_loss: 1.717859 acc: 0.778600\n",
      "Epoch  124/2000 train_loss: 1.714076 acc: 0.780900\n",
      "Epoch  125/2000 train_loss: 1.710673 acc: 0.781600\n",
      "Epoch  126/2000 train_loss: 1.707569 acc: 0.784900\n",
      "Epoch  127/2000 train_loss: 1.704793 acc: 0.785700\n",
      "Epoch  128/2000 train_loss: 1.702255 acc: 0.787000\n",
      "Epoch  129/2000 train_loss: 1.699976 acc: 0.788000\n",
      "Epoch  130/2000 train_loss: 1.697882 acc: 0.789300\n",
      "Epoch  131/2000 train_loss: 1.695958 acc: 0.789600\n",
      "Epoch  132/2000 train_loss: 1.694211 acc: 0.790400\n",
      "Epoch  133/2000 train_loss: 1.692563 acc: 0.790900\n",
      "Epoch  134/2000 train_loss: 1.691024 acc: 0.793400\n",
      "Epoch  135/2000 train_loss: 1.689619 acc: 0.792300\n",
      "Epoch  136/2000 train_loss: 1.688290 acc: 0.792900\n",
      "Epoch  137/2000 train_loss: 1.687042 acc: 0.793900\n",
      "Epoch  138/2000 train_loss: 1.685830 acc: 0.794400\n",
      "Epoch  139/2000 train_loss: 1.684739 acc: 0.795600\n",
      "Epoch  140/2000 train_loss: 1.683662 acc: 0.795400\n",
      "Epoch  141/2000 train_loss: 1.682639 acc: 0.795500\n",
      "Epoch  142/2000 train_loss: 1.681671 acc: 0.796600\n",
      "Epoch  143/2000 train_loss: 1.680766 acc: 0.796500\n",
      "Epoch  144/2000 train_loss: 1.679863 acc: 0.797000\n",
      "Epoch  145/2000 train_loss: 1.679010 acc: 0.796900\n",
      "Epoch  146/2000 train_loss: 1.678201 acc: 0.797400\n",
      "Epoch  147/2000 train_loss: 1.677384 acc: 0.797500\n",
      "Epoch  148/2000 train_loss: 1.676568 acc: 0.797800\n",
      "Epoch  149/2000 train_loss: 1.675875 acc: 0.798700\n",
      "Epoch  150/2000 train_loss: 1.675145 acc: 0.798600\n",
      "Epoch  151/2000 train_loss: 1.674446 acc: 0.799400\n",
      "Epoch  152/2000 train_loss: 1.673771 acc: 0.799100\n",
      "Epoch  153/2000 train_loss: 1.673109 acc: 0.799600\n",
      "Epoch  154/2000 train_loss: 1.672446 acc: 0.800400\n",
      "Epoch  155/2000 train_loss: 1.671840 acc: 0.800300\n",
      "Epoch  156/2000 train_loss: 1.671229 acc: 0.801100\n",
      "Epoch  157/2000 train_loss: 1.670629 acc: 0.801200\n",
      "Epoch  158/2000 train_loss: 1.670060 acc: 0.801500\n",
      "Epoch  159/2000 train_loss: 1.669503 acc: 0.802200\n",
      "Epoch  160/2000 train_loss: 1.668953 acc: 0.802900\n",
      "Epoch  161/2000 train_loss: 1.668407 acc: 0.803800\n",
      "Epoch  162/2000 train_loss: 1.667884 acc: 0.803400\n",
      "Epoch  163/2000 train_loss: 1.667365 acc: 0.803800\n",
      "Epoch  164/2000 train_loss: 1.666855 acc: 0.804500\n",
      "Epoch  165/2000 train_loss: 1.666359 acc: 0.804700\n",
      "Epoch  166/2000 train_loss: 1.665888 acc: 0.805200\n",
      "Epoch  167/2000 train_loss: 1.665377 acc: 0.805000\n",
      "Epoch  168/2000 train_loss: 1.664936 acc: 0.806200\n",
      "Epoch  169/2000 train_loss: 1.664447 acc: 0.806300\n",
      "Epoch  170/2000 train_loss: 1.664037 acc: 0.807200\n",
      "Epoch  171/2000 train_loss: 1.663575 acc: 0.806400\n",
      "Epoch  172/2000 train_loss: 1.663133 acc: 0.807400\n",
      "Epoch  173/2000 train_loss: 1.662693 acc: 0.808100\n",
      "Epoch  174/2000 train_loss: 1.662299 acc: 0.807800\n",
      "Epoch  175/2000 train_loss: 1.661851 acc: 0.807900\n",
      "Epoch  176/2000 train_loss: 1.661454 acc: 0.808500\n",
      "Epoch  177/2000 train_loss: 1.661065 acc: 0.808800\n",
      "Epoch  178/2000 train_loss: 1.660633 acc: 0.809000\n",
      "Epoch  179/2000 train_loss: 1.660267 acc: 0.809000\n",
      "Epoch  180/2000 train_loss: 1.659896 acc: 0.809000\n",
      "Epoch  181/2000 train_loss: 1.659491 acc: 0.809700\n",
      "Epoch  182/2000 train_loss: 1.659128 acc: 0.810000\n",
      "Epoch  183/2000 train_loss: 1.658765 acc: 0.809700\n",
      "Epoch  184/2000 train_loss: 1.658389 acc: 0.810200\n",
      "Epoch  185/2000 train_loss: 1.658002 acc: 0.810900\n",
      "Epoch  186/2000 train_loss: 1.657680 acc: 0.811200\n",
      "Epoch  187/2000 train_loss: 1.657301 acc: 0.810900\n",
      "Epoch  188/2000 train_loss: 1.656967 acc: 0.810800\n",
      "Epoch  189/2000 train_loss: 1.656634 acc: 0.810800\n",
      "Epoch  190/2000 train_loss: 1.656291 acc: 0.812400\n",
      "Epoch  191/2000 train_loss: 1.655956 acc: 0.811600\n",
      "Epoch  192/2000 train_loss: 1.655638 acc: 0.812100\n",
      "Epoch  193/2000 train_loss: 1.655294 acc: 0.811900\n",
      "Epoch  194/2000 train_loss: 1.655007 acc: 0.812000\n",
      "Epoch  195/2000 train_loss: 1.654676 acc: 0.812400\n",
      "Epoch  196/2000 train_loss: 1.654359 acc: 0.812600\n",
      "Epoch  197/2000 train_loss: 1.654052 acc: 0.812500\n",
      "Epoch  198/2000 train_loss: 1.653743 acc: 0.813600\n",
      "Epoch  199/2000 train_loss: 1.653420 acc: 0.813200\n",
      "Epoch  200/2000 train_loss: 1.653143 acc: 0.814200\n",
      "Epoch  201/2000 train_loss: 1.652839 acc: 0.814300\n",
      "Epoch  202/2000 train_loss: 1.652556 acc: 0.814800\n",
      "Epoch  203/2000 train_loss: 1.652251 acc: 0.814200\n",
      "Epoch  204/2000 train_loss: 1.651963 acc: 0.814500\n",
      "Epoch  205/2000 train_loss: 1.651676 acc: 0.814800\n",
      "Epoch  206/2000 train_loss: 1.651415 acc: 0.814800\n",
      "Epoch  207/2000 train_loss: 1.651129 acc: 0.815900\n",
      "Epoch  208/2000 train_loss: 1.650854 acc: 0.815400\n",
      "Epoch  209/2000 train_loss: 1.650591 acc: 0.816400\n",
      "Epoch  210/2000 train_loss: 1.650314 acc: 0.816000\n",
      "Epoch  211/2000 train_loss: 1.650034 acc: 0.815900\n",
      "Epoch  212/2000 train_loss: 1.649771 acc: 0.816200\n",
      "Epoch  213/2000 train_loss: 1.649502 acc: 0.816600\n",
      "Epoch  214/2000 train_loss: 1.649240 acc: 0.816400\n",
      "Epoch  215/2000 train_loss: 1.649003 acc: 0.816700\n",
      "Epoch  216/2000 train_loss: 1.648744 acc: 0.817400\n",
      "Epoch  217/2000 train_loss: 1.648475 acc: 0.817300\n",
      "Epoch  218/2000 train_loss: 1.648225 acc: 0.816600\n",
      "Epoch  219/2000 train_loss: 1.647985 acc: 0.817400\n",
      "Epoch  220/2000 train_loss: 1.647744 acc: 0.817500\n",
      "Epoch  221/2000 train_loss: 1.647473 acc: 0.816800\n",
      "Epoch  222/2000 train_loss: 1.647244 acc: 0.818600\n",
      "Epoch  223/2000 train_loss: 1.647027 acc: 0.818600\n",
      "Epoch  224/2000 train_loss: 1.646768 acc: 0.818700\n",
      "Epoch  225/2000 train_loss: 1.646539 acc: 0.819500\n",
      "Epoch  226/2000 train_loss: 1.646323 acc: 0.819000\n",
      "Epoch  227/2000 train_loss: 1.646075 acc: 0.818700\n",
      "Epoch  228/2000 train_loss: 1.645833 acc: 0.819600\n",
      "Epoch  229/2000 train_loss: 1.645598 acc: 0.819400\n",
      "Epoch  230/2000 train_loss: 1.645378 acc: 0.819600\n",
      "Epoch  231/2000 train_loss: 1.645167 acc: 0.820400\n",
      "Epoch  232/2000 train_loss: 1.644951 acc: 0.820400\n",
      "Epoch  233/2000 train_loss: 1.644730 acc: 0.820100\n",
      "Epoch  234/2000 train_loss: 1.644517 acc: 0.820300\n",
      "Epoch  235/2000 train_loss: 1.644316 acc: 0.820600\n",
      "Epoch  236/2000 train_loss: 1.644099 acc: 0.821700\n",
      "Epoch  237/2000 train_loss: 1.643879 acc: 0.821100\n",
      "Epoch  238/2000 train_loss: 1.643669 acc: 0.821200\n",
      "Epoch  239/2000 train_loss: 1.643464 acc: 0.821600\n",
      "Epoch  240/2000 train_loss: 1.643264 acc: 0.821500\n",
      "Epoch  241/2000 train_loss: 1.643078 acc: 0.821600\n",
      "Epoch  242/2000 train_loss: 1.642862 acc: 0.821500\n",
      "Epoch  243/2000 train_loss: 1.642682 acc: 0.823000\n",
      "Epoch  244/2000 train_loss: 1.642458 acc: 0.822600\n",
      "Epoch  245/2000 train_loss: 1.642283 acc: 0.822200\n",
      "Epoch  246/2000 train_loss: 1.642073 acc: 0.822200\n",
      "Epoch  247/2000 train_loss: 1.641921 acc: 0.823000\n",
      "Epoch  248/2000 train_loss: 1.641706 acc: 0.822600\n",
      "Epoch  249/2000 train_loss: 1.641520 acc: 0.823200\n",
      "Epoch  250/2000 train_loss: 1.641332 acc: 0.823300\n",
      "Epoch  251/2000 train_loss: 1.641153 acc: 0.822500\n",
      "Epoch  252/2000 train_loss: 1.640986 acc: 0.823200\n",
      "Epoch  253/2000 train_loss: 1.640784 acc: 0.823800\n",
      "Epoch  254/2000 train_loss: 1.640610 acc: 0.824000\n",
      "Epoch  255/2000 train_loss: 1.640433 acc: 0.824100\n",
      "Epoch  256/2000 train_loss: 1.640258 acc: 0.824100\n",
      "Epoch  257/2000 train_loss: 1.640059 acc: 0.824200\n",
      "Epoch  258/2000 train_loss: 1.639920 acc: 0.824000\n",
      "Epoch  259/2000 train_loss: 1.639726 acc: 0.824700\n",
      "Epoch  260/2000 train_loss: 1.639561 acc: 0.824300\n",
      "Epoch  261/2000 train_loss: 1.639402 acc: 0.825200\n",
      "Epoch  262/2000 train_loss: 1.639235 acc: 0.825300\n",
      "Epoch  263/2000 train_loss: 1.639063 acc: 0.824800\n",
      "Epoch  264/2000 train_loss: 1.638903 acc: 0.825100\n",
      "Epoch  265/2000 train_loss: 1.638734 acc: 0.825000\n",
      "Epoch  266/2000 train_loss: 1.638587 acc: 0.825000\n",
      "Epoch  267/2000 train_loss: 1.638401 acc: 0.825700\n",
      "Epoch  268/2000 train_loss: 1.638252 acc: 0.825800\n",
      "Epoch  269/2000 train_loss: 1.638079 acc: 0.826200\n",
      "Epoch  270/2000 train_loss: 1.637920 acc: 0.825600\n",
      "Epoch  271/2000 train_loss: 1.637768 acc: 0.825900\n",
      "Epoch  272/2000 train_loss: 1.637627 acc: 0.826700\n",
      "Epoch  273/2000 train_loss: 1.637469 acc: 0.826500\n",
      "Epoch  274/2000 train_loss: 1.637320 acc: 0.826400\n",
      "Epoch  275/2000 train_loss: 1.637139 acc: 0.826400\n",
      "Epoch  276/2000 train_loss: 1.637004 acc: 0.826300\n",
      "Epoch  277/2000 train_loss: 1.636830 acc: 0.827200\n",
      "Epoch  278/2000 train_loss: 1.636703 acc: 0.826700\n",
      "Epoch  279/2000 train_loss: 1.636552 acc: 0.827000\n",
      "Epoch  280/2000 train_loss: 1.636402 acc: 0.827100\n",
      "Epoch  281/2000 train_loss: 1.636249 acc: 0.827100\n",
      "Epoch  282/2000 train_loss: 1.636088 acc: 0.828000\n",
      "Epoch  283/2000 train_loss: 1.635961 acc: 0.827200\n",
      "Epoch  284/2000 train_loss: 1.635803 acc: 0.827500\n",
      "Epoch  285/2000 train_loss: 1.635648 acc: 0.827200\n",
      "Epoch  286/2000 train_loss: 1.635507 acc: 0.827200\n",
      "Epoch  287/2000 train_loss: 1.635375 acc: 0.828500\n",
      "Epoch  288/2000 train_loss: 1.635236 acc: 0.827900\n",
      "Epoch  289/2000 train_loss: 1.635082 acc: 0.828300\n",
      "Epoch  290/2000 train_loss: 1.634929 acc: 0.828800\n",
      "Epoch  291/2000 train_loss: 1.634810 acc: 0.828700\n",
      "Epoch  292/2000 train_loss: 1.634663 acc: 0.829100\n",
      "Epoch  293/2000 train_loss: 1.634518 acc: 0.829100\n",
      "Epoch  294/2000 train_loss: 1.634379 acc: 0.828200\n",
      "Epoch  295/2000 train_loss: 1.634238 acc: 0.829200\n",
      "Epoch  296/2000 train_loss: 1.634113 acc: 0.829600\n",
      "Epoch  297/2000 train_loss: 1.633960 acc: 0.830100\n",
      "Epoch  298/2000 train_loss: 1.633829 acc: 0.829500\n",
      "Epoch  299/2000 train_loss: 1.633700 acc: 0.829800\n",
      "Epoch  300/2000 train_loss: 1.633545 acc: 0.829700\n",
      "Epoch  301/2000 train_loss: 1.633417 acc: 0.829700\n",
      "Epoch  302/2000 train_loss: 1.633305 acc: 0.830000\n",
      "Epoch  303/2000 train_loss: 1.633146 acc: 0.830500\n",
      "Epoch  304/2000 train_loss: 1.633015 acc: 0.830900\n",
      "Epoch  305/2000 train_loss: 1.632876 acc: 0.830500\n",
      "Epoch  306/2000 train_loss: 1.632744 acc: 0.829600\n",
      "Epoch  307/2000 train_loss: 1.632613 acc: 0.830500\n",
      "Epoch  308/2000 train_loss: 1.632473 acc: 0.831100\n",
      "Epoch  309/2000 train_loss: 1.632372 acc: 0.831000\n",
      "Epoch  310/2000 train_loss: 1.632215 acc: 0.831100\n",
      "Epoch  311/2000 train_loss: 1.632093 acc: 0.831300\n",
      "Epoch  312/2000 train_loss: 1.631967 acc: 0.832200\n",
      "Epoch  313/2000 train_loss: 1.631828 acc: 0.831300\n",
      "Epoch  314/2000 train_loss: 1.631706 acc: 0.831400\n",
      "Epoch  315/2000 train_loss: 1.631590 acc: 0.831800\n",
      "Epoch  316/2000 train_loss: 1.631448 acc: 0.831900\n",
      "Epoch  317/2000 train_loss: 1.631327 acc: 0.832000\n",
      "Epoch  318/2000 train_loss: 1.631159 acc: 0.832200\n",
      "Epoch  319/2000 train_loss: 1.631085 acc: 0.831700\n",
      "Epoch  320/2000 train_loss: 1.630974 acc: 0.832300\n",
      "Epoch  321/2000 train_loss: 1.630803 acc: 0.832200\n",
      "Epoch  322/2000 train_loss: 1.630684 acc: 0.832600\n",
      "Epoch  323/2000 train_loss: 1.630600 acc: 0.832200\n",
      "Epoch  324/2000 train_loss: 1.630458 acc: 0.832200\n",
      "Epoch  325/2000 train_loss: 1.630345 acc: 0.833000\n",
      "Epoch  326/2000 train_loss: 1.630224 acc: 0.832900\n",
      "Epoch  327/2000 train_loss: 1.630102 acc: 0.833300\n",
      "Epoch  328/2000 train_loss: 1.629960 acc: 0.833400\n",
      "Epoch  329/2000 train_loss: 1.629844 acc: 0.833200\n",
      "Epoch  330/2000 train_loss: 1.629746 acc: 0.833400\n",
      "Epoch  331/2000 train_loss: 1.629625 acc: 0.833100\n",
      "Epoch  332/2000 train_loss: 1.629519 acc: 0.833100\n",
      "Epoch  333/2000 train_loss: 1.629395 acc: 0.833900\n",
      "Epoch  334/2000 train_loss: 1.629257 acc: 0.834000\n",
      "Epoch  335/2000 train_loss: 1.629133 acc: 0.834000\n",
      "Epoch  336/2000 train_loss: 1.629045 acc: 0.834600\n",
      "Epoch  337/2000 train_loss: 1.628914 acc: 0.833500\n",
      "Epoch  338/2000 train_loss: 1.628794 acc: 0.834200\n",
      "Epoch  339/2000 train_loss: 1.628669 acc: 0.834900\n",
      "Epoch  340/2000 train_loss: 1.628564 acc: 0.834600\n",
      "Epoch  341/2000 train_loss: 1.628453 acc: 0.834500\n",
      "Epoch  342/2000 train_loss: 1.628350 acc: 0.834700\n",
      "Epoch  343/2000 train_loss: 1.628242 acc: 0.834900\n",
      "Epoch  344/2000 train_loss: 1.628114 acc: 0.834800\n",
      "Epoch  345/2000 train_loss: 1.628009 acc: 0.834700\n",
      "Epoch  346/2000 train_loss: 1.627894 acc: 0.835000\n",
      "Epoch  347/2000 train_loss: 1.627784 acc: 0.834800\n",
      "Epoch  348/2000 train_loss: 1.627669 acc: 0.834900\n",
      "Epoch  349/2000 train_loss: 1.627560 acc: 0.835100\n",
      "Epoch  350/2000 train_loss: 1.627448 acc: 0.835600\n",
      "Epoch  351/2000 train_loss: 1.627331 acc: 0.835800\n",
      "Epoch  352/2000 train_loss: 1.627246 acc: 0.835800\n",
      "Epoch  353/2000 train_loss: 1.627117 acc: 0.836200\n",
      "Epoch  354/2000 train_loss: 1.627016 acc: 0.836200\n",
      "Epoch  355/2000 train_loss: 1.626887 acc: 0.836400\n",
      "Epoch  356/2000 train_loss: 1.626797 acc: 0.835900\n",
      "Epoch  357/2000 train_loss: 1.626701 acc: 0.835900\n",
      "Epoch  358/2000 train_loss: 1.626580 acc: 0.836600\n",
      "Epoch  359/2000 train_loss: 1.626469 acc: 0.836100\n",
      "Epoch  360/2000 train_loss: 1.626370 acc: 0.836300\n",
      "Epoch  361/2000 train_loss: 1.626245 acc: 0.835800\n",
      "Epoch  362/2000 train_loss: 1.626159 acc: 0.837100\n",
      "Epoch  363/2000 train_loss: 1.626065 acc: 0.836900\n",
      "Epoch  364/2000 train_loss: 1.625955 acc: 0.836800\n",
      "Epoch  365/2000 train_loss: 1.625857 acc: 0.836700\n",
      "Epoch  366/2000 train_loss: 1.625731 acc: 0.837300\n",
      "Epoch  367/2000 train_loss: 1.625627 acc: 0.836900\n",
      "Epoch  368/2000 train_loss: 1.625536 acc: 0.836700\n",
      "Epoch  369/2000 train_loss: 1.625435 acc: 0.837400\n",
      "Epoch  370/2000 train_loss: 1.625334 acc: 0.836900\n",
      "Epoch  371/2000 train_loss: 1.625228 acc: 0.837400\n",
      "Epoch  372/2000 train_loss: 1.625111 acc: 0.837100\n",
      "Epoch  373/2000 train_loss: 1.625012 acc: 0.837800\n",
      "Epoch  374/2000 train_loss: 1.624923 acc: 0.836700\n",
      "Epoch  375/2000 train_loss: 1.624822 acc: 0.837500\n",
      "Epoch  376/2000 train_loss: 1.624726 acc: 0.837600\n",
      "Epoch  377/2000 train_loss: 1.624612 acc: 0.837500\n",
      "Epoch  378/2000 train_loss: 1.624524 acc: 0.837800\n",
      "Epoch  379/2000 train_loss: 1.624413 acc: 0.837500\n",
      "Epoch  380/2000 train_loss: 1.624306 acc: 0.837800\n",
      "Epoch  381/2000 train_loss: 1.624220 acc: 0.837500\n",
      "Epoch  382/2000 train_loss: 1.624124 acc: 0.838200\n",
      "Epoch  383/2000 train_loss: 1.624004 acc: 0.838000\n",
      "Epoch  384/2000 train_loss: 1.623918 acc: 0.838300\n",
      "Epoch  385/2000 train_loss: 1.623817 acc: 0.837800\n",
      "Epoch  386/2000 train_loss: 1.623717 acc: 0.838700\n",
      "Epoch  387/2000 train_loss: 1.623639 acc: 0.838300\n",
      "Epoch  388/2000 train_loss: 1.623529 acc: 0.838500\n",
      "Epoch  389/2000 train_loss: 1.623423 acc: 0.838700\n",
      "Epoch  390/2000 train_loss: 1.623323 acc: 0.838800\n",
      "Epoch  391/2000 train_loss: 1.623230 acc: 0.838900\n",
      "Epoch  392/2000 train_loss: 1.623136 acc: 0.838000\n",
      "Epoch  393/2000 train_loss: 1.623041 acc: 0.838300\n",
      "Epoch  394/2000 train_loss: 1.622943 acc: 0.838400\n",
      "Epoch  395/2000 train_loss: 1.622837 acc: 0.838600\n",
      "Epoch  396/2000 train_loss: 1.622775 acc: 0.838700\n",
      "Epoch  397/2000 train_loss: 1.622662 acc: 0.838600\n",
      "Epoch  398/2000 train_loss: 1.622560 acc: 0.839500\n",
      "Epoch  399/2000 train_loss: 1.622488 acc: 0.838900\n",
      "Epoch  400/2000 train_loss: 1.622401 acc: 0.838500\n",
      "Epoch  401/2000 train_loss: 1.622287 acc: 0.839100\n",
      "Epoch  402/2000 train_loss: 1.622195 acc: 0.839200\n",
      "Epoch  403/2000 train_loss: 1.622107 acc: 0.839500\n",
      "Epoch  404/2000 train_loss: 1.622031 acc: 0.839000\n",
      "Epoch  405/2000 train_loss: 1.621930 acc: 0.839400\n",
      "Epoch  406/2000 train_loss: 1.621814 acc: 0.839300\n",
      "Epoch  407/2000 train_loss: 1.621756 acc: 0.839400\n",
      "Epoch  408/2000 train_loss: 1.621656 acc: 0.839500\n",
      "Epoch  409/2000 train_loss: 1.621568 acc: 0.839300\n",
      "Epoch  410/2000 train_loss: 1.621465 acc: 0.839300\n",
      "Epoch  411/2000 train_loss: 1.621392 acc: 0.839600\n",
      "Epoch  412/2000 train_loss: 1.621307 acc: 0.839800\n",
      "Epoch  413/2000 train_loss: 1.621217 acc: 0.839700\n",
      "Epoch  414/2000 train_loss: 1.621130 acc: 0.839700\n",
      "Epoch  415/2000 train_loss: 1.621039 acc: 0.840200\n",
      "Epoch  416/2000 train_loss: 1.620948 acc: 0.840500\n",
      "Epoch  417/2000 train_loss: 1.620849 acc: 0.839800\n",
      "Epoch  418/2000 train_loss: 1.620774 acc: 0.840500\n",
      "Epoch  419/2000 train_loss: 1.620693 acc: 0.840700\n",
      "Epoch  420/2000 train_loss: 1.620597 acc: 0.840200\n",
      "Epoch  421/2000 train_loss: 1.620507 acc: 0.840600\n",
      "Epoch  422/2000 train_loss: 1.620421 acc: 0.840300\n",
      "Epoch  423/2000 train_loss: 1.620344 acc: 0.840700\n",
      "Epoch  424/2000 train_loss: 1.620267 acc: 0.841000\n",
      "Epoch  425/2000 train_loss: 1.620176 acc: 0.841300\n",
      "Epoch  426/2000 train_loss: 1.620069 acc: 0.841300\n",
      "Epoch  427/2000 train_loss: 1.619992 acc: 0.841400\n",
      "Epoch  428/2000 train_loss: 1.619918 acc: 0.841500\n",
      "Epoch  429/2000 train_loss: 1.619836 acc: 0.841600\n",
      "Epoch  430/2000 train_loss: 1.619751 acc: 0.841600\n",
      "Epoch  431/2000 train_loss: 1.619662 acc: 0.841800\n",
      "Epoch  432/2000 train_loss: 1.619595 acc: 0.841100\n",
      "Epoch  433/2000 train_loss: 1.619504 acc: 0.841700\n",
      "Epoch  434/2000 train_loss: 1.619437 acc: 0.841600\n",
      "Epoch  435/2000 train_loss: 1.619337 acc: 0.842300\n",
      "Epoch  436/2000 train_loss: 1.619267 acc: 0.842300\n",
      "Epoch  437/2000 train_loss: 1.619175 acc: 0.842300\n",
      "Epoch  438/2000 train_loss: 1.619097 acc: 0.842000\n",
      "Epoch  439/2000 train_loss: 1.619017 acc: 0.842800\n",
      "Epoch  440/2000 train_loss: 1.618929 acc: 0.842800\n",
      "Epoch  441/2000 train_loss: 1.618831 acc: 0.843000\n",
      "Epoch  442/2000 train_loss: 1.618758 acc: 0.842700\n",
      "Epoch  443/2000 train_loss: 1.618697 acc: 0.842600\n",
      "Epoch  444/2000 train_loss: 1.618603 acc: 0.842900\n",
      "Epoch  445/2000 train_loss: 1.618527 acc: 0.843200\n",
      "Epoch  446/2000 train_loss: 1.618438 acc: 0.843400\n",
      "Epoch  447/2000 train_loss: 1.618367 acc: 0.843100\n",
      "Epoch  448/2000 train_loss: 1.618276 acc: 0.842900\n",
      "Epoch  449/2000 train_loss: 1.618201 acc: 0.843800\n",
      "Epoch  450/2000 train_loss: 1.618126 acc: 0.843400\n",
      "Epoch  451/2000 train_loss: 1.618056 acc: 0.843300\n",
      "Epoch  452/2000 train_loss: 1.617947 acc: 0.843700\n",
      "Epoch  453/2000 train_loss: 1.617879 acc: 0.843700\n",
      "Epoch  454/2000 train_loss: 1.617798 acc: 0.843700\n",
      "Epoch  455/2000 train_loss: 1.617719 acc: 0.844300\n",
      "Epoch  456/2000 train_loss: 1.617626 acc: 0.843600\n",
      "Epoch  457/2000 train_loss: 1.617572 acc: 0.844200\n",
      "Epoch  458/2000 train_loss: 1.617497 acc: 0.844100\n",
      "Epoch  459/2000 train_loss: 1.617403 acc: 0.844200\n",
      "Epoch  460/2000 train_loss: 1.617333 acc: 0.844600\n",
      "Epoch  461/2000 train_loss: 1.617253 acc: 0.844600\n",
      "Epoch  462/2000 train_loss: 1.617195 acc: 0.845200\n",
      "Epoch  463/2000 train_loss: 1.617058 acc: 0.844700\n",
      "Epoch  464/2000 train_loss: 1.617020 acc: 0.845100\n",
      "Epoch  465/2000 train_loss: 1.616938 acc: 0.844500\n",
      "Epoch  466/2000 train_loss: 1.616872 acc: 0.845100\n",
      "Epoch  467/2000 train_loss: 1.616777 acc: 0.845100\n",
      "Epoch  468/2000 train_loss: 1.616707 acc: 0.845000\n",
      "Epoch  469/2000 train_loss: 1.616635 acc: 0.845800\n",
      "Epoch  470/2000 train_loss: 1.616540 acc: 0.845500\n",
      "Epoch  471/2000 train_loss: 1.616492 acc: 0.845700\n",
      "Epoch  472/2000 train_loss: 1.616366 acc: 0.845500\n",
      "Epoch  473/2000 train_loss: 1.616324 acc: 0.845100\n",
      "Epoch  474/2000 train_loss: 1.616250 acc: 0.845300\n",
      "Epoch  475/2000 train_loss: 1.616147 acc: 0.846000\n",
      "Epoch  476/2000 train_loss: 1.616068 acc: 0.845400\n",
      "Epoch  477/2000 train_loss: 1.616010 acc: 0.845500\n",
      "Epoch  478/2000 train_loss: 1.615922 acc: 0.845800\n",
      "Epoch  479/2000 train_loss: 1.615849 acc: 0.846100\n",
      "Epoch  480/2000 train_loss: 1.615784 acc: 0.846200\n",
      "Epoch  481/2000 train_loss: 1.615702 acc: 0.846100\n",
      "Epoch  482/2000 train_loss: 1.615642 acc: 0.846200\n",
      "Epoch  483/2000 train_loss: 1.615533 acc: 0.846000\n",
      "Epoch  484/2000 train_loss: 1.615482 acc: 0.846300\n",
      "Epoch  485/2000 train_loss: 1.615399 acc: 0.847000\n",
      "Epoch  486/2000 train_loss: 1.615324 acc: 0.845900\n",
      "Epoch  487/2000 train_loss: 1.615262 acc: 0.846500\n",
      "Epoch  488/2000 train_loss: 1.615166 acc: 0.846200\n",
      "Epoch  489/2000 train_loss: 1.615107 acc: 0.846900\n",
      "Epoch  490/2000 train_loss: 1.615027 acc: 0.846500\n",
      "Epoch  491/2000 train_loss: 1.614947 acc: 0.846500\n",
      "Epoch  492/2000 train_loss: 1.614869 acc: 0.847000\n",
      "Epoch  493/2000 train_loss: 1.614812 acc: 0.847100\n",
      "Epoch  494/2000 train_loss: 1.614719 acc: 0.846700\n",
      "Epoch  495/2000 train_loss: 1.614651 acc: 0.846900\n",
      "Epoch  496/2000 train_loss: 1.614586 acc: 0.847300\n",
      "Epoch  497/2000 train_loss: 1.614523 acc: 0.846800\n",
      "Epoch  498/2000 train_loss: 1.614438 acc: 0.847300\n",
      "Epoch  499/2000 train_loss: 1.614347 acc: 0.847400\n",
      "Epoch  500/2000 train_loss: 1.614271 acc: 0.847200\n",
      "Epoch  501/2000 train_loss: 1.614198 acc: 0.847200\n",
      "Epoch  502/2000 train_loss: 1.614121 acc: 0.847600\n",
      "Epoch  503/2000 train_loss: 1.614056 acc: 0.847200\n",
      "Epoch  504/2000 train_loss: 1.614000 acc: 0.847800\n",
      "Epoch  505/2000 train_loss: 1.613921 acc: 0.847400\n",
      "Epoch  506/2000 train_loss: 1.613822 acc: 0.848000\n",
      "Epoch  507/2000 train_loss: 1.613773 acc: 0.847600\n",
      "Epoch  508/2000 train_loss: 1.613696 acc: 0.848000\n",
      "Epoch  509/2000 train_loss: 1.613633 acc: 0.848200\n",
      "Epoch  510/2000 train_loss: 1.613548 acc: 0.847800\n",
      "Epoch  511/2000 train_loss: 1.613483 acc: 0.847800\n",
      "Epoch  512/2000 train_loss: 1.613410 acc: 0.848300\n",
      "Epoch  513/2000 train_loss: 1.613350 acc: 0.848000\n",
      "Epoch  514/2000 train_loss: 1.613267 acc: 0.848300\n",
      "Epoch  515/2000 train_loss: 1.613205 acc: 0.848200\n",
      "Epoch  516/2000 train_loss: 1.613129 acc: 0.848400\n",
      "Epoch  517/2000 train_loss: 1.613052 acc: 0.848300\n",
      "Epoch  518/2000 train_loss: 1.612990 acc: 0.848300\n",
      "Epoch  519/2000 train_loss: 1.612923 acc: 0.848800\n",
      "Epoch  520/2000 train_loss: 1.612850 acc: 0.847900\n",
      "Epoch  521/2000 train_loss: 1.612763 acc: 0.848300\n",
      "Epoch  522/2000 train_loss: 1.612681 acc: 0.848300\n",
      "Epoch  523/2000 train_loss: 1.612625 acc: 0.848900\n",
      "Epoch  524/2000 train_loss: 1.612569 acc: 0.848800\n",
      "Epoch  525/2000 train_loss: 1.612487 acc: 0.848700\n",
      "Epoch  526/2000 train_loss: 1.612385 acc: 0.848700\n",
      "Epoch  527/2000 train_loss: 1.612328 acc: 0.848600\n",
      "Epoch  528/2000 train_loss: 1.612263 acc: 0.848900\n",
      "Epoch  529/2000 train_loss: 1.612200 acc: 0.849100\n",
      "Epoch  530/2000 train_loss: 1.612127 acc: 0.849400\n",
      "Epoch  531/2000 train_loss: 1.612069 acc: 0.849100\n",
      "Epoch  532/2000 train_loss: 1.611989 acc: 0.849000\n",
      "Epoch  533/2000 train_loss: 1.611930 acc: 0.849200\n",
      "Epoch  534/2000 train_loss: 1.611833 acc: 0.848900\n",
      "Epoch  535/2000 train_loss: 1.611792 acc: 0.849000\n",
      "Epoch  536/2000 train_loss: 1.611706 acc: 0.849100\n",
      "Epoch  537/2000 train_loss: 1.611626 acc: 0.848500\n",
      "Epoch  538/2000 train_loss: 1.611551 acc: 0.849100\n",
      "Epoch  539/2000 train_loss: 1.611507 acc: 0.849600\n",
      "Epoch  540/2000 train_loss: 1.611441 acc: 0.849700\n",
      "Epoch  541/2000 train_loss: 1.611363 acc: 0.849500\n",
      "Epoch  542/2000 train_loss: 1.611286 acc: 0.849600\n",
      "Epoch  543/2000 train_loss: 1.611221 acc: 0.849200\n",
      "Epoch  544/2000 train_loss: 1.611138 acc: 0.849300\n",
      "Epoch  545/2000 train_loss: 1.611078 acc: 0.850000\n",
      "Epoch  546/2000 train_loss: 1.610999 acc: 0.849200\n",
      "Epoch  547/2000 train_loss: 1.610947 acc: 0.849400\n",
      "Epoch  548/2000 train_loss: 1.610866 acc: 0.849300\n",
      "Epoch  549/2000 train_loss: 1.610802 acc: 0.849600\n",
      "Epoch  550/2000 train_loss: 1.610737 acc: 0.849400\n",
      "Epoch  551/2000 train_loss: 1.610659 acc: 0.849300\n",
      "Epoch  552/2000 train_loss: 1.610595 acc: 0.849700\n",
      "Epoch  553/2000 train_loss: 1.610552 acc: 0.850100\n",
      "Epoch  554/2000 train_loss: 1.610453 acc: 0.849600\n",
      "Epoch  555/2000 train_loss: 1.610390 acc: 0.849500\n",
      "Epoch  556/2000 train_loss: 1.610327 acc: 0.850100\n",
      "Epoch  557/2000 train_loss: 1.610226 acc: 0.850000\n",
      "Epoch  558/2000 train_loss: 1.610186 acc: 0.849600\n",
      "Epoch  559/2000 train_loss: 1.610119 acc: 0.850000\n",
      "Epoch  560/2000 train_loss: 1.610051 acc: 0.850400\n",
      "Epoch  561/2000 train_loss: 1.609979 acc: 0.850000\n",
      "Epoch  562/2000 train_loss: 1.609917 acc: 0.850100\n",
      "Epoch  563/2000 train_loss: 1.609856 acc: 0.850400\n",
      "Epoch  564/2000 train_loss: 1.609767 acc: 0.850500\n",
      "Epoch  565/2000 train_loss: 1.609715 acc: 0.850700\n",
      "Epoch  566/2000 train_loss: 1.609638 acc: 0.850400\n",
      "Epoch  567/2000 train_loss: 1.609582 acc: 0.850300\n",
      "Epoch  568/2000 train_loss: 1.609517 acc: 0.850500\n",
      "Epoch  569/2000 train_loss: 1.609437 acc: 0.850700\n",
      "Epoch  570/2000 train_loss: 1.609379 acc: 0.850800\n",
      "Epoch  571/2000 train_loss: 1.609301 acc: 0.850500\n",
      "Epoch  572/2000 train_loss: 1.609222 acc: 0.850800\n",
      "Epoch  573/2000 train_loss: 1.609173 acc: 0.850700\n",
      "Epoch  574/2000 train_loss: 1.609087 acc: 0.850500\n",
      "Epoch  575/2000 train_loss: 1.609018 acc: 0.850900\n",
      "Epoch  576/2000 train_loss: 1.608973 acc: 0.851200\n",
      "Epoch  577/2000 train_loss: 1.608888 acc: 0.850700\n",
      "Epoch  578/2000 train_loss: 1.608827 acc: 0.851000\n",
      "Epoch  579/2000 train_loss: 1.608757 acc: 0.851100\n",
      "Epoch  580/2000 train_loss: 1.608705 acc: 0.851000\n",
      "Epoch  581/2000 train_loss: 1.608630 acc: 0.851200\n",
      "Epoch  582/2000 train_loss: 1.608578 acc: 0.851100\n",
      "Epoch  583/2000 train_loss: 1.608510 acc: 0.850500\n",
      "Epoch  584/2000 train_loss: 1.608437 acc: 0.851200\n",
      "Epoch  585/2000 train_loss: 1.608384 acc: 0.851600\n",
      "Epoch  586/2000 train_loss: 1.608297 acc: 0.851800\n",
      "Epoch  587/2000 train_loss: 1.608226 acc: 0.851000\n",
      "Epoch  588/2000 train_loss: 1.608187 acc: 0.851200\n",
      "Epoch  589/2000 train_loss: 1.608118 acc: 0.851400\n",
      "Epoch  590/2000 train_loss: 1.608037 acc: 0.852000\n",
      "Epoch  591/2000 train_loss: 1.608000 acc: 0.851200\n",
      "Epoch  592/2000 train_loss: 1.607923 acc: 0.851800\n",
      "Epoch  593/2000 train_loss: 1.607856 acc: 0.851500\n",
      "Epoch  594/2000 train_loss: 1.607796 acc: 0.852100\n",
      "Epoch  595/2000 train_loss: 1.607730 acc: 0.852100\n",
      "Epoch  596/2000 train_loss: 1.607665 acc: 0.852200\n",
      "Epoch  597/2000 train_loss: 1.607603 acc: 0.851900\n",
      "Epoch  598/2000 train_loss: 1.607538 acc: 0.851600\n",
      "Epoch  599/2000 train_loss: 1.607490 acc: 0.851900\n",
      "Epoch  600/2000 train_loss: 1.607402 acc: 0.851900\n",
      "Epoch  601/2000 train_loss: 1.607368 acc: 0.852200\n",
      "Epoch  602/2000 train_loss: 1.607294 acc: 0.852400\n",
      "Epoch  603/2000 train_loss: 1.607235 acc: 0.852100\n",
      "Epoch  604/2000 train_loss: 1.607168 acc: 0.852100\n",
      "Epoch  605/2000 train_loss: 1.607112 acc: 0.851900\n",
      "Epoch  606/2000 train_loss: 1.607051 acc: 0.852600\n",
      "Epoch  607/2000 train_loss: 1.606983 acc: 0.852400\n",
      "Epoch  608/2000 train_loss: 1.606927 acc: 0.852400\n",
      "Epoch  609/2000 train_loss: 1.606850 acc: 0.852000\n",
      "Epoch  610/2000 train_loss: 1.606801 acc: 0.851800\n",
      "Epoch  611/2000 train_loss: 1.606736 acc: 0.852300\n",
      "Epoch  612/2000 train_loss: 1.606680 acc: 0.852600\n",
      "Epoch  613/2000 train_loss: 1.606626 acc: 0.852500\n",
      "Epoch  614/2000 train_loss: 1.606566 acc: 0.852300\n",
      "Epoch  615/2000 train_loss: 1.606469 acc: 0.852100\n",
      "Epoch  616/2000 train_loss: 1.606444 acc: 0.851900\n",
      "Epoch  617/2000 train_loss: 1.606360 acc: 0.852200\n",
      "Epoch  618/2000 train_loss: 1.606307 acc: 0.852400\n",
      "Epoch  619/2000 train_loss: 1.606257 acc: 0.852600\n",
      "Epoch  620/2000 train_loss: 1.606191 acc: 0.852700\n",
      "Epoch  621/2000 train_loss: 1.606140 acc: 0.852700\n",
      "Epoch  622/2000 train_loss: 1.606080 acc: 0.852600\n",
      "Epoch  623/2000 train_loss: 1.606015 acc: 0.852600\n",
      "Epoch  624/2000 train_loss: 1.605971 acc: 0.852500\n",
      "Epoch  625/2000 train_loss: 1.605906 acc: 0.852700\n",
      "Epoch  626/2000 train_loss: 1.605854 acc: 0.852900\n",
      "Epoch  627/2000 train_loss: 1.605793 acc: 0.852900\n",
      "Epoch  628/2000 train_loss: 1.605707 acc: 0.852700\n",
      "Epoch  629/2000 train_loss: 1.605686 acc: 0.853100\n",
      "Epoch  630/2000 train_loss: 1.605611 acc: 0.852800\n",
      "Epoch  631/2000 train_loss: 1.605548 acc: 0.852500\n",
      "Epoch  632/2000 train_loss: 1.605506 acc: 0.853000\n",
      "Epoch  633/2000 train_loss: 1.605417 acc: 0.852900\n",
      "Epoch  634/2000 train_loss: 1.605390 acc: 0.852600\n",
      "Epoch  635/2000 train_loss: 1.605339 acc: 0.853200\n",
      "Epoch  636/2000 train_loss: 1.605266 acc: 0.852800\n",
      "Epoch  637/2000 train_loss: 1.605225 acc: 0.853000\n",
      "Epoch  638/2000 train_loss: 1.605164 acc: 0.853200\n",
      "Epoch  639/2000 train_loss: 1.605108 acc: 0.853000\n",
      "Epoch  640/2000 train_loss: 1.605041 acc: 0.853100\n",
      "Epoch  641/2000 train_loss: 1.605007 acc: 0.853500\n",
      "Epoch  642/2000 train_loss: 1.604938 acc: 0.853100\n",
      "Epoch  643/2000 train_loss: 1.604872 acc: 0.853400\n",
      "Epoch  644/2000 train_loss: 1.604839 acc: 0.853700\n",
      "Epoch  645/2000 train_loss: 1.604777 acc: 0.853800\n",
      "Epoch  646/2000 train_loss: 1.604717 acc: 0.853100\n",
      "Epoch  647/2000 train_loss: 1.604664 acc: 0.853400\n",
      "Epoch  648/2000 train_loss: 1.604619 acc: 0.853500\n",
      "Epoch  649/2000 train_loss: 1.604558 acc: 0.853700\n",
      "Epoch  650/2000 train_loss: 1.604497 acc: 0.853700\n",
      "Epoch  651/2000 train_loss: 1.604446 acc: 0.853800\n",
      "Epoch  652/2000 train_loss: 1.604399 acc: 0.854000\n",
      "Epoch  653/2000 train_loss: 1.604331 acc: 0.854000\n",
      "Epoch  654/2000 train_loss: 1.604279 acc: 0.854100\n",
      "Epoch  655/2000 train_loss: 1.604251 acc: 0.854000\n",
      "Epoch  656/2000 train_loss: 1.604188 acc: 0.853900\n",
      "Epoch  657/2000 train_loss: 1.604136 acc: 0.854300\n",
      "Epoch  658/2000 train_loss: 1.604098 acc: 0.854200\n",
      "Epoch  659/2000 train_loss: 1.604035 acc: 0.854100\n",
      "Epoch  660/2000 train_loss: 1.603979 acc: 0.854300\n",
      "Epoch  661/2000 train_loss: 1.603922 acc: 0.854500\n",
      "Epoch  662/2000 train_loss: 1.603881 acc: 0.854300\n",
      "Epoch  663/2000 train_loss: 1.603813 acc: 0.854700\n",
      "Epoch  664/2000 train_loss: 1.603763 acc: 0.854700\n",
      "Epoch  665/2000 train_loss: 1.603723 acc: 0.854500\n",
      "Epoch  666/2000 train_loss: 1.603669 acc: 0.854500\n",
      "Epoch  667/2000 train_loss: 1.603621 acc: 0.854700\n",
      "Epoch  668/2000 train_loss: 1.603578 acc: 0.855000\n",
      "Epoch  669/2000 train_loss: 1.603527 acc: 0.854500\n",
      "Epoch  670/2000 train_loss: 1.603473 acc: 0.854700\n",
      "Epoch  671/2000 train_loss: 1.603408 acc: 0.854800\n",
      "Epoch  672/2000 train_loss: 1.603361 acc: 0.854800\n",
      "Epoch  673/2000 train_loss: 1.603299 acc: 0.855100\n",
      "Epoch  674/2000 train_loss: 1.603267 acc: 0.855100\n",
      "Epoch  675/2000 train_loss: 1.603223 acc: 0.854700\n",
      "Epoch  676/2000 train_loss: 1.603159 acc: 0.855100\n",
      "Epoch  677/2000 train_loss: 1.603116 acc: 0.854500\n",
      "Epoch  678/2000 train_loss: 1.603066 acc: 0.855000\n",
      "Epoch  679/2000 train_loss: 1.603016 acc: 0.854700\n",
      "Epoch  680/2000 train_loss: 1.602950 acc: 0.855100\n",
      "Epoch  681/2000 train_loss: 1.602908 acc: 0.854900\n",
      "Epoch  682/2000 train_loss: 1.602849 acc: 0.854700\n",
      "Epoch  683/2000 train_loss: 1.602808 acc: 0.855300\n",
      "Epoch  684/2000 train_loss: 1.602764 acc: 0.855000\n",
      "Epoch  685/2000 train_loss: 1.602721 acc: 0.855400\n",
      "Epoch  686/2000 train_loss: 1.602661 acc: 0.854800\n",
      "Epoch  687/2000 train_loss: 1.602610 acc: 0.855200\n",
      "Epoch  688/2000 train_loss: 1.602564 acc: 0.855100\n",
      "Epoch  689/2000 train_loss: 1.602512 acc: 0.855400\n",
      "Epoch  690/2000 train_loss: 1.602466 acc: 0.855100\n",
      "Epoch  691/2000 train_loss: 1.602425 acc: 0.855200\n",
      "Epoch  692/2000 train_loss: 1.602366 acc: 0.855200\n",
      "Epoch  693/2000 train_loss: 1.602302 acc: 0.855800\n",
      "Epoch  694/2000 train_loss: 1.602268 acc: 0.855400\n",
      "Epoch  695/2000 train_loss: 1.602226 acc: 0.855200\n",
      "Epoch  696/2000 train_loss: 1.602175 acc: 0.855300\n",
      "Epoch  697/2000 train_loss: 1.602129 acc: 0.855500\n",
      "Epoch  698/2000 train_loss: 1.602077 acc: 0.855300\n",
      "Epoch  699/2000 train_loss: 1.602009 acc: 0.855700\n",
      "Epoch  700/2000 train_loss: 1.601973 acc: 0.854900\n",
      "Epoch  701/2000 train_loss: 1.601925 acc: 0.855500\n",
      "Epoch  702/2000 train_loss: 1.601864 acc: 0.855300\n",
      "Epoch  703/2000 train_loss: 1.601835 acc: 0.855300\n",
      "Epoch  704/2000 train_loss: 1.601791 acc: 0.855200\n",
      "Epoch  705/2000 train_loss: 1.601725 acc: 0.855600\n",
      "Epoch  706/2000 train_loss: 1.601693 acc: 0.855800\n",
      "Epoch  707/2000 train_loss: 1.601635 acc: 0.855500\n",
      "Epoch  708/2000 train_loss: 1.601578 acc: 0.855200\n",
      "Epoch  709/2000 train_loss: 1.601543 acc: 0.856000\n",
      "Epoch  710/2000 train_loss: 1.601486 acc: 0.855900\n",
      "Epoch  711/2000 train_loss: 1.601459 acc: 0.855800\n",
      "Epoch  712/2000 train_loss: 1.601397 acc: 0.855700\n",
      "Epoch  713/2000 train_loss: 1.601340 acc: 0.855600\n",
      "Epoch  714/2000 train_loss: 1.601304 acc: 0.855500\n",
      "Epoch  715/2000 train_loss: 1.601251 acc: 0.855900\n",
      "Epoch  716/2000 train_loss: 1.601202 acc: 0.856200\n",
      "Epoch  717/2000 train_loss: 1.601146 acc: 0.855700\n",
      "Epoch  718/2000 train_loss: 1.601102 acc: 0.856200\n",
      "Epoch  719/2000 train_loss: 1.601059 acc: 0.855800\n",
      "Epoch  720/2000 train_loss: 1.601001 acc: 0.856300\n",
      "Epoch  721/2000 train_loss: 1.600952 acc: 0.856500\n",
      "Epoch  722/2000 train_loss: 1.600924 acc: 0.855800\n",
      "Epoch  723/2000 train_loss: 1.600870 acc: 0.855800\n",
      "Epoch  724/2000 train_loss: 1.600817 acc: 0.856200\n",
      "Epoch  725/2000 train_loss: 1.600771 acc: 0.856500\n",
      "Epoch  726/2000 train_loss: 1.600721 acc: 0.856400\n",
      "Epoch  727/2000 train_loss: 1.600689 acc: 0.856500\n",
      "Epoch  728/2000 train_loss: 1.600619 acc: 0.856200\n",
      "Epoch  729/2000 train_loss: 1.600581 acc: 0.856800\n",
      "Epoch  730/2000 train_loss: 1.600540 acc: 0.857200\n",
      "Epoch  731/2000 train_loss: 1.600487 acc: 0.856500\n",
      "Epoch  732/2000 train_loss: 1.600448 acc: 0.856900\n",
      "Epoch  733/2000 train_loss: 1.600405 acc: 0.856100\n",
      "Epoch  734/2000 train_loss: 1.600350 acc: 0.856300\n",
      "Epoch  735/2000 train_loss: 1.600305 acc: 0.856000\n",
      "Epoch  736/2000 train_loss: 1.600261 acc: 0.856900\n",
      "Epoch  737/2000 train_loss: 1.600204 acc: 0.856500\n",
      "Epoch  738/2000 train_loss: 1.600167 acc: 0.856500\n",
      "Epoch  739/2000 train_loss: 1.600127 acc: 0.856600\n",
      "Epoch  740/2000 train_loss: 1.600067 acc: 0.856900\n",
      "Epoch  741/2000 train_loss: 1.600023 acc: 0.856900\n",
      "Epoch  742/2000 train_loss: 1.599987 acc: 0.856500\n",
      "Epoch  743/2000 train_loss: 1.599933 acc: 0.857100\n",
      "Epoch  744/2000 train_loss: 1.599892 acc: 0.857000\n",
      "Epoch  745/2000 train_loss: 1.599849 acc: 0.857000\n",
      "Epoch  746/2000 train_loss: 1.599805 acc: 0.856500\n",
      "Epoch  747/2000 train_loss: 1.599768 acc: 0.857000\n",
      "Epoch  748/2000 train_loss: 1.599725 acc: 0.857000\n",
      "Epoch  749/2000 train_loss: 1.599669 acc: 0.857000\n",
      "Epoch  750/2000 train_loss: 1.599621 acc: 0.856800\n",
      "Epoch  751/2000 train_loss: 1.599580 acc: 0.857100\n",
      "Epoch  752/2000 train_loss: 1.599534 acc: 0.857900\n",
      "Epoch  753/2000 train_loss: 1.599467 acc: 0.856900\n",
      "Epoch  754/2000 train_loss: 1.599459 acc: 0.857000\n",
      "Epoch  755/2000 train_loss: 1.599404 acc: 0.857300\n",
      "Epoch  756/2000 train_loss: 1.599369 acc: 0.856900\n",
      "Epoch  757/2000 train_loss: 1.599324 acc: 0.857600\n",
      "Epoch  758/2000 train_loss: 1.599278 acc: 0.857300\n",
      "Epoch  759/2000 train_loss: 1.599240 acc: 0.857600\n",
      "Epoch  760/2000 train_loss: 1.599199 acc: 0.857700\n",
      "Epoch  761/2000 train_loss: 1.599150 acc: 0.857400\n",
      "Epoch  762/2000 train_loss: 1.599123 acc: 0.857700\n",
      "Epoch  763/2000 train_loss: 1.599042 acc: 0.857600\n",
      "Epoch  764/2000 train_loss: 1.599026 acc: 0.857500\n",
      "Epoch  765/2000 train_loss: 1.598980 acc: 0.857800\n",
      "Epoch  766/2000 train_loss: 1.598940 acc: 0.857900\n",
      "Epoch  767/2000 train_loss: 1.598883 acc: 0.857900\n",
      "Epoch  768/2000 train_loss: 1.598849 acc: 0.858400\n",
      "Epoch  769/2000 train_loss: 1.598809 acc: 0.857900\n",
      "Epoch  770/2000 train_loss: 1.598739 acc: 0.857900\n",
      "Epoch  771/2000 train_loss: 1.598719 acc: 0.857500\n",
      "Epoch  772/2000 train_loss: 1.598693 acc: 0.857900\n",
      "Epoch  773/2000 train_loss: 1.598637 acc: 0.857900\n",
      "Epoch  774/2000 train_loss: 1.598586 acc: 0.858200\n",
      "Epoch  775/2000 train_loss: 1.598551 acc: 0.858300\n",
      "Epoch  776/2000 train_loss: 1.598499 acc: 0.857800\n",
      "Epoch  777/2000 train_loss: 1.598477 acc: 0.857800\n",
      "Epoch  778/2000 train_loss: 1.598432 acc: 0.858100\n",
      "Epoch  779/2000 train_loss: 1.598376 acc: 0.858100\n",
      "Epoch  780/2000 train_loss: 1.598353 acc: 0.858300\n",
      "Epoch  781/2000 train_loss: 1.598288 acc: 0.858100\n",
      "Epoch  782/2000 train_loss: 1.598266 acc: 0.857900\n",
      "Epoch  783/2000 train_loss: 1.598199 acc: 0.858200\n",
      "Epoch  784/2000 train_loss: 1.598184 acc: 0.858000\n",
      "Epoch  785/2000 train_loss: 1.598139 acc: 0.858000\n",
      "Epoch  786/2000 train_loss: 1.598092 acc: 0.857800\n",
      "Epoch  787/2000 train_loss: 1.598053 acc: 0.858500\n",
      "Epoch  788/2000 train_loss: 1.597990 acc: 0.858300\n",
      "Epoch  789/2000 train_loss: 1.597985 acc: 0.858200\n",
      "Epoch  790/2000 train_loss: 1.597914 acc: 0.858800\n",
      "Epoch  791/2000 train_loss: 1.597887 acc: 0.858500\n",
      "Epoch  792/2000 train_loss: 1.597861 acc: 0.858600\n",
      "Epoch  793/2000 train_loss: 1.597814 acc: 0.858500\n",
      "Epoch  794/2000 train_loss: 1.597772 acc: 0.858300\n",
      "Epoch  795/2000 train_loss: 1.597731 acc: 0.858200\n",
      "Epoch  796/2000 train_loss: 1.597681 acc: 0.858800\n",
      "Epoch  797/2000 train_loss: 1.597653 acc: 0.858900\n",
      "Epoch  798/2000 train_loss: 1.597612 acc: 0.858500\n",
      "Epoch  799/2000 train_loss: 1.597572 acc: 0.858900\n",
      "Epoch  800/2000 train_loss: 1.597516 acc: 0.858500\n",
      "Epoch  801/2000 train_loss: 1.597495 acc: 0.858200\n",
      "Epoch  802/2000 train_loss: 1.597459 acc: 0.859000\n",
      "Epoch  803/2000 train_loss: 1.597417 acc: 0.858300\n",
      "Epoch  804/2000 train_loss: 1.597373 acc: 0.858500\n",
      "Epoch  805/2000 train_loss: 1.597327 acc: 0.858900\n",
      "Epoch  806/2000 train_loss: 1.597289 acc: 0.858800\n",
      "Epoch  807/2000 train_loss: 1.597247 acc: 0.858600\n",
      "Epoch  808/2000 train_loss: 1.597207 acc: 0.859300\n",
      "Epoch  809/2000 train_loss: 1.597162 acc: 0.859100\n",
      "Epoch  810/2000 train_loss: 1.597129 acc: 0.858900\n",
      "Epoch  811/2000 train_loss: 1.597094 acc: 0.858900\n",
      "Epoch  812/2000 train_loss: 1.597047 acc: 0.858900\n",
      "Epoch  813/2000 train_loss: 1.597019 acc: 0.859000\n",
      "Epoch  814/2000 train_loss: 1.596962 acc: 0.858600\n",
      "Epoch  815/2000 train_loss: 1.596937 acc: 0.859100\n",
      "Epoch  816/2000 train_loss: 1.596888 acc: 0.859300\n",
      "Epoch  817/2000 train_loss: 1.596859 acc: 0.859200\n",
      "Epoch  818/2000 train_loss: 1.596818 acc: 0.859600\n",
      "Epoch  819/2000 train_loss: 1.596757 acc: 0.859300\n",
      "Epoch  820/2000 train_loss: 1.596740 acc: 0.859000\n",
      "Epoch  821/2000 train_loss: 1.596677 acc: 0.859700\n",
      "Epoch  822/2000 train_loss: 1.596658 acc: 0.859400\n",
      "Epoch  823/2000 train_loss: 1.596609 acc: 0.859500\n",
      "Epoch  824/2000 train_loss: 1.596584 acc: 0.859400\n",
      "Epoch  825/2000 train_loss: 1.596526 acc: 0.859600\n",
      "Epoch  826/2000 train_loss: 1.596505 acc: 0.859800\n",
      "Epoch  827/2000 train_loss: 1.596458 acc: 0.859900\n",
      "Epoch  828/2000 train_loss: 1.596424 acc: 0.860000\n",
      "Epoch  829/2000 train_loss: 1.596368 acc: 0.859800\n",
      "Epoch  830/2000 train_loss: 1.596341 acc: 0.859900\n",
      "Epoch  831/2000 train_loss: 1.596303 acc: 0.859500\n",
      "Epoch  832/2000 train_loss: 1.596259 acc: 0.860100\n",
      "Epoch  833/2000 train_loss: 1.596223 acc: 0.860300\n",
      "Epoch  834/2000 train_loss: 1.596190 acc: 0.860000\n",
      "Epoch  835/2000 train_loss: 1.596139 acc: 0.859600\n",
      "Epoch  836/2000 train_loss: 1.596107 acc: 0.859700\n",
      "Epoch  837/2000 train_loss: 1.596060 acc: 0.859400\n",
      "Epoch  838/2000 train_loss: 1.596032 acc: 0.859600\n",
      "Epoch  839/2000 train_loss: 1.596001 acc: 0.860300\n",
      "Epoch  840/2000 train_loss: 1.595948 acc: 0.859800\n",
      "Epoch  841/2000 train_loss: 1.595912 acc: 0.860100\n",
      "Epoch  842/2000 train_loss: 1.595883 acc: 0.860200\n",
      "Epoch  843/2000 train_loss: 1.595830 acc: 0.859800\n",
      "Epoch  844/2000 train_loss: 1.595799 acc: 0.859800\n",
      "Epoch  845/2000 train_loss: 1.595749 acc: 0.860200\n",
      "Epoch  846/2000 train_loss: 1.595736 acc: 0.860300\n",
      "Epoch  847/2000 train_loss: 1.595682 acc: 0.860200\n",
      "Epoch  848/2000 train_loss: 1.595634 acc: 0.860400\n",
      "Epoch  849/2000 train_loss: 1.595592 acc: 0.860500\n",
      "Epoch  850/2000 train_loss: 1.595563 acc: 0.860200\n",
      "Epoch  851/2000 train_loss: 1.595527 acc: 0.860100\n",
      "Epoch  852/2000 train_loss: 1.595492 acc: 0.859900\n",
      "Epoch  853/2000 train_loss: 1.595450 acc: 0.860700\n",
      "Epoch  854/2000 train_loss: 1.595418 acc: 0.860400\n",
      "Epoch  855/2000 train_loss: 1.595368 acc: 0.860100\n",
      "Epoch  856/2000 train_loss: 1.595341 acc: 0.860400\n",
      "Epoch  857/2000 train_loss: 1.595300 acc: 0.861000\n",
      "Epoch  858/2000 train_loss: 1.595262 acc: 0.860900\n",
      "Epoch  859/2000 train_loss: 1.595218 acc: 0.860900\n",
      "Epoch  860/2000 train_loss: 1.595175 acc: 0.861100\n",
      "Epoch  861/2000 train_loss: 1.595131 acc: 0.861100\n",
      "Epoch  862/2000 train_loss: 1.595092 acc: 0.860200\n",
      "Epoch  863/2000 train_loss: 1.595065 acc: 0.860900\n",
      "Epoch  864/2000 train_loss: 1.595019 acc: 0.860600\n",
      "Epoch  865/2000 train_loss: 1.594980 acc: 0.860800\n",
      "Epoch  866/2000 train_loss: 1.594947 acc: 0.860700\n",
      "Epoch  867/2000 train_loss: 1.594912 acc: 0.861000\n",
      "Epoch  868/2000 train_loss: 1.594868 acc: 0.861500\n",
      "Epoch  869/2000 train_loss: 1.594832 acc: 0.860600\n",
      "Epoch  870/2000 train_loss: 1.594793 acc: 0.861100\n",
      "Epoch  871/2000 train_loss: 1.594761 acc: 0.861000\n",
      "Epoch  872/2000 train_loss: 1.594718 acc: 0.861200\n",
      "Epoch  873/2000 train_loss: 1.594697 acc: 0.860700\n",
      "Epoch  874/2000 train_loss: 1.594642 acc: 0.861000\n",
      "Epoch  875/2000 train_loss: 1.594595 acc: 0.861000\n",
      "Epoch  876/2000 train_loss: 1.594573 acc: 0.861500\n",
      "Epoch  877/2000 train_loss: 1.594523 acc: 0.861500\n",
      "Epoch  878/2000 train_loss: 1.594491 acc: 0.861200\n",
      "Epoch  879/2000 train_loss: 1.594458 acc: 0.861000\n",
      "Epoch  880/2000 train_loss: 1.594428 acc: 0.861400\n",
      "Epoch  881/2000 train_loss: 1.594387 acc: 0.861700\n",
      "Epoch  882/2000 train_loss: 1.594350 acc: 0.861100\n",
      "Epoch  883/2000 train_loss: 1.594300 acc: 0.861400\n",
      "Epoch  884/2000 train_loss: 1.594260 acc: 0.861600\n",
      "Epoch  885/2000 train_loss: 1.594226 acc: 0.861100\n",
      "Epoch  886/2000 train_loss: 1.594216 acc: 0.861200\n",
      "Epoch  887/2000 train_loss: 1.594162 acc: 0.861400\n",
      "Epoch  888/2000 train_loss: 1.594122 acc: 0.860900\n",
      "Epoch  889/2000 train_loss: 1.594090 acc: 0.861200\n",
      "Epoch  890/2000 train_loss: 1.594068 acc: 0.861500\n",
      "Epoch  891/2000 train_loss: 1.594026 acc: 0.861300\n",
      "Epoch  892/2000 train_loss: 1.593996 acc: 0.861700\n",
      "Epoch  893/2000 train_loss: 1.593927 acc: 0.861400\n",
      "Epoch  894/2000 train_loss: 1.593912 acc: 0.861200\n",
      "Epoch  895/2000 train_loss: 1.593885 acc: 0.861500\n",
      "Epoch  896/2000 train_loss: 1.593847 acc: 0.861200\n",
      "Epoch  897/2000 train_loss: 1.593807 acc: 0.861300\n",
      "Epoch  898/2000 train_loss: 1.593774 acc: 0.861300\n",
      "Epoch  899/2000 train_loss: 1.593751 acc: 0.862100\n",
      "Epoch  900/2000 train_loss: 1.593706 acc: 0.861600\n",
      "Epoch  901/2000 train_loss: 1.593672 acc: 0.861400\n",
      "Epoch  902/2000 train_loss: 1.593649 acc: 0.862200\n",
      "Epoch  903/2000 train_loss: 1.593602 acc: 0.861500\n",
      "Epoch  904/2000 train_loss: 1.593577 acc: 0.861400\n",
      "Epoch  905/2000 train_loss: 1.593538 acc: 0.861800\n",
      "Epoch  906/2000 train_loss: 1.593494 acc: 0.861900\n",
      "Epoch  907/2000 train_loss: 1.593482 acc: 0.862200\n",
      "Epoch  908/2000 train_loss: 1.593431 acc: 0.861900\n",
      "Epoch  909/2000 train_loss: 1.593402 acc: 0.862100\n",
      "Epoch  910/2000 train_loss: 1.593373 acc: 0.861600\n",
      "Epoch  911/2000 train_loss: 1.593333 acc: 0.861600\n",
      "Epoch  912/2000 train_loss: 1.593295 acc: 0.861900\n",
      "Epoch  913/2000 train_loss: 1.593270 acc: 0.861800\n",
      "Epoch  914/2000 train_loss: 1.593228 acc: 0.862100\n",
      "Epoch  915/2000 train_loss: 1.593208 acc: 0.861900\n",
      "Epoch  916/2000 train_loss: 1.593162 acc: 0.862100\n",
      "Epoch  917/2000 train_loss: 1.593129 acc: 0.861700\n",
      "Epoch  918/2000 train_loss: 1.593100 acc: 0.862400\n",
      "Epoch  919/2000 train_loss: 1.593074 acc: 0.862200\n",
      "Epoch  920/2000 train_loss: 1.593019 acc: 0.861800\n",
      "Epoch  921/2000 train_loss: 1.593001 acc: 0.862100\n",
      "Epoch  922/2000 train_loss: 1.592965 acc: 0.861800\n",
      "Epoch  923/2000 train_loss: 1.592928 acc: 0.862100\n",
      "Epoch  924/2000 train_loss: 1.592918 acc: 0.862000\n",
      "Epoch  925/2000 train_loss: 1.592865 acc: 0.862100\n",
      "Epoch  926/2000 train_loss: 1.592832 acc: 0.862200\n",
      "Epoch  927/2000 train_loss: 1.592803 acc: 0.862000\n",
      "Epoch  928/2000 train_loss: 1.592770 acc: 0.861900\n",
      "Epoch  929/2000 train_loss: 1.592739 acc: 0.862200\n",
      "Epoch  930/2000 train_loss: 1.592704 acc: 0.862300\n",
      "Epoch  931/2000 train_loss: 1.592680 acc: 0.862200\n",
      "Epoch  932/2000 train_loss: 1.592642 acc: 0.862200\n",
      "Epoch  933/2000 train_loss: 1.592619 acc: 0.862000\n",
      "Epoch  934/2000 train_loss: 1.592582 acc: 0.862100\n",
      "Epoch  935/2000 train_loss: 1.592554 acc: 0.862300\n",
      "Epoch  936/2000 train_loss: 1.592512 acc: 0.862100\n",
      "Epoch  937/2000 train_loss: 1.592485 acc: 0.861800\n",
      "Epoch  938/2000 train_loss: 1.592459 acc: 0.862400\n",
      "Epoch  939/2000 train_loss: 1.592419 acc: 0.862300\n",
      "Epoch  940/2000 train_loss: 1.592389 acc: 0.862400\n",
      "Epoch  941/2000 train_loss: 1.592357 acc: 0.862200\n",
      "Epoch  942/2000 train_loss: 1.592312 acc: 0.862200\n",
      "Epoch  943/2000 train_loss: 1.592297 acc: 0.861900\n",
      "Epoch  944/2000 train_loss: 1.592265 acc: 0.862400\n",
      "Epoch  945/2000 train_loss: 1.592229 acc: 0.861900\n",
      "Epoch  946/2000 train_loss: 1.592197 acc: 0.862400\n",
      "Epoch  947/2000 train_loss: 1.592168 acc: 0.862200\n",
      "Epoch  948/2000 train_loss: 1.592120 acc: 0.862200\n",
      "Epoch  949/2000 train_loss: 1.592093 acc: 0.862300\n",
      "Epoch  950/2000 train_loss: 1.592062 acc: 0.862700\n",
      "Epoch  951/2000 train_loss: 1.592044 acc: 0.862500\n",
      "Epoch  952/2000 train_loss: 1.591990 acc: 0.862500\n",
      "Epoch  953/2000 train_loss: 1.591979 acc: 0.862600\n",
      "Epoch  954/2000 train_loss: 1.591944 acc: 0.862500\n",
      "Epoch  955/2000 train_loss: 1.591900 acc: 0.862700\n",
      "Epoch  956/2000 train_loss: 1.591869 acc: 0.863300\n",
      "Epoch  957/2000 train_loss: 1.591847 acc: 0.862500\n",
      "Epoch  958/2000 train_loss: 1.591802 acc: 0.862700\n",
      "Epoch  959/2000 train_loss: 1.591789 acc: 0.862800\n",
      "Epoch  960/2000 train_loss: 1.591748 acc: 0.862800\n",
      "Epoch  961/2000 train_loss: 1.591731 acc: 0.863100\n",
      "Epoch  962/2000 train_loss: 1.591679 acc: 0.862600\n",
      "Epoch  963/2000 train_loss: 1.591660 acc: 0.862800\n",
      "Epoch  964/2000 train_loss: 1.591633 acc: 0.862900\n",
      "Epoch  965/2000 train_loss: 1.591570 acc: 0.862800\n",
      "Epoch  966/2000 train_loss: 1.591561 acc: 0.862500\n",
      "Epoch  967/2000 train_loss: 1.591529 acc: 0.862600\n",
      "Epoch  968/2000 train_loss: 1.591493 acc: 0.862800\n",
      "Epoch  969/2000 train_loss: 1.591474 acc: 0.863300\n",
      "Epoch  970/2000 train_loss: 1.591435 acc: 0.862800\n",
      "Epoch  971/2000 train_loss: 1.591399 acc: 0.863100\n",
      "Epoch  972/2000 train_loss: 1.591362 acc: 0.863100\n",
      "Epoch  973/2000 train_loss: 1.591333 acc: 0.863700\n",
      "Epoch  974/2000 train_loss: 1.591311 acc: 0.862800\n",
      "Epoch  975/2000 train_loss: 1.591279 acc: 0.863100\n",
      "Epoch  976/2000 train_loss: 1.591240 acc: 0.863400\n",
      "Epoch  977/2000 train_loss: 1.591200 acc: 0.863300\n",
      "Epoch  978/2000 train_loss: 1.591185 acc: 0.863100\n",
      "Epoch  979/2000 train_loss: 1.591158 acc: 0.863400\n",
      "Epoch  980/2000 train_loss: 1.591105 acc: 0.863400\n",
      "Epoch  981/2000 train_loss: 1.591078 acc: 0.863300\n",
      "Epoch  982/2000 train_loss: 1.591049 acc: 0.863400\n",
      "Epoch  983/2000 train_loss: 1.591015 acc: 0.863200\n",
      "Epoch  984/2000 train_loss: 1.590993 acc: 0.863400\n",
      "Epoch  985/2000 train_loss: 1.590961 acc: 0.863000\n",
      "Epoch  986/2000 train_loss: 1.590921 acc: 0.863300\n",
      "Epoch  987/2000 train_loss: 1.590901 acc: 0.863000\n",
      "Epoch  988/2000 train_loss: 1.590861 acc: 0.863400\n",
      "Epoch  989/2000 train_loss: 1.590833 acc: 0.863400\n",
      "Epoch  990/2000 train_loss: 1.590797 acc: 0.863500\n",
      "Epoch  991/2000 train_loss: 1.590764 acc: 0.863500\n",
      "Epoch  992/2000 train_loss: 1.590744 acc: 0.863700\n",
      "Epoch  993/2000 train_loss: 1.590711 acc: 0.863500\n",
      "Epoch  994/2000 train_loss: 1.590677 acc: 0.863600\n",
      "Epoch  995/2000 train_loss: 1.590646 acc: 0.863500\n",
      "Epoch  996/2000 train_loss: 1.590618 acc: 0.863400\n",
      "Epoch  997/2000 train_loss: 1.590583 acc: 0.863900\n",
      "Epoch  998/2000 train_loss: 1.590565 acc: 0.863300\n",
      "Epoch  999/2000 train_loss: 1.590532 acc: 0.863500\n",
      "Epoch 1000/2000 train_loss: 1.590499 acc: 0.864100\n",
      "Epoch 1001/2000 train_loss: 1.590473 acc: 0.863600\n",
      "Epoch 1002/2000 train_loss: 1.590444 acc: 0.863600\n",
      "Epoch 1003/2000 train_loss: 1.590405 acc: 0.863800\n",
      "Epoch 1004/2000 train_loss: 1.590381 acc: 0.863500\n",
      "Epoch 1005/2000 train_loss: 1.590331 acc: 0.863400\n",
      "Epoch 1006/2000 train_loss: 1.590318 acc: 0.863800\n",
      "Epoch 1007/2000 train_loss: 1.590280 acc: 0.863600\n",
      "Epoch 1008/2000 train_loss: 1.590245 acc: 0.863600\n",
      "Epoch 1009/2000 train_loss: 1.590219 acc: 0.863700\n",
      "Epoch 1010/2000 train_loss: 1.590200 acc: 0.863500\n",
      "Epoch 1011/2000 train_loss: 1.590168 acc: 0.863400\n",
      "Epoch 1012/2000 train_loss: 1.590134 acc: 0.863600\n",
      "Epoch 1013/2000 train_loss: 1.590105 acc: 0.863900\n",
      "Epoch 1014/2000 train_loss: 1.590078 acc: 0.864100\n",
      "Epoch 1015/2000 train_loss: 1.590041 acc: 0.863600\n",
      "Epoch 1016/2000 train_loss: 1.590019 acc: 0.863500\n",
      "Epoch 1017/2000 train_loss: 1.589998 acc: 0.863600\n",
      "Epoch 1018/2000 train_loss: 1.589961 acc: 0.863700\n",
      "Epoch 1019/2000 train_loss: 1.589924 acc: 0.863300\n",
      "Epoch 1020/2000 train_loss: 1.589906 acc: 0.863600\n",
      "Epoch 1021/2000 train_loss: 1.589884 acc: 0.863500\n",
      "Epoch 1022/2000 train_loss: 1.589846 acc: 0.863400\n",
      "Epoch 1023/2000 train_loss: 1.589823 acc: 0.863700\n",
      "Epoch 1024/2000 train_loss: 1.589788 acc: 0.863400\n",
      "Epoch 1025/2000 train_loss: 1.589774 acc: 0.863800\n",
      "Epoch 1026/2000 train_loss: 1.589730 acc: 0.863400\n",
      "Epoch 1027/2000 train_loss: 1.589701 acc: 0.863500\n",
      "Epoch 1028/2000 train_loss: 1.589672 acc: 0.863200\n",
      "Epoch 1029/2000 train_loss: 1.589643 acc: 0.863700\n",
      "Epoch 1030/2000 train_loss: 1.589615 acc: 0.863400\n",
      "Epoch 1031/2000 train_loss: 1.589577 acc: 0.863700\n",
      "Epoch 1032/2000 train_loss: 1.589565 acc: 0.863400\n",
      "Epoch 1033/2000 train_loss: 1.589524 acc: 0.863500\n",
      "Epoch 1034/2000 train_loss: 1.589505 acc: 0.863400\n",
      "Epoch 1035/2000 train_loss: 1.589470 acc: 0.863200\n",
      "Epoch 1036/2000 train_loss: 1.589453 acc: 0.863200\n",
      "Epoch 1037/2000 train_loss: 1.589413 acc: 0.863600\n",
      "Epoch 1038/2000 train_loss: 1.589389 acc: 0.863700\n",
      "Epoch 1039/2000 train_loss: 1.589353 acc: 0.864000\n",
      "Epoch 1040/2000 train_loss: 1.589328 acc: 0.863600\n",
      "Epoch 1041/2000 train_loss: 1.589310 acc: 0.864000\n",
      "Epoch 1042/2000 train_loss: 1.589268 acc: 0.863600\n",
      "Epoch 1043/2000 train_loss: 1.589240 acc: 0.863400\n",
      "Epoch 1044/2000 train_loss: 1.589224 acc: 0.863800\n",
      "Epoch 1045/2000 train_loss: 1.589190 acc: 0.864000\n",
      "Epoch 1046/2000 train_loss: 1.589165 acc: 0.864000\n",
      "Epoch 1047/2000 train_loss: 1.589138 acc: 0.863600\n",
      "Epoch 1048/2000 train_loss: 1.589117 acc: 0.863500\n",
      "Epoch 1049/2000 train_loss: 1.589078 acc: 0.863300\n",
      "Epoch 1050/2000 train_loss: 1.589056 acc: 0.864000\n",
      "Epoch 1051/2000 train_loss: 1.589028 acc: 0.863900\n",
      "Epoch 1052/2000 train_loss: 1.588991 acc: 0.864000\n",
      "Epoch 1053/2000 train_loss: 1.588969 acc: 0.864200\n",
      "Epoch 1054/2000 train_loss: 1.588948 acc: 0.864100\n",
      "Epoch 1055/2000 train_loss: 1.588914 acc: 0.864000\n",
      "Epoch 1056/2000 train_loss: 1.588883 acc: 0.864000\n",
      "Epoch 1057/2000 train_loss: 1.588856 acc: 0.864100\n",
      "Epoch 1058/2000 train_loss: 1.588835 acc: 0.864300\n",
      "Epoch 1059/2000 train_loss: 1.588816 acc: 0.863800\n",
      "Epoch 1060/2000 train_loss: 1.588786 acc: 0.864500\n",
      "Epoch 1061/2000 train_loss: 1.588748 acc: 0.864300\n",
      "Epoch 1062/2000 train_loss: 1.588719 acc: 0.864000\n",
      "Epoch 1063/2000 train_loss: 1.588701 acc: 0.863900\n",
      "Epoch 1064/2000 train_loss: 1.588686 acc: 0.864300\n",
      "Epoch 1065/2000 train_loss: 1.588645 acc: 0.864000\n",
      "Epoch 1066/2000 train_loss: 1.588625 acc: 0.864500\n",
      "Epoch 1067/2000 train_loss: 1.588598 acc: 0.864300\n",
      "Epoch 1068/2000 train_loss: 1.588575 acc: 0.864200\n",
      "Epoch 1069/2000 train_loss: 1.588551 acc: 0.863900\n",
      "Epoch 1070/2000 train_loss: 1.588516 acc: 0.863900\n",
      "Epoch 1071/2000 train_loss: 1.588506 acc: 0.864200\n",
      "Epoch 1072/2000 train_loss: 1.588472 acc: 0.864200\n",
      "Epoch 1073/2000 train_loss: 1.588439 acc: 0.864000\n",
      "Epoch 1074/2000 train_loss: 1.588431 acc: 0.864300\n",
      "Epoch 1075/2000 train_loss: 1.588388 acc: 0.864500\n",
      "Epoch 1076/2000 train_loss: 1.588371 acc: 0.864400\n",
      "Epoch 1077/2000 train_loss: 1.588354 acc: 0.864500\n",
      "Epoch 1078/2000 train_loss: 1.588317 acc: 0.864400\n",
      "Epoch 1079/2000 train_loss: 1.588289 acc: 0.864100\n",
      "Epoch 1080/2000 train_loss: 1.588266 acc: 0.864500\n",
      "Epoch 1081/2000 train_loss: 1.588234 acc: 0.864500\n",
      "Epoch 1082/2000 train_loss: 1.588216 acc: 0.864700\n",
      "Epoch 1083/2000 train_loss: 1.588196 acc: 0.864400\n",
      "Epoch 1084/2000 train_loss: 1.588157 acc: 0.864600\n",
      "Epoch 1085/2000 train_loss: 1.588136 acc: 0.864400\n",
      "Epoch 1086/2000 train_loss: 1.588112 acc: 0.864400\n",
      "Epoch 1087/2000 train_loss: 1.588087 acc: 0.864700\n",
      "Epoch 1088/2000 train_loss: 1.588062 acc: 0.863900\n",
      "Epoch 1089/2000 train_loss: 1.588038 acc: 0.864600\n",
      "Epoch 1090/2000 train_loss: 1.588022 acc: 0.864500\n",
      "Epoch 1091/2000 train_loss: 1.587984 acc: 0.864100\n",
      "Epoch 1092/2000 train_loss: 1.587959 acc: 0.864100\n",
      "Epoch 1093/2000 train_loss: 1.587934 acc: 0.864200\n",
      "Epoch 1094/2000 train_loss: 1.587919 acc: 0.864600\n",
      "Epoch 1095/2000 train_loss: 1.587880 acc: 0.864700\n",
      "Epoch 1096/2000 train_loss: 1.587868 acc: 0.864600\n",
      "Epoch 1097/2000 train_loss: 1.587830 acc: 0.864500\n",
      "Epoch 1098/2000 train_loss: 1.587816 acc: 0.864900\n",
      "Epoch 1099/2000 train_loss: 1.587791 acc: 0.864800\n",
      "Epoch 1100/2000 train_loss: 1.587773 acc: 0.864500\n",
      "Epoch 1101/2000 train_loss: 1.587738 acc: 0.864800\n",
      "Epoch 1102/2000 train_loss: 1.587718 acc: 0.864800\n",
      "Epoch 1103/2000 train_loss: 1.587685 acc: 0.864400\n",
      "Epoch 1104/2000 train_loss: 1.587676 acc: 0.865000\n",
      "Epoch 1105/2000 train_loss: 1.587645 acc: 0.864300\n",
      "Epoch 1106/2000 train_loss: 1.587620 acc: 0.864700\n",
      "Epoch 1107/2000 train_loss: 1.587600 acc: 0.864400\n",
      "Epoch 1108/2000 train_loss: 1.587564 acc: 0.864500\n",
      "Epoch 1109/2000 train_loss: 1.587547 acc: 0.864700\n",
      "Epoch 1110/2000 train_loss: 1.587520 acc: 0.864600\n",
      "Epoch 1111/2000 train_loss: 1.587504 acc: 0.864800\n",
      "Epoch 1112/2000 train_loss: 1.587474 acc: 0.864600\n",
      "Epoch 1113/2000 train_loss: 1.587463 acc: 0.864800\n",
      "Epoch 1114/2000 train_loss: 1.587423 acc: 0.864700\n",
      "Epoch 1115/2000 train_loss: 1.587409 acc: 0.865200\n",
      "Epoch 1116/2000 train_loss: 1.587370 acc: 0.865000\n",
      "Epoch 1117/2000 train_loss: 1.587345 acc: 0.864800\n",
      "Epoch 1118/2000 train_loss: 1.587327 acc: 0.865100\n",
      "Epoch 1119/2000 train_loss: 1.587305 acc: 0.864600\n",
      "Epoch 1120/2000 train_loss: 1.587282 acc: 0.864700\n",
      "Epoch 1121/2000 train_loss: 1.587243 acc: 0.865200\n",
      "Epoch 1122/2000 train_loss: 1.587250 acc: 0.865000\n",
      "Epoch 1123/2000 train_loss: 1.587209 acc: 0.865100\n",
      "Epoch 1124/2000 train_loss: 1.587184 acc: 0.865300\n",
      "Epoch 1125/2000 train_loss: 1.587167 acc: 0.864700\n",
      "Epoch 1126/2000 train_loss: 1.587140 acc: 0.865200\n",
      "Epoch 1127/2000 train_loss: 1.587130 acc: 0.865000\n",
      "Epoch 1128/2000 train_loss: 1.587103 acc: 0.865400\n",
      "Epoch 1129/2000 train_loss: 1.587072 acc: 0.864900\n",
      "Epoch 1130/2000 train_loss: 1.587051 acc: 0.864900\n",
      "Epoch 1131/2000 train_loss: 1.587031 acc: 0.865300\n",
      "Epoch 1132/2000 train_loss: 1.587006 acc: 0.865100\n",
      "Epoch 1133/2000 train_loss: 1.586969 acc: 0.864700\n",
      "Epoch 1134/2000 train_loss: 1.586954 acc: 0.865100\n",
      "Epoch 1135/2000 train_loss: 1.586936 acc: 0.865000\n",
      "Epoch 1136/2000 train_loss: 1.586919 acc: 0.865300\n",
      "Epoch 1137/2000 train_loss: 1.586901 acc: 0.864800\n",
      "Epoch 1138/2000 train_loss: 1.586872 acc: 0.865100\n",
      "Epoch 1139/2000 train_loss: 1.586844 acc: 0.865400\n",
      "Epoch 1140/2000 train_loss: 1.586819 acc: 0.865400\n",
      "Epoch 1141/2000 train_loss: 1.586798 acc: 0.865400\n",
      "Epoch 1142/2000 train_loss: 1.586784 acc: 0.865100\n",
      "Epoch 1143/2000 train_loss: 1.586751 acc: 0.865700\n",
      "Epoch 1144/2000 train_loss: 1.586728 acc: 0.865200\n",
      "Epoch 1145/2000 train_loss: 1.586717 acc: 0.865200\n",
      "Epoch 1146/2000 train_loss: 1.586688 acc: 0.865200\n",
      "Epoch 1147/2000 train_loss: 1.586663 acc: 0.865200\n",
      "Epoch 1148/2000 train_loss: 1.586640 acc: 0.865500\n",
      "Epoch 1149/2000 train_loss: 1.586633 acc: 0.865500\n",
      "Epoch 1150/2000 train_loss: 1.586609 acc: 0.865200\n",
      "Epoch 1151/2000 train_loss: 1.586580 acc: 0.865300\n",
      "Epoch 1152/2000 train_loss: 1.586559 acc: 0.865400\n",
      "Epoch 1153/2000 train_loss: 1.586531 acc: 0.865100\n",
      "Epoch 1154/2000 train_loss: 1.586514 acc: 0.865400\n",
      "Epoch 1155/2000 train_loss: 1.586498 acc: 0.865400\n",
      "Epoch 1156/2000 train_loss: 1.586473 acc: 0.865300\n",
      "Epoch 1157/2000 train_loss: 1.586451 acc: 0.865500\n",
      "Epoch 1158/2000 train_loss: 1.586432 acc: 0.865200\n",
      "Epoch 1159/2000 train_loss: 1.586390 acc: 0.865600\n",
      "Epoch 1160/2000 train_loss: 1.586392 acc: 0.865500\n",
      "Epoch 1161/2000 train_loss: 1.586367 acc: 0.865400\n",
      "Epoch 1162/2000 train_loss: 1.586335 acc: 0.865300\n",
      "Epoch 1163/2000 train_loss: 1.586323 acc: 0.865200\n",
      "Epoch 1164/2000 train_loss: 1.586282 acc: 0.865100\n",
      "Epoch 1165/2000 train_loss: 1.586274 acc: 0.865100\n",
      "Epoch 1166/2000 train_loss: 1.586249 acc: 0.865200\n",
      "Epoch 1167/2000 train_loss: 1.586234 acc: 0.865500\n",
      "Epoch 1168/2000 train_loss: 1.586211 acc: 0.865600\n",
      "Epoch 1169/2000 train_loss: 1.586184 acc: 0.865300\n",
      "Epoch 1170/2000 train_loss: 1.586170 acc: 0.865100\n",
      "Epoch 1171/2000 train_loss: 1.586151 acc: 0.865700\n",
      "Epoch 1172/2000 train_loss: 1.586121 acc: 0.865500\n",
      "Epoch 1173/2000 train_loss: 1.586100 acc: 0.865200\n",
      "Epoch 1174/2000 train_loss: 1.586079 acc: 0.865400\n",
      "Epoch 1175/2000 train_loss: 1.586059 acc: 0.865800\n",
      "Epoch 1176/2000 train_loss: 1.586036 acc: 0.865200\n",
      "Epoch 1177/2000 train_loss: 1.586016 acc: 0.865300\n",
      "Epoch 1178/2000 train_loss: 1.585985 acc: 0.865400\n",
      "Epoch 1179/2000 train_loss: 1.585981 acc: 0.865500\n",
      "Epoch 1180/2000 train_loss: 1.585957 acc: 0.865600\n",
      "Epoch 1181/2000 train_loss: 1.585937 acc: 0.865600\n",
      "Epoch 1182/2000 train_loss: 1.585907 acc: 0.865600\n",
      "Epoch 1183/2000 train_loss: 1.585888 acc: 0.865400\n",
      "Epoch 1184/2000 train_loss: 1.585859 acc: 0.865600\n",
      "Epoch 1185/2000 train_loss: 1.585854 acc: 0.865400\n",
      "Epoch 1186/2000 train_loss: 1.585828 acc: 0.865600\n",
      "Epoch 1187/2000 train_loss: 1.585800 acc: 0.865900\n",
      "Epoch 1188/2000 train_loss: 1.585783 acc: 0.865200\n",
      "Epoch 1189/2000 train_loss: 1.585761 acc: 0.865600\n",
      "Epoch 1190/2000 train_loss: 1.585730 acc: 0.865400\n",
      "Epoch 1191/2000 train_loss: 1.585733 acc: 0.865500\n",
      "Epoch 1192/2000 train_loss: 1.585687 acc: 0.865300\n",
      "Epoch 1193/2000 train_loss: 1.585685 acc: 0.865700\n",
      "Epoch 1194/2000 train_loss: 1.585661 acc: 0.865600\n",
      "Epoch 1195/2000 train_loss: 1.585639 acc: 0.865600\n",
      "Epoch 1196/2000 train_loss: 1.585605 acc: 0.865600\n",
      "Epoch 1197/2000 train_loss: 1.585597 acc: 0.865800\n",
      "Epoch 1198/2000 train_loss: 1.585570 acc: 0.865100\n",
      "Epoch 1199/2000 train_loss: 1.585554 acc: 0.865600\n",
      "Epoch 1200/2000 train_loss: 1.585527 acc: 0.866000\n",
      "Epoch 1201/2000 train_loss: 1.585506 acc: 0.865200\n",
      "Epoch 1202/2000 train_loss: 1.585491 acc: 0.865000\n",
      "Epoch 1203/2000 train_loss: 1.585468 acc: 0.865700\n",
      "Epoch 1204/2000 train_loss: 1.585454 acc: 0.865400\n",
      "Epoch 1205/2000 train_loss: 1.585431 acc: 0.865900\n",
      "Epoch 1206/2000 train_loss: 1.585404 acc: 0.865700\n",
      "Epoch 1207/2000 train_loss: 1.585380 acc: 0.865500\n",
      "Epoch 1208/2000 train_loss: 1.585366 acc: 0.865700\n",
      "Epoch 1209/2000 train_loss: 1.585351 acc: 0.865900\n",
      "Epoch 1210/2000 train_loss: 1.585318 acc: 0.865300\n",
      "Epoch 1211/2000 train_loss: 1.585297 acc: 0.865700\n",
      "Epoch 1212/2000 train_loss: 1.585292 acc: 0.865600\n",
      "Epoch 1213/2000 train_loss: 1.585256 acc: 0.865600\n",
      "Epoch 1214/2000 train_loss: 1.585240 acc: 0.866100\n",
      "Epoch 1215/2000 train_loss: 1.585222 acc: 0.865700\n",
      "Epoch 1216/2000 train_loss: 1.585184 acc: 0.866100\n",
      "Epoch 1217/2000 train_loss: 1.585182 acc: 0.865800\n",
      "Epoch 1218/2000 train_loss: 1.585160 acc: 0.865900\n",
      "Epoch 1219/2000 train_loss: 1.585134 acc: 0.866100\n",
      "Epoch 1220/2000 train_loss: 1.585124 acc: 0.866100\n",
      "Epoch 1221/2000 train_loss: 1.585085 acc: 0.866000\n",
      "Epoch 1222/2000 train_loss: 1.585082 acc: 0.865700\n",
      "Epoch 1223/2000 train_loss: 1.585053 acc: 0.866300\n",
      "Epoch 1224/2000 train_loss: 1.585032 acc: 0.866100\n",
      "Epoch 1225/2000 train_loss: 1.585013 acc: 0.865900\n",
      "Epoch 1226/2000 train_loss: 1.584988 acc: 0.865600\n",
      "Epoch 1227/2000 train_loss: 1.584977 acc: 0.865600\n",
      "Epoch 1228/2000 train_loss: 1.584955 acc: 0.866100\n",
      "Epoch 1229/2000 train_loss: 1.584940 acc: 0.865800\n",
      "Epoch 1230/2000 train_loss: 1.584923 acc: 0.865900\n",
      "Epoch 1231/2000 train_loss: 1.584902 acc: 0.865400\n",
      "Epoch 1232/2000 train_loss: 1.584870 acc: 0.865800\n",
      "Epoch 1233/2000 train_loss: 1.584844 acc: 0.865900\n",
      "Epoch 1234/2000 train_loss: 1.584834 acc: 0.865800\n",
      "Epoch 1235/2000 train_loss: 1.584813 acc: 0.865700\n",
      "Epoch 1236/2000 train_loss: 1.584801 acc: 0.865800\n",
      "Epoch 1237/2000 train_loss: 1.584781 acc: 0.866200\n",
      "Epoch 1238/2000 train_loss: 1.584755 acc: 0.865900\n",
      "Epoch 1239/2000 train_loss: 1.584727 acc: 0.866000\n",
      "Epoch 1240/2000 train_loss: 1.584713 acc: 0.866000\n",
      "Epoch 1241/2000 train_loss: 1.584699 acc: 0.866400\n",
      "Epoch 1242/2000 train_loss: 1.584686 acc: 0.866200\n",
      "Epoch 1243/2000 train_loss: 1.584663 acc: 0.865800\n",
      "Epoch 1244/2000 train_loss: 1.584639 acc: 0.865800\n",
      "Epoch 1245/2000 train_loss: 1.584617 acc: 0.865900\n",
      "Epoch 1246/2000 train_loss: 1.584610 acc: 0.866300\n",
      "Epoch 1247/2000 train_loss: 1.584577 acc: 0.866100\n",
      "Epoch 1248/2000 train_loss: 1.584561 acc: 0.865900\n",
      "Epoch 1249/2000 train_loss: 1.584547 acc: 0.866200\n",
      "Epoch 1250/2000 train_loss: 1.584519 acc: 0.866400\n",
      "Epoch 1251/2000 train_loss: 1.584496 acc: 0.866400\n",
      "Epoch 1252/2000 train_loss: 1.584488 acc: 0.866000\n",
      "Epoch 1253/2000 train_loss: 1.584465 acc: 0.866300\n",
      "Epoch 1254/2000 train_loss: 1.584445 acc: 0.866500\n",
      "Epoch 1255/2000 train_loss: 1.584427 acc: 0.866600\n",
      "Epoch 1256/2000 train_loss: 1.584407 acc: 0.866200\n",
      "Epoch 1257/2000 train_loss: 1.584376 acc: 0.866100\n",
      "Epoch 1258/2000 train_loss: 1.584367 acc: 0.866200\n",
      "Epoch 1259/2000 train_loss: 1.584348 acc: 0.866200\n",
      "Epoch 1260/2000 train_loss: 1.584314 acc: 0.866300\n",
      "Epoch 1261/2000 train_loss: 1.584320 acc: 0.866300\n",
      "Epoch 1262/2000 train_loss: 1.584284 acc: 0.866300\n",
      "Epoch 1263/2000 train_loss: 1.584270 acc: 0.866100\n",
      "Epoch 1264/2000 train_loss: 1.584257 acc: 0.866500\n",
      "Epoch 1265/2000 train_loss: 1.584228 acc: 0.866200\n",
      "Epoch 1266/2000 train_loss: 1.584211 acc: 0.866400\n",
      "Epoch 1267/2000 train_loss: 1.584185 acc: 0.866600\n",
      "Epoch 1268/2000 train_loss: 1.584162 acc: 0.866200\n",
      "Epoch 1269/2000 train_loss: 1.584154 acc: 0.866400\n",
      "Epoch 1270/2000 train_loss: 1.584136 acc: 0.866800\n",
      "Epoch 1271/2000 train_loss: 1.584110 acc: 0.866300\n",
      "Epoch 1272/2000 train_loss: 1.584089 acc: 0.866400\n",
      "Epoch 1273/2000 train_loss: 1.584075 acc: 0.866600\n",
      "Epoch 1274/2000 train_loss: 1.584059 acc: 0.866100\n",
      "Epoch 1275/2000 train_loss: 1.584031 acc: 0.865900\n",
      "Epoch 1276/2000 train_loss: 1.584027 acc: 0.866700\n",
      "Epoch 1277/2000 train_loss: 1.584001 acc: 0.866200\n",
      "Epoch 1278/2000 train_loss: 1.583974 acc: 0.866700\n",
      "Epoch 1279/2000 train_loss: 1.583954 acc: 0.866600\n",
      "Epoch 1280/2000 train_loss: 1.583941 acc: 0.867000\n",
      "Epoch 1281/2000 train_loss: 1.583922 acc: 0.867000\n",
      "Epoch 1282/2000 train_loss: 1.583906 acc: 0.866200\n",
      "Epoch 1283/2000 train_loss: 1.583880 acc: 0.866500\n",
      "Epoch 1284/2000 train_loss: 1.583868 acc: 0.866300\n",
      "Epoch 1285/2000 train_loss: 1.583840 acc: 0.866700\n",
      "Epoch 1286/2000 train_loss: 1.583817 acc: 0.866700\n",
      "Epoch 1287/2000 train_loss: 1.583810 acc: 0.866600\n",
      "Epoch 1288/2000 train_loss: 1.583793 acc: 0.866600\n",
      "Epoch 1289/2000 train_loss: 1.583767 acc: 0.866400\n",
      "Epoch 1290/2000 train_loss: 1.583748 acc: 0.866600\n",
      "Epoch 1291/2000 train_loss: 1.583724 acc: 0.866500\n",
      "Epoch 1292/2000 train_loss: 1.583704 acc: 0.866700\n",
      "Epoch 1293/2000 train_loss: 1.583686 acc: 0.866700\n",
      "Epoch 1294/2000 train_loss: 1.583662 acc: 0.866800\n",
      "Epoch 1295/2000 train_loss: 1.583647 acc: 0.867000\n",
      "Epoch 1296/2000 train_loss: 1.583627 acc: 0.866200\n",
      "Epoch 1297/2000 train_loss: 1.583609 acc: 0.866800\n",
      "Epoch 1298/2000 train_loss: 1.583590 acc: 0.866700\n",
      "Epoch 1299/2000 train_loss: 1.583578 acc: 0.866600\n",
      "Epoch 1300/2000 train_loss: 1.583561 acc: 0.866700\n",
      "Epoch 1301/2000 train_loss: 1.583538 acc: 0.866700\n",
      "Epoch 1302/2000 train_loss: 1.583522 acc: 0.866800\n",
      "Epoch 1303/2000 train_loss: 1.583495 acc: 0.867100\n",
      "Epoch 1304/2000 train_loss: 1.583467 acc: 0.867100\n",
      "Epoch 1305/2000 train_loss: 1.583456 acc: 0.866700\n",
      "Epoch 1306/2000 train_loss: 1.583438 acc: 0.867100\n",
      "Epoch 1307/2000 train_loss: 1.583425 acc: 0.867000\n",
      "Epoch 1308/2000 train_loss: 1.583406 acc: 0.866900\n",
      "Epoch 1309/2000 train_loss: 1.583375 acc: 0.866900\n",
      "Epoch 1310/2000 train_loss: 1.583363 acc: 0.866600\n",
      "Epoch 1311/2000 train_loss: 1.583348 acc: 0.867100\n",
      "Epoch 1312/2000 train_loss: 1.583325 acc: 0.866600\n",
      "Epoch 1313/2000 train_loss: 1.583313 acc: 0.867000\n",
      "Epoch 1314/2000 train_loss: 1.583282 acc: 0.867000\n",
      "Epoch 1315/2000 train_loss: 1.583273 acc: 0.866900\n",
      "Epoch 1316/2000 train_loss: 1.583248 acc: 0.866900\n",
      "Epoch 1317/2000 train_loss: 1.583235 acc: 0.866800\n",
      "Epoch 1318/2000 train_loss: 1.583209 acc: 0.867000\n",
      "Epoch 1319/2000 train_loss: 1.583192 acc: 0.867000\n",
      "Epoch 1320/2000 train_loss: 1.583173 acc: 0.866800\n",
      "Epoch 1321/2000 train_loss: 1.583159 acc: 0.866900\n",
      "Epoch 1322/2000 train_loss: 1.583151 acc: 0.866900\n",
      "Epoch 1323/2000 train_loss: 1.583121 acc: 0.866900\n",
      "Epoch 1324/2000 train_loss: 1.583099 acc: 0.867100\n",
      "Epoch 1325/2000 train_loss: 1.583088 acc: 0.866800\n",
      "Epoch 1326/2000 train_loss: 1.583070 acc: 0.866800\n",
      "Epoch 1327/2000 train_loss: 1.583040 acc: 0.866900\n",
      "Epoch 1328/2000 train_loss: 1.583031 acc: 0.867000\n",
      "Epoch 1329/2000 train_loss: 1.583004 acc: 0.867200\n",
      "Epoch 1330/2000 train_loss: 1.582996 acc: 0.866500\n",
      "Epoch 1331/2000 train_loss: 1.582973 acc: 0.866700\n",
      "Epoch 1332/2000 train_loss: 1.582966 acc: 0.867100\n",
      "Epoch 1333/2000 train_loss: 1.582929 acc: 0.866900\n",
      "Epoch 1334/2000 train_loss: 1.582919 acc: 0.866700\n",
      "Epoch 1335/2000 train_loss: 1.582908 acc: 0.867000\n",
      "Epoch 1336/2000 train_loss: 1.582886 acc: 0.866800\n",
      "Epoch 1337/2000 train_loss: 1.582868 acc: 0.867000\n",
      "Epoch 1338/2000 train_loss: 1.582850 acc: 0.866800\n",
      "Epoch 1339/2000 train_loss: 1.582833 acc: 0.866800\n",
      "Epoch 1340/2000 train_loss: 1.582809 acc: 0.867100\n",
      "Epoch 1341/2000 train_loss: 1.582801 acc: 0.866800\n",
      "Epoch 1342/2000 train_loss: 1.582772 acc: 0.866600\n",
      "Epoch 1343/2000 train_loss: 1.582755 acc: 0.866700\n",
      "Epoch 1344/2000 train_loss: 1.582734 acc: 0.866800\n",
      "Epoch 1345/2000 train_loss: 1.582720 acc: 0.866700\n",
      "Epoch 1346/2000 train_loss: 1.582693 acc: 0.867000\n",
      "Epoch 1347/2000 train_loss: 1.582685 acc: 0.867000\n",
      "Epoch 1348/2000 train_loss: 1.582669 acc: 0.867000\n",
      "Epoch 1349/2000 train_loss: 1.582651 acc: 0.866700\n",
      "Epoch 1350/2000 train_loss: 1.582618 acc: 0.867300\n",
      "Epoch 1351/2000 train_loss: 1.582612 acc: 0.866700\n",
      "Epoch 1352/2000 train_loss: 1.582597 acc: 0.866800\n",
      "Epoch 1353/2000 train_loss: 1.582566 acc: 0.867100\n",
      "Epoch 1354/2000 train_loss: 1.582552 acc: 0.866600\n",
      "Epoch 1355/2000 train_loss: 1.582543 acc: 0.867000\n",
      "Epoch 1356/2000 train_loss: 1.582521 acc: 0.867300\n",
      "Epoch 1357/2000 train_loss: 1.582497 acc: 0.867100\n",
      "Epoch 1358/2000 train_loss: 1.582494 acc: 0.866900\n",
      "Epoch 1359/2000 train_loss: 1.582471 acc: 0.866800\n",
      "Epoch 1360/2000 train_loss: 1.582456 acc: 0.866500\n",
      "Epoch 1361/2000 train_loss: 1.582437 acc: 0.866900\n",
      "Epoch 1362/2000 train_loss: 1.582430 acc: 0.867500\n",
      "Epoch 1363/2000 train_loss: 1.582413 acc: 0.866900\n",
      "Epoch 1364/2000 train_loss: 1.582385 acc: 0.867000\n",
      "Epoch 1365/2000 train_loss: 1.582371 acc: 0.867400\n",
      "Epoch 1366/2000 train_loss: 1.582356 acc: 0.867200\n",
      "Epoch 1367/2000 train_loss: 1.582338 acc: 0.867200\n",
      "Epoch 1368/2000 train_loss: 1.582319 acc: 0.867300\n",
      "Epoch 1369/2000 train_loss: 1.582298 acc: 0.867200\n",
      "Epoch 1370/2000 train_loss: 1.582282 acc: 0.867000\n",
      "Epoch 1371/2000 train_loss: 1.582260 acc: 0.867000\n",
      "Epoch 1372/2000 train_loss: 1.582255 acc: 0.867100\n",
      "Epoch 1373/2000 train_loss: 1.582235 acc: 0.867500\n",
      "Epoch 1374/2000 train_loss: 1.582217 acc: 0.867500\n",
      "Epoch 1375/2000 train_loss: 1.582199 acc: 0.867100\n",
      "Epoch 1376/2000 train_loss: 1.582189 acc: 0.867100\n",
      "Epoch 1377/2000 train_loss: 1.582172 acc: 0.867000\n",
      "Epoch 1378/2000 train_loss: 1.582151 acc: 0.867000\n",
      "Epoch 1379/2000 train_loss: 1.582133 acc: 0.867400\n",
      "Epoch 1380/2000 train_loss: 1.582119 acc: 0.867100\n",
      "Epoch 1381/2000 train_loss: 1.582095 acc: 0.867100\n",
      "Epoch 1382/2000 train_loss: 1.582084 acc: 0.867600\n",
      "Epoch 1383/2000 train_loss: 1.582072 acc: 0.867100\n",
      "Epoch 1384/2000 train_loss: 1.582052 acc: 0.867500\n",
      "Epoch 1385/2000 train_loss: 1.582036 acc: 0.867500\n",
      "Epoch 1386/2000 train_loss: 1.582015 acc: 0.867000\n",
      "Epoch 1387/2000 train_loss: 1.582007 acc: 0.867300\n",
      "Epoch 1388/2000 train_loss: 1.581983 acc: 0.867100\n",
      "Epoch 1389/2000 train_loss: 1.581974 acc: 0.867200\n",
      "Epoch 1390/2000 train_loss: 1.581960 acc: 0.867300\n",
      "Epoch 1391/2000 train_loss: 1.581932 acc: 0.867300\n",
      "Epoch 1392/2000 train_loss: 1.581920 acc: 0.867100\n",
      "Epoch 1393/2000 train_loss: 1.581914 acc: 0.867500\n",
      "Epoch 1394/2000 train_loss: 1.581892 acc: 0.867500\n",
      "Epoch 1395/2000 train_loss: 1.581867 acc: 0.867400\n",
      "Epoch 1396/2000 train_loss: 1.581854 acc: 0.867000\n",
      "Epoch 1397/2000 train_loss: 1.581845 acc: 0.867400\n",
      "Epoch 1398/2000 train_loss: 1.581826 acc: 0.867400\n",
      "Epoch 1399/2000 train_loss: 1.581807 acc: 0.867200\n",
      "Epoch 1400/2000 train_loss: 1.581804 acc: 0.867300\n",
      "Epoch 1401/2000 train_loss: 1.581782 acc: 0.867200\n",
      "Epoch 1402/2000 train_loss: 1.581762 acc: 0.867600\n",
      "Epoch 1403/2000 train_loss: 1.581744 acc: 0.867100\n",
      "Epoch 1404/2000 train_loss: 1.581736 acc: 0.867300\n",
      "Epoch 1405/2000 train_loss: 1.581711 acc: 0.867400\n",
      "Epoch 1406/2000 train_loss: 1.581706 acc: 0.867100\n",
      "Epoch 1407/2000 train_loss: 1.581683 acc: 0.867400\n",
      "Epoch 1408/2000 train_loss: 1.581670 acc: 0.867400\n",
      "Epoch 1409/2000 train_loss: 1.581655 acc: 0.867500\n",
      "Epoch 1410/2000 train_loss: 1.581630 acc: 0.867000\n",
      "Epoch 1411/2000 train_loss: 1.581622 acc: 0.867300\n",
      "Epoch 1412/2000 train_loss: 1.581607 acc: 0.867100\n",
      "Epoch 1413/2000 train_loss: 1.581591 acc: 0.867300\n",
      "Epoch 1414/2000 train_loss: 1.581572 acc: 0.867200\n",
      "Epoch 1415/2000 train_loss: 1.581565 acc: 0.867200\n",
      "Epoch 1416/2000 train_loss: 1.581548 acc: 0.867600\n",
      "Epoch 1417/2000 train_loss: 1.581529 acc: 0.867500\n",
      "Epoch 1418/2000 train_loss: 1.581520 acc: 0.867300\n",
      "Epoch 1419/2000 train_loss: 1.581491 acc: 0.867600\n",
      "Epoch 1420/2000 train_loss: 1.581482 acc: 0.867400\n",
      "Epoch 1421/2000 train_loss: 1.581475 acc: 0.867500\n",
      "Epoch 1422/2000 train_loss: 1.581446 acc: 0.867600\n",
      "Epoch 1423/2000 train_loss: 1.581436 acc: 0.867700\n",
      "Epoch 1424/2000 train_loss: 1.581412 acc: 0.867600\n",
      "Epoch 1425/2000 train_loss: 1.581419 acc: 0.867400\n",
      "Epoch 1426/2000 train_loss: 1.581398 acc: 0.867700\n",
      "Epoch 1427/2000 train_loss: 1.581368 acc: 0.867400\n",
      "Epoch 1428/2000 train_loss: 1.581362 acc: 0.867800\n",
      "Epoch 1429/2000 train_loss: 1.581349 acc: 0.867300\n",
      "Epoch 1430/2000 train_loss: 1.581333 acc: 0.867200\n",
      "Epoch 1431/2000 train_loss: 1.581327 acc: 0.867600\n",
      "Epoch 1432/2000 train_loss: 1.581297 acc: 0.867000\n",
      "Epoch 1433/2000 train_loss: 1.581290 acc: 0.867600\n",
      "Epoch 1434/2000 train_loss: 1.581275 acc: 0.867000\n",
      "Epoch 1435/2000 train_loss: 1.581256 acc: 0.867500\n",
      "Epoch 1436/2000 train_loss: 1.581248 acc: 0.867300\n",
      "Epoch 1437/2000 train_loss: 1.581220 acc: 0.867400\n",
      "Epoch 1438/2000 train_loss: 1.581209 acc: 0.867300\n",
      "Epoch 1439/2000 train_loss: 1.581203 acc: 0.867400\n",
      "Epoch 1440/2000 train_loss: 1.581184 acc: 0.867500\n",
      "Epoch 1441/2000 train_loss: 1.581166 acc: 0.867400\n",
      "Epoch 1442/2000 train_loss: 1.581142 acc: 0.867400\n",
      "Epoch 1443/2000 train_loss: 1.581135 acc: 0.867600\n",
      "Epoch 1444/2000 train_loss: 1.581114 acc: 0.867400\n",
      "Epoch 1445/2000 train_loss: 1.581096 acc: 0.867600\n",
      "Epoch 1446/2000 train_loss: 1.581086 acc: 0.867300\n",
      "Epoch 1447/2000 train_loss: 1.581074 acc: 0.867600\n",
      "Epoch 1448/2000 train_loss: 1.581060 acc: 0.867900\n",
      "Epoch 1449/2000 train_loss: 1.581037 acc: 0.867300\n",
      "Epoch 1450/2000 train_loss: 1.581031 acc: 0.867600\n",
      "Epoch 1451/2000 train_loss: 1.581006 acc: 0.867600\n",
      "Epoch 1452/2000 train_loss: 1.581004 acc: 0.868000\n",
      "Epoch 1453/2000 train_loss: 1.580985 acc: 0.867500\n",
      "Epoch 1454/2000 train_loss: 1.580968 acc: 0.867500\n",
      "Epoch 1455/2000 train_loss: 1.580951 acc: 0.867500\n",
      "Epoch 1456/2000 train_loss: 1.580935 acc: 0.867600\n",
      "Epoch 1457/2000 train_loss: 1.580917 acc: 0.867300\n",
      "Epoch 1458/2000 train_loss: 1.580907 acc: 0.867300\n",
      "Epoch 1459/2000 train_loss: 1.580892 acc: 0.867300\n",
      "Epoch 1460/2000 train_loss: 1.580879 acc: 0.867800\n",
      "Epoch 1461/2000 train_loss: 1.580860 acc: 0.867600\n",
      "Epoch 1462/2000 train_loss: 1.580853 acc: 0.867800\n",
      "Epoch 1463/2000 train_loss: 1.580841 acc: 0.867900\n",
      "Epoch 1464/2000 train_loss: 1.580817 acc: 0.867200\n",
      "Epoch 1465/2000 train_loss: 1.580802 acc: 0.867500\n",
      "Epoch 1466/2000 train_loss: 1.580788 acc: 0.867500\n",
      "Epoch 1467/2000 train_loss: 1.580770 acc: 0.867300\n",
      "Epoch 1468/2000 train_loss: 1.580756 acc: 0.867500\n",
      "Epoch 1469/2000 train_loss: 1.580750 acc: 0.867400\n",
      "Epoch 1470/2000 train_loss: 1.580727 acc: 0.867800\n",
      "Epoch 1471/2000 train_loss: 1.580716 acc: 0.867700\n",
      "Epoch 1472/2000 train_loss: 1.580705 acc: 0.867600\n",
      "Epoch 1473/2000 train_loss: 1.580680 acc: 0.867400\n",
      "Epoch 1474/2000 train_loss: 1.580672 acc: 0.867100\n",
      "Epoch 1475/2000 train_loss: 1.580661 acc: 0.867800\n",
      "Epoch 1476/2000 train_loss: 1.580642 acc: 0.867800\n",
      "Epoch 1477/2000 train_loss: 1.580618 acc: 0.867800\n",
      "Epoch 1478/2000 train_loss: 1.580610 acc: 0.867700\n",
      "Epoch 1479/2000 train_loss: 1.580604 acc: 0.867400\n",
      "Epoch 1480/2000 train_loss: 1.580580 acc: 0.867600\n",
      "Epoch 1481/2000 train_loss: 1.580576 acc: 0.867600\n",
      "Epoch 1482/2000 train_loss: 1.580556 acc: 0.867400\n",
      "Epoch 1483/2000 train_loss: 1.580541 acc: 0.867400\n",
      "Epoch 1484/2000 train_loss: 1.580518 acc: 0.867300\n",
      "Epoch 1485/2000 train_loss: 1.580504 acc: 0.867300\n",
      "Epoch 1486/2000 train_loss: 1.580498 acc: 0.867700\n",
      "Epoch 1487/2000 train_loss: 1.580480 acc: 0.867300\n",
      "Epoch 1488/2000 train_loss: 1.580473 acc: 0.867700\n",
      "Epoch 1489/2000 train_loss: 1.580450 acc: 0.867400\n",
      "Epoch 1490/2000 train_loss: 1.580443 acc: 0.867700\n",
      "Epoch 1491/2000 train_loss: 1.580423 acc: 0.867700\n",
      "Epoch 1492/2000 train_loss: 1.580406 acc: 0.867400\n",
      "Epoch 1493/2000 train_loss: 1.580401 acc: 0.867600\n",
      "Epoch 1494/2000 train_loss: 1.580392 acc: 0.867600\n",
      "Epoch 1495/2000 train_loss: 1.580365 acc: 0.867600\n",
      "Epoch 1496/2000 train_loss: 1.580359 acc: 0.867400\n",
      "Epoch 1497/2000 train_loss: 1.580330 acc: 0.868000\n",
      "Epoch 1498/2000 train_loss: 1.580321 acc: 0.867600\n",
      "Epoch 1499/2000 train_loss: 1.580318 acc: 0.867700\n",
      "Epoch 1500/2000 train_loss: 1.580297 acc: 0.867600\n",
      "Epoch 1501/2000 train_loss: 1.580277 acc: 0.867800\n",
      "Epoch 1502/2000 train_loss: 1.580274 acc: 0.867900\n",
      "Epoch 1503/2000 train_loss: 1.580248 acc: 0.868000\n",
      "Epoch 1504/2000 train_loss: 1.580240 acc: 0.867700\n",
      "Epoch 1505/2000 train_loss: 1.580226 acc: 0.867800\n",
      "Epoch 1506/2000 train_loss: 1.580214 acc: 0.867800\n",
      "Epoch 1507/2000 train_loss: 1.580200 acc: 0.867800\n",
      "Epoch 1508/2000 train_loss: 1.580182 acc: 0.867900\n",
      "Epoch 1509/2000 train_loss: 1.580167 acc: 0.868000\n",
      "Epoch 1510/2000 train_loss: 1.580159 acc: 0.868000\n",
      "Epoch 1511/2000 train_loss: 1.580142 acc: 0.867800\n",
      "Epoch 1512/2000 train_loss: 1.580125 acc: 0.868200\n",
      "Epoch 1513/2000 train_loss: 1.580112 acc: 0.867700\n",
      "Epoch 1514/2000 train_loss: 1.580103 acc: 0.868100\n",
      "Epoch 1515/2000 train_loss: 1.580085 acc: 0.867800\n",
      "Epoch 1516/2000 train_loss: 1.580077 acc: 0.867700\n",
      "Epoch 1517/2000 train_loss: 1.580057 acc: 0.867700\n",
      "Epoch 1518/2000 train_loss: 1.580046 acc: 0.868100\n",
      "Epoch 1519/2000 train_loss: 1.580036 acc: 0.867900\n",
      "Epoch 1520/2000 train_loss: 1.580024 acc: 0.867900\n",
      "Epoch 1521/2000 train_loss: 1.580006 acc: 0.868200\n",
      "Epoch 1522/2000 train_loss: 1.579995 acc: 0.868000\n",
      "Epoch 1523/2000 train_loss: 1.579978 acc: 0.867700\n",
      "Epoch 1524/2000 train_loss: 1.579967 acc: 0.867800\n",
      "Epoch 1525/2000 train_loss: 1.579962 acc: 0.868000\n",
      "Epoch 1526/2000 train_loss: 1.579942 acc: 0.868200\n",
      "Epoch 1527/2000 train_loss: 1.579930 acc: 0.868000\n",
      "Epoch 1528/2000 train_loss: 1.579909 acc: 0.868000\n",
      "Epoch 1529/2000 train_loss: 1.579905 acc: 0.868200\n",
      "Epoch 1530/2000 train_loss: 1.579893 acc: 0.868100\n",
      "Epoch 1531/2000 train_loss: 1.579881 acc: 0.868100\n",
      "Epoch 1532/2000 train_loss: 1.579863 acc: 0.867900\n",
      "Epoch 1533/2000 train_loss: 1.579851 acc: 0.868000\n",
      "Epoch 1534/2000 train_loss: 1.579842 acc: 0.868000\n",
      "Epoch 1535/2000 train_loss: 1.579826 acc: 0.868000\n",
      "Epoch 1536/2000 train_loss: 1.579812 acc: 0.868200\n",
      "Epoch 1537/2000 train_loss: 1.579789 acc: 0.868100\n",
      "Epoch 1538/2000 train_loss: 1.579792 acc: 0.868200\n",
      "Epoch 1539/2000 train_loss: 1.579771 acc: 0.868000\n",
      "Epoch 1540/2000 train_loss: 1.579763 acc: 0.868300\n",
      "Epoch 1541/2000 train_loss: 1.579740 acc: 0.868100\n",
      "Epoch 1542/2000 train_loss: 1.579732 acc: 0.868600\n",
      "Epoch 1543/2000 train_loss: 1.579724 acc: 0.868200\n",
      "Epoch 1544/2000 train_loss: 1.579709 acc: 0.868400\n",
      "Epoch 1545/2000 train_loss: 1.579698 acc: 0.868100\n",
      "Epoch 1546/2000 train_loss: 1.579682 acc: 0.867900\n",
      "Epoch 1547/2000 train_loss: 1.579671 acc: 0.868200\n",
      "Epoch 1548/2000 train_loss: 1.579655 acc: 0.868000\n",
      "Epoch 1549/2000 train_loss: 1.579644 acc: 0.868300\n",
      "Epoch 1550/2000 train_loss: 1.579627 acc: 0.868200\n",
      "Epoch 1551/2000 train_loss: 1.579616 acc: 0.868500\n",
      "Epoch 1552/2000 train_loss: 1.579603 acc: 0.868200\n",
      "Epoch 1553/2000 train_loss: 1.579589 acc: 0.868500\n",
      "Epoch 1554/2000 train_loss: 1.579577 acc: 0.868100\n",
      "Epoch 1555/2000 train_loss: 1.579561 acc: 0.868200\n",
      "Epoch 1556/2000 train_loss: 1.579542 acc: 0.868000\n",
      "Epoch 1557/2000 train_loss: 1.579530 acc: 0.868500\n",
      "Epoch 1558/2000 train_loss: 1.579525 acc: 0.868400\n",
      "Epoch 1559/2000 train_loss: 1.579507 acc: 0.868300\n",
      "Epoch 1560/2000 train_loss: 1.579496 acc: 0.868400\n",
      "Epoch 1561/2000 train_loss: 1.579479 acc: 0.868700\n",
      "Epoch 1562/2000 train_loss: 1.579468 acc: 0.867900\n",
      "Epoch 1563/2000 train_loss: 1.579450 acc: 0.868400\n",
      "Epoch 1564/2000 train_loss: 1.579446 acc: 0.868300\n",
      "Epoch 1565/2000 train_loss: 1.579428 acc: 0.868500\n",
      "Epoch 1566/2000 train_loss: 1.579414 acc: 0.868400\n",
      "Epoch 1567/2000 train_loss: 1.579399 acc: 0.868200\n",
      "Epoch 1568/2000 train_loss: 1.579384 acc: 0.868800\n",
      "Epoch 1569/2000 train_loss: 1.579377 acc: 0.868300\n",
      "Epoch 1570/2000 train_loss: 1.579365 acc: 0.868700\n",
      "Epoch 1571/2000 train_loss: 1.579352 acc: 0.868600\n",
      "Epoch 1572/2000 train_loss: 1.579344 acc: 0.868300\n",
      "Epoch 1573/2000 train_loss: 1.579320 acc: 0.868500\n",
      "Epoch 1574/2000 train_loss: 1.579307 acc: 0.868700\n",
      "Epoch 1575/2000 train_loss: 1.579298 acc: 0.868500\n",
      "Epoch 1576/2000 train_loss: 1.579284 acc: 0.869000\n",
      "Epoch 1577/2000 train_loss: 1.579269 acc: 0.868400\n",
      "Epoch 1578/2000 train_loss: 1.579267 acc: 0.868300\n",
      "Epoch 1579/2000 train_loss: 1.579238 acc: 0.868400\n",
      "Epoch 1580/2000 train_loss: 1.579236 acc: 0.868300\n",
      "Epoch 1581/2000 train_loss: 1.579227 acc: 0.868700\n",
      "Epoch 1582/2000 train_loss: 1.579219 acc: 0.868200\n",
      "Epoch 1583/2000 train_loss: 1.579198 acc: 0.868000\n",
      "Epoch 1584/2000 train_loss: 1.579189 acc: 0.868800\n",
      "Epoch 1585/2000 train_loss: 1.579171 acc: 0.868200\n",
      "Epoch 1586/2000 train_loss: 1.579159 acc: 0.868500\n",
      "Epoch 1587/2000 train_loss: 1.579145 acc: 0.868500\n",
      "Epoch 1588/2000 train_loss: 1.579135 acc: 0.868600\n",
      "Epoch 1589/2000 train_loss: 1.579128 acc: 0.868500\n",
      "Epoch 1590/2000 train_loss: 1.579113 acc: 0.868300\n",
      "Epoch 1591/2000 train_loss: 1.579106 acc: 0.868300\n",
      "Epoch 1592/2000 train_loss: 1.579092 acc: 0.868300\n",
      "Epoch 1593/2000 train_loss: 1.579080 acc: 0.868600\n",
      "Epoch 1594/2000 train_loss: 1.579062 acc: 0.868900\n",
      "Epoch 1595/2000 train_loss: 1.579053 acc: 0.868700\n",
      "Epoch 1596/2000 train_loss: 1.579040 acc: 0.868800\n",
      "Epoch 1597/2000 train_loss: 1.579023 acc: 0.868600\n",
      "Epoch 1598/2000 train_loss: 1.579014 acc: 0.868800\n",
      "Epoch 1599/2000 train_loss: 1.579002 acc: 0.868500\n",
      "Epoch 1600/2000 train_loss: 1.578988 acc: 0.868200\n",
      "Epoch 1601/2000 train_loss: 1.578975 acc: 0.868400\n",
      "Epoch 1602/2000 train_loss: 1.578968 acc: 0.868700\n",
      "Epoch 1603/2000 train_loss: 1.578955 acc: 0.868300\n",
      "Epoch 1604/2000 train_loss: 1.578936 acc: 0.868700\n",
      "Epoch 1605/2000 train_loss: 1.578923 acc: 0.869100\n",
      "Epoch 1606/2000 train_loss: 1.578918 acc: 0.868600\n",
      "Epoch 1607/2000 train_loss: 1.578902 acc: 0.868700\n",
      "Epoch 1608/2000 train_loss: 1.578887 acc: 0.868500\n",
      "Epoch 1609/2000 train_loss: 1.578879 acc: 0.868400\n",
      "Epoch 1610/2000 train_loss: 1.578865 acc: 0.868400\n",
      "Epoch 1611/2000 train_loss: 1.578852 acc: 0.868700\n",
      "Epoch 1612/2000 train_loss: 1.578841 acc: 0.868900\n",
      "Epoch 1613/2000 train_loss: 1.578827 acc: 0.868400\n",
      "Epoch 1614/2000 train_loss: 1.578808 acc: 0.868600\n",
      "Epoch 1615/2000 train_loss: 1.578803 acc: 0.868500\n",
      "Epoch 1616/2000 train_loss: 1.578791 acc: 0.868300\n",
      "Epoch 1617/2000 train_loss: 1.578774 acc: 0.868300\n",
      "Epoch 1618/2000 train_loss: 1.578762 acc: 0.868700\n",
      "Epoch 1619/2000 train_loss: 1.578754 acc: 0.868900\n",
      "Epoch 1620/2000 train_loss: 1.578736 acc: 0.868600\n",
      "Epoch 1621/2000 train_loss: 1.578729 acc: 0.868600\n",
      "Epoch 1622/2000 train_loss: 1.578715 acc: 0.868600\n",
      "Epoch 1623/2000 train_loss: 1.578698 acc: 0.868400\n",
      "Epoch 1624/2000 train_loss: 1.578692 acc: 0.868500\n",
      "Epoch 1625/2000 train_loss: 1.578667 acc: 0.868600\n",
      "Epoch 1626/2000 train_loss: 1.578658 acc: 0.868800\n",
      "Epoch 1627/2000 train_loss: 1.578651 acc: 0.868900\n",
      "Epoch 1628/2000 train_loss: 1.578632 acc: 0.868600\n",
      "Epoch 1629/2000 train_loss: 1.578624 acc: 0.868400\n",
      "Epoch 1630/2000 train_loss: 1.578607 acc: 0.868500\n",
      "Epoch 1631/2000 train_loss: 1.578587 acc: 0.868900\n",
      "Epoch 1632/2000 train_loss: 1.578583 acc: 0.868500\n",
      "Epoch 1633/2000 train_loss: 1.578573 acc: 0.868800\n",
      "Epoch 1634/2000 train_loss: 1.578557 acc: 0.868700\n",
      "Epoch 1635/2000 train_loss: 1.578552 acc: 0.868900\n",
      "Epoch 1636/2000 train_loss: 1.578529 acc: 0.868800\n",
      "Epoch 1637/2000 train_loss: 1.578530 acc: 0.868300\n",
      "Epoch 1638/2000 train_loss: 1.578513 acc: 0.869000\n",
      "Epoch 1639/2000 train_loss: 1.578501 acc: 0.868500\n",
      "Epoch 1640/2000 train_loss: 1.578488 acc: 0.868500\n",
      "Epoch 1641/2000 train_loss: 1.578479 acc: 0.868800\n",
      "Epoch 1642/2000 train_loss: 1.578459 acc: 0.868300\n",
      "Epoch 1643/2000 train_loss: 1.578450 acc: 0.868500\n",
      "Epoch 1644/2000 train_loss: 1.578436 acc: 0.868300\n",
      "Epoch 1645/2000 train_loss: 1.578431 acc: 0.868600\n",
      "Epoch 1646/2000 train_loss: 1.578415 acc: 0.868700\n",
      "Epoch 1647/2000 train_loss: 1.578400 acc: 0.868400\n",
      "Epoch 1648/2000 train_loss: 1.578393 acc: 0.868400\n",
      "Epoch 1649/2000 train_loss: 1.578381 acc: 0.868400\n",
      "Epoch 1650/2000 train_loss: 1.578368 acc: 0.868500\n",
      "Epoch 1651/2000 train_loss: 1.578357 acc: 0.868100\n",
      "Epoch 1652/2000 train_loss: 1.578344 acc: 0.868700\n",
      "Epoch 1653/2000 train_loss: 1.578333 acc: 0.868600\n",
      "Epoch 1654/2000 train_loss: 1.578321 acc: 0.868400\n",
      "Epoch 1655/2000 train_loss: 1.578314 acc: 0.868700\n",
      "Epoch 1656/2000 train_loss: 1.578300 acc: 0.868600\n",
      "Epoch 1657/2000 train_loss: 1.578288 acc: 0.868700\n",
      "Epoch 1658/2000 train_loss: 1.578279 acc: 0.868400\n",
      "Epoch 1659/2000 train_loss: 1.578261 acc: 0.868500\n",
      "Epoch 1660/2000 train_loss: 1.578257 acc: 0.868600\n",
      "Epoch 1661/2000 train_loss: 1.578242 acc: 0.868700\n",
      "Epoch 1662/2000 train_loss: 1.578228 acc: 0.868700\n",
      "Epoch 1663/2000 train_loss: 1.578223 acc: 0.868300\n",
      "Epoch 1664/2000 train_loss: 1.578205 acc: 0.868500\n",
      "Epoch 1665/2000 train_loss: 1.578195 acc: 0.868700\n",
      "Epoch 1666/2000 train_loss: 1.578181 acc: 0.868600\n",
      "Epoch 1667/2000 train_loss: 1.578174 acc: 0.868700\n",
      "Epoch 1668/2000 train_loss: 1.578160 acc: 0.869000\n",
      "Epoch 1669/2000 train_loss: 1.578154 acc: 0.868300\n",
      "Epoch 1670/2000 train_loss: 1.578134 acc: 0.868700\n",
      "Epoch 1671/2000 train_loss: 1.578131 acc: 0.868600\n",
      "Epoch 1672/2000 train_loss: 1.578116 acc: 0.868400\n",
      "Epoch 1673/2000 train_loss: 1.578108 acc: 0.868800\n",
      "Epoch 1674/2000 train_loss: 1.578095 acc: 0.868800\n",
      "Epoch 1675/2000 train_loss: 1.578083 acc: 0.868200\n",
      "Epoch 1676/2000 train_loss: 1.578067 acc: 0.868900\n",
      "Epoch 1677/2000 train_loss: 1.578062 acc: 0.868300\n",
      "Epoch 1678/2000 train_loss: 1.578039 acc: 0.868500\n",
      "Epoch 1679/2000 train_loss: 1.578035 acc: 0.868500\n",
      "Epoch 1680/2000 train_loss: 1.578025 acc: 0.869100\n",
      "Epoch 1681/2000 train_loss: 1.578014 acc: 0.868600\n",
      "Epoch 1682/2000 train_loss: 1.578001 acc: 0.868500\n",
      "Epoch 1683/2000 train_loss: 1.577989 acc: 0.868800\n",
      "Epoch 1684/2000 train_loss: 1.577986 acc: 0.868000\n",
      "Epoch 1685/2000 train_loss: 1.577970 acc: 0.868800\n",
      "Epoch 1686/2000 train_loss: 1.577957 acc: 0.868900\n",
      "Epoch 1687/2000 train_loss: 1.577944 acc: 0.868700\n",
      "Epoch 1688/2000 train_loss: 1.577936 acc: 0.868500\n",
      "Epoch 1689/2000 train_loss: 1.577927 acc: 0.868900\n",
      "Epoch 1690/2000 train_loss: 1.577919 acc: 0.868700\n",
      "Epoch 1691/2000 train_loss: 1.577907 acc: 0.868900\n",
      "Epoch 1692/2000 train_loss: 1.577897 acc: 0.868800\n",
      "Epoch 1693/2000 train_loss: 1.577887 acc: 0.868600\n",
      "Epoch 1694/2000 train_loss: 1.577875 acc: 0.868800\n",
      "Epoch 1695/2000 train_loss: 1.577863 acc: 0.868800\n",
      "Epoch 1696/2000 train_loss: 1.577847 acc: 0.868900\n",
      "Epoch 1697/2000 train_loss: 1.577840 acc: 0.868700\n",
      "Epoch 1698/2000 train_loss: 1.577829 acc: 0.868800\n",
      "Epoch 1699/2000 train_loss: 1.577823 acc: 0.868600\n",
      "Epoch 1700/2000 train_loss: 1.577806 acc: 0.868600\n",
      "Epoch 1701/2000 train_loss: 1.577800 acc: 0.868700\n",
      "Epoch 1702/2000 train_loss: 1.577790 acc: 0.868500\n",
      "Epoch 1703/2000 train_loss: 1.577773 acc: 0.868900\n",
      "Epoch 1704/2000 train_loss: 1.577767 acc: 0.868800\n",
      "Epoch 1705/2000 train_loss: 1.577754 acc: 0.868900\n",
      "Epoch 1706/2000 train_loss: 1.577748 acc: 0.868900\n",
      "Epoch 1707/2000 train_loss: 1.577730 acc: 0.868300\n",
      "Epoch 1708/2000 train_loss: 1.577727 acc: 0.868700\n",
      "Epoch 1709/2000 train_loss: 1.577710 acc: 0.868800\n",
      "Epoch 1710/2000 train_loss: 1.577702 acc: 0.869100\n",
      "Epoch 1711/2000 train_loss: 1.577689 acc: 0.868900\n",
      "Epoch 1712/2000 train_loss: 1.577678 acc: 0.869000\n",
      "Epoch 1713/2000 train_loss: 1.577673 acc: 0.868900\n",
      "Epoch 1714/2000 train_loss: 1.577663 acc: 0.868900\n",
      "Epoch 1715/2000 train_loss: 1.577647 acc: 0.869200\n",
      "Epoch 1716/2000 train_loss: 1.577640 acc: 0.868600\n",
      "Epoch 1717/2000 train_loss: 1.577626 acc: 0.868600\n",
      "Epoch 1718/2000 train_loss: 1.577620 acc: 0.868700\n",
      "Epoch 1719/2000 train_loss: 1.577604 acc: 0.868600\n",
      "Epoch 1720/2000 train_loss: 1.577597 acc: 0.869100\n",
      "Epoch 1721/2000 train_loss: 1.577584 acc: 0.868600\n",
      "Epoch 1722/2000 train_loss: 1.577573 acc: 0.868700\n",
      "Epoch 1723/2000 train_loss: 1.577562 acc: 0.868800\n",
      "Epoch 1724/2000 train_loss: 1.577552 acc: 0.868800\n",
      "Epoch 1725/2000 train_loss: 1.577529 acc: 0.868700\n",
      "Epoch 1726/2000 train_loss: 1.577536 acc: 0.868800\n",
      "Epoch 1727/2000 train_loss: 1.577524 acc: 0.869200\n",
      "Epoch 1728/2000 train_loss: 1.577510 acc: 0.868900\n",
      "Epoch 1729/2000 train_loss: 1.577503 acc: 0.868900\n",
      "Epoch 1730/2000 train_loss: 1.577488 acc: 0.869100\n",
      "Epoch 1731/2000 train_loss: 1.577478 acc: 0.869000\n",
      "Epoch 1732/2000 train_loss: 1.577465 acc: 0.868800\n",
      "Epoch 1733/2000 train_loss: 1.577454 acc: 0.869100\n",
      "Epoch 1734/2000 train_loss: 1.577449 acc: 0.868700\n",
      "Epoch 1735/2000 train_loss: 1.577437 acc: 0.869200\n",
      "Epoch 1736/2000 train_loss: 1.577419 acc: 0.868700\n",
      "Epoch 1737/2000 train_loss: 1.577417 acc: 0.869000\n",
      "Epoch 1738/2000 train_loss: 1.577400 acc: 0.869100\n",
      "Epoch 1739/2000 train_loss: 1.577397 acc: 0.869100\n",
      "Epoch 1740/2000 train_loss: 1.577385 acc: 0.868900\n",
      "Epoch 1741/2000 train_loss: 1.577373 acc: 0.869300\n",
      "Epoch 1742/2000 train_loss: 1.577358 acc: 0.868400\n",
      "Epoch 1743/2000 train_loss: 1.577348 acc: 0.869100\n",
      "Epoch 1744/2000 train_loss: 1.577342 acc: 0.868900\n",
      "Epoch 1745/2000 train_loss: 1.577335 acc: 0.868900\n",
      "Epoch 1746/2000 train_loss: 1.577324 acc: 0.869100\n",
      "Epoch 1747/2000 train_loss: 1.577310 acc: 0.869300\n",
      "Epoch 1748/2000 train_loss: 1.577300 acc: 0.869000\n",
      "Epoch 1749/2000 train_loss: 1.577290 acc: 0.868600\n",
      "Epoch 1750/2000 train_loss: 1.577273 acc: 0.868900\n",
      "Epoch 1751/2000 train_loss: 1.577274 acc: 0.869100\n",
      "Epoch 1752/2000 train_loss: 1.577261 acc: 0.869000\n",
      "Epoch 1753/2000 train_loss: 1.577246 acc: 0.868700\n",
      "Epoch 1754/2000 train_loss: 1.577242 acc: 0.869000\n",
      "Epoch 1755/2000 train_loss: 1.577229 acc: 0.869300\n",
      "Epoch 1756/2000 train_loss: 1.577215 acc: 0.869200\n",
      "Epoch 1757/2000 train_loss: 1.577211 acc: 0.869200\n",
      "Epoch 1758/2000 train_loss: 1.577194 acc: 0.869200\n",
      "Epoch 1759/2000 train_loss: 1.577192 acc: 0.869100\n",
      "Epoch 1760/2000 train_loss: 1.577175 acc: 0.868700\n",
      "Epoch 1761/2000 train_loss: 1.577171 acc: 0.869200\n",
      "Epoch 1762/2000 train_loss: 1.577156 acc: 0.869000\n",
      "Epoch 1763/2000 train_loss: 1.577151 acc: 0.868900\n",
      "Epoch 1764/2000 train_loss: 1.577141 acc: 0.868700\n",
      "Epoch 1765/2000 train_loss: 1.577129 acc: 0.868800\n",
      "Epoch 1766/2000 train_loss: 1.577122 acc: 0.868800\n",
      "Epoch 1767/2000 train_loss: 1.577113 acc: 0.868800\n",
      "Epoch 1768/2000 train_loss: 1.577103 acc: 0.868700\n",
      "Epoch 1769/2000 train_loss: 1.577099 acc: 0.868900\n",
      "Epoch 1770/2000 train_loss: 1.577085 acc: 0.868700\n",
      "Epoch 1771/2000 train_loss: 1.577075 acc: 0.869100\n",
      "Epoch 1772/2000 train_loss: 1.577064 acc: 0.868500\n",
      "Epoch 1773/2000 train_loss: 1.577054 acc: 0.869000\n",
      "Epoch 1774/2000 train_loss: 1.577048 acc: 0.869100\n",
      "Epoch 1775/2000 train_loss: 1.577040 acc: 0.868900\n",
      "Epoch 1776/2000 train_loss: 1.577026 acc: 0.869000\n",
      "Epoch 1777/2000 train_loss: 1.577020 acc: 0.869100\n",
      "Epoch 1778/2000 train_loss: 1.577011 acc: 0.869000\n",
      "Epoch 1779/2000 train_loss: 1.577002 acc: 0.869000\n",
      "Epoch 1780/2000 train_loss: 1.576989 acc: 0.868500\n",
      "Epoch 1781/2000 train_loss: 1.576980 acc: 0.868800\n",
      "Epoch 1782/2000 train_loss: 1.576978 acc: 0.868800\n",
      "Epoch 1783/2000 train_loss: 1.576968 acc: 0.868700\n",
      "Epoch 1784/2000 train_loss: 1.576953 acc: 0.868900\n",
      "Epoch 1785/2000 train_loss: 1.576948 acc: 0.868900\n",
      "Epoch 1786/2000 train_loss: 1.576935 acc: 0.868500\n",
      "Epoch 1787/2000 train_loss: 1.576928 acc: 0.868900\n",
      "Epoch 1788/2000 train_loss: 1.576922 acc: 0.868900\n",
      "Epoch 1789/2000 train_loss: 1.576911 acc: 0.868800\n",
      "Epoch 1790/2000 train_loss: 1.576900 acc: 0.868200\n",
      "Epoch 1791/2000 train_loss: 1.576892 acc: 0.869100\n",
      "Epoch 1792/2000 train_loss: 1.576879 acc: 0.868600\n",
      "Epoch 1793/2000 train_loss: 1.576875 acc: 0.868900\n",
      "Epoch 1794/2000 train_loss: 1.576869 acc: 0.869100\n",
      "Epoch 1795/2000 train_loss: 1.576861 acc: 0.868600\n",
      "Epoch 1796/2000 train_loss: 1.576855 acc: 0.868900\n",
      "Epoch 1797/2000 train_loss: 1.576841 acc: 0.868800\n",
      "Epoch 1798/2000 train_loss: 1.576833 acc: 0.868700\n",
      "Epoch 1799/2000 train_loss: 1.576822 acc: 0.868800\n",
      "Epoch 1800/2000 train_loss: 1.576813 acc: 0.868700\n",
      "Epoch 1801/2000 train_loss: 1.576798 acc: 0.868600\n",
      "Epoch 1802/2000 train_loss: 1.576800 acc: 0.868900\n",
      "Epoch 1803/2000 train_loss: 1.576782 acc: 0.868800\n",
      "Epoch 1804/2000 train_loss: 1.576773 acc: 0.868500\n",
      "Epoch 1805/2000 train_loss: 1.576768 acc: 0.868800\n",
      "Epoch 1806/2000 train_loss: 1.576760 acc: 0.868500\n",
      "Epoch 1807/2000 train_loss: 1.576753 acc: 0.868700\n",
      "Epoch 1808/2000 train_loss: 1.576744 acc: 0.868800\n",
      "Epoch 1809/2000 train_loss: 1.576733 acc: 0.868800\n",
      "Epoch 1810/2000 train_loss: 1.576720 acc: 0.868900\n",
      "Epoch 1811/2000 train_loss: 1.576715 acc: 0.869000\n",
      "Epoch 1812/2000 train_loss: 1.576707 acc: 0.868700\n",
      "Epoch 1813/2000 train_loss: 1.576695 acc: 0.868400\n",
      "Epoch 1814/2000 train_loss: 1.576686 acc: 0.868600\n",
      "Epoch 1815/2000 train_loss: 1.576676 acc: 0.868900\n",
      "Epoch 1816/2000 train_loss: 1.576672 acc: 0.868800\n",
      "Epoch 1817/2000 train_loss: 1.576659 acc: 0.869000\n",
      "Epoch 1818/2000 train_loss: 1.576655 acc: 0.868800\n",
      "Epoch 1819/2000 train_loss: 1.576645 acc: 0.868800\n",
      "Epoch 1820/2000 train_loss: 1.576636 acc: 0.868600\n",
      "Epoch 1821/2000 train_loss: 1.576630 acc: 0.868900\n",
      "Epoch 1822/2000 train_loss: 1.576615 acc: 0.868800\n",
      "Epoch 1823/2000 train_loss: 1.576608 acc: 0.868600\n",
      "Epoch 1824/2000 train_loss: 1.576603 acc: 0.868800\n",
      "Epoch 1825/2000 train_loss: 1.576594 acc: 0.868500\n",
      "Epoch 1826/2000 train_loss: 1.576582 acc: 0.868900\n",
      "Epoch 1827/2000 train_loss: 1.576580 acc: 0.868900\n",
      "Epoch 1828/2000 train_loss: 1.576559 acc: 0.868600\n",
      "Epoch 1829/2000 train_loss: 1.576555 acc: 0.868700\n",
      "Epoch 1830/2000 train_loss: 1.576546 acc: 0.869000\n",
      "Epoch 1831/2000 train_loss: 1.576540 acc: 0.868400\n",
      "Epoch 1832/2000 train_loss: 1.576534 acc: 0.868800\n",
      "Epoch 1833/2000 train_loss: 1.576516 acc: 0.869000\n",
      "Epoch 1834/2000 train_loss: 1.576512 acc: 0.868800\n",
      "Epoch 1835/2000 train_loss: 1.576503 acc: 0.868600\n",
      "Epoch 1836/2000 train_loss: 1.576496 acc: 0.868800\n",
      "Epoch 1837/2000 train_loss: 1.576486 acc: 0.868600\n",
      "Epoch 1838/2000 train_loss: 1.576474 acc: 0.868900\n",
      "Epoch 1839/2000 train_loss: 1.576469 acc: 0.868800\n",
      "Epoch 1840/2000 train_loss: 1.576460 acc: 0.868500\n",
      "Epoch 1841/2000 train_loss: 1.576450 acc: 0.868900\n",
      "Epoch 1842/2000 train_loss: 1.576441 acc: 0.869000\n",
      "Epoch 1843/2000 train_loss: 1.576433 acc: 0.868700\n",
      "Epoch 1844/2000 train_loss: 1.576422 acc: 0.868800\n",
      "Epoch 1845/2000 train_loss: 1.576416 acc: 0.868500\n",
      "Epoch 1846/2000 train_loss: 1.576407 acc: 0.868700\n",
      "Epoch 1847/2000 train_loss: 1.576400 acc: 0.868800\n",
      "Epoch 1848/2000 train_loss: 1.576385 acc: 0.868600\n",
      "Epoch 1849/2000 train_loss: 1.576382 acc: 0.868700\n",
      "Epoch 1850/2000 train_loss: 1.576369 acc: 0.868800\n",
      "Epoch 1851/2000 train_loss: 1.576360 acc: 0.868600\n",
      "Epoch 1852/2000 train_loss: 1.576350 acc: 0.868800\n",
      "Epoch 1853/2000 train_loss: 1.576349 acc: 0.868900\n",
      "Epoch 1854/2000 train_loss: 1.576340 acc: 0.868800\n",
      "Epoch 1855/2000 train_loss: 1.576324 acc: 0.868900\n",
      "Epoch 1856/2000 train_loss: 1.576318 acc: 0.868600\n",
      "Epoch 1857/2000 train_loss: 1.576310 acc: 0.868900\n",
      "Epoch 1858/2000 train_loss: 1.576300 acc: 0.868600\n",
      "Epoch 1859/2000 train_loss: 1.576291 acc: 0.868800\n",
      "Epoch 1860/2000 train_loss: 1.576273 acc: 0.869000\n",
      "Epoch 1861/2000 train_loss: 1.576269 acc: 0.868800\n",
      "Epoch 1862/2000 train_loss: 1.576260 acc: 0.869000\n",
      "Epoch 1863/2000 train_loss: 1.576254 acc: 0.868900\n",
      "Epoch 1864/2000 train_loss: 1.576247 acc: 0.868400\n",
      "Epoch 1865/2000 train_loss: 1.576239 acc: 0.868800\n",
      "Epoch 1866/2000 train_loss: 1.576231 acc: 0.868700\n",
      "Epoch 1867/2000 train_loss: 1.576217 acc: 0.868500\n",
      "Epoch 1868/2000 train_loss: 1.576212 acc: 0.868700\n",
      "Epoch 1869/2000 train_loss: 1.576198 acc: 0.868600\n",
      "Epoch 1870/2000 train_loss: 1.576192 acc: 0.869000\n",
      "Epoch 1871/2000 train_loss: 1.576186 acc: 0.868800\n",
      "Epoch 1872/2000 train_loss: 1.576176 acc: 0.869000\n",
      "Epoch 1873/2000 train_loss: 1.576163 acc: 0.869000\n",
      "Epoch 1874/2000 train_loss: 1.576162 acc: 0.868700\n",
      "Epoch 1875/2000 train_loss: 1.576142 acc: 0.869000\n",
      "Epoch 1876/2000 train_loss: 1.576141 acc: 0.868900\n",
      "Epoch 1877/2000 train_loss: 1.576131 acc: 0.868800\n",
      "Epoch 1878/2000 train_loss: 1.576118 acc: 0.868700\n",
      "Epoch 1879/2000 train_loss: 1.576112 acc: 0.868900\n",
      "Epoch 1880/2000 train_loss: 1.576100 acc: 0.868700\n",
      "Epoch 1881/2000 train_loss: 1.576093 acc: 0.868900\n",
      "Epoch 1882/2000 train_loss: 1.576083 acc: 0.868900\n",
      "Epoch 1883/2000 train_loss: 1.576074 acc: 0.868600\n",
      "Epoch 1884/2000 train_loss: 1.576060 acc: 0.868900\n",
      "Epoch 1885/2000 train_loss: 1.576055 acc: 0.868800\n",
      "Epoch 1886/2000 train_loss: 1.576045 acc: 0.869300\n",
      "Epoch 1887/2000 train_loss: 1.576035 acc: 0.869000\n",
      "Epoch 1888/2000 train_loss: 1.576028 acc: 0.868900\n",
      "Epoch 1889/2000 train_loss: 1.576012 acc: 0.868900\n",
      "Epoch 1890/2000 train_loss: 1.576008 acc: 0.868900\n",
      "Epoch 1891/2000 train_loss: 1.575998 acc: 0.868500\n",
      "Epoch 1892/2000 train_loss: 1.575989 acc: 0.868700\n",
      "Epoch 1893/2000 train_loss: 1.575975 acc: 0.869200\n",
      "Epoch 1894/2000 train_loss: 1.575978 acc: 0.868700\n",
      "Epoch 1895/2000 train_loss: 1.575963 acc: 0.869100\n",
      "Epoch 1896/2000 train_loss: 1.575956 acc: 0.868700\n",
      "Epoch 1897/2000 train_loss: 1.575946 acc: 0.869000\n",
      "Epoch 1898/2000 train_loss: 1.575935 acc: 0.868900\n",
      "Epoch 1899/2000 train_loss: 1.575933 acc: 0.868900\n",
      "Epoch 1900/2000 train_loss: 1.575921 acc: 0.869100\n",
      "Epoch 1901/2000 train_loss: 1.575913 acc: 0.869000\n",
      "Epoch 1902/2000 train_loss: 1.575902 acc: 0.869000\n",
      "Epoch 1903/2000 train_loss: 1.575898 acc: 0.869000\n",
      "Epoch 1904/2000 train_loss: 1.575879 acc: 0.868900\n",
      "Epoch 1905/2000 train_loss: 1.575877 acc: 0.869000\n",
      "Epoch 1906/2000 train_loss: 1.575865 acc: 0.868900\n",
      "Epoch 1907/2000 train_loss: 1.575856 acc: 0.868900\n",
      "Epoch 1908/2000 train_loss: 1.575846 acc: 0.868900\n",
      "Epoch 1909/2000 train_loss: 1.575845 acc: 0.868700\n",
      "Epoch 1910/2000 train_loss: 1.575832 acc: 0.868800\n",
      "Epoch 1911/2000 train_loss: 1.575825 acc: 0.868800\n",
      "Epoch 1912/2000 train_loss: 1.575807 acc: 0.868600\n",
      "Epoch 1913/2000 train_loss: 1.575810 acc: 0.869000\n",
      "Epoch 1914/2000 train_loss: 1.575796 acc: 0.868700\n",
      "Epoch 1915/2000 train_loss: 1.575787 acc: 0.868900\n",
      "Epoch 1916/2000 train_loss: 1.575782 acc: 0.868900\n",
      "Epoch 1917/2000 train_loss: 1.575771 acc: 0.868900\n",
      "Epoch 1918/2000 train_loss: 1.575763 acc: 0.868800\n",
      "Epoch 1919/2000 train_loss: 1.575756 acc: 0.868900\n",
      "Epoch 1920/2000 train_loss: 1.575742 acc: 0.869100\n",
      "Epoch 1921/2000 train_loss: 1.575743 acc: 0.869000\n",
      "Epoch 1922/2000 train_loss: 1.575725 acc: 0.868700\n",
      "Epoch 1923/2000 train_loss: 1.575726 acc: 0.868800\n",
      "Epoch 1924/2000 train_loss: 1.575713 acc: 0.868800\n",
      "Epoch 1925/2000 train_loss: 1.575705 acc: 0.868800\n",
      "Epoch 1926/2000 train_loss: 1.575698 acc: 0.869000\n",
      "Epoch 1927/2000 train_loss: 1.575688 acc: 0.868800\n",
      "Epoch 1928/2000 train_loss: 1.575676 acc: 0.869100\n",
      "Epoch 1929/2000 train_loss: 1.575672 acc: 0.868900\n",
      "Epoch 1930/2000 train_loss: 1.575663 acc: 0.869300\n",
      "Epoch 1931/2000 train_loss: 1.575655 acc: 0.868400\n",
      "Epoch 1932/2000 train_loss: 1.575648 acc: 0.869100\n",
      "Epoch 1933/2000 train_loss: 1.575641 acc: 0.868800\n",
      "Epoch 1934/2000 train_loss: 1.575635 acc: 0.868800\n",
      "Epoch 1935/2000 train_loss: 1.575624 acc: 0.868900\n",
      "Epoch 1936/2000 train_loss: 1.575615 acc: 0.869000\n",
      "Epoch 1937/2000 train_loss: 1.575604 acc: 0.869300\n",
      "Epoch 1938/2000 train_loss: 1.575602 acc: 0.868900\n",
      "Epoch 1939/2000 train_loss: 1.575590 acc: 0.868700\n",
      "Epoch 1940/2000 train_loss: 1.575582 acc: 0.868800\n",
      "Epoch 1941/2000 train_loss: 1.575576 acc: 0.869000\n",
      "Epoch 1942/2000 train_loss: 1.575562 acc: 0.869100\n",
      "Epoch 1943/2000 train_loss: 1.575554 acc: 0.868900\n",
      "Epoch 1944/2000 train_loss: 1.575554 acc: 0.869000\n",
      "Epoch 1945/2000 train_loss: 1.575539 acc: 0.869100\n",
      "Epoch 1946/2000 train_loss: 1.575536 acc: 0.868900\n",
      "Epoch 1947/2000 train_loss: 1.575528 acc: 0.869300\n",
      "Epoch 1948/2000 train_loss: 1.575526 acc: 0.869000\n",
      "Epoch 1949/2000 train_loss: 1.575512 acc: 0.869000\n",
      "Epoch 1950/2000 train_loss: 1.575507 acc: 0.869200\n",
      "Epoch 1951/2000 train_loss: 1.575500 acc: 0.869100\n",
      "Epoch 1952/2000 train_loss: 1.575486 acc: 0.869300\n",
      "Epoch 1953/2000 train_loss: 1.575483 acc: 0.869100\n",
      "Epoch 1954/2000 train_loss: 1.575472 acc: 0.869300\n",
      "Epoch 1955/2000 train_loss: 1.575464 acc: 0.869200\n",
      "Epoch 1956/2000 train_loss: 1.575463 acc: 0.868800\n",
      "Epoch 1957/2000 train_loss: 1.575457 acc: 0.869200\n",
      "Epoch 1958/2000 train_loss: 1.575446 acc: 0.869000\n",
      "Epoch 1959/2000 train_loss: 1.575434 acc: 0.868700\n",
      "Epoch 1960/2000 train_loss: 1.575425 acc: 0.869200\n",
      "Epoch 1961/2000 train_loss: 1.575422 acc: 0.869100\n",
      "Epoch 1962/2000 train_loss: 1.575416 acc: 0.869100\n",
      "Epoch 1963/2000 train_loss: 1.575402 acc: 0.869200\n",
      "Epoch 1964/2000 train_loss: 1.575400 acc: 0.869000\n",
      "Epoch 1965/2000 train_loss: 1.575395 acc: 0.869200\n",
      "Epoch 1966/2000 train_loss: 1.575386 acc: 0.869000\n",
      "Epoch 1967/2000 train_loss: 1.575379 acc: 0.869100\n",
      "Epoch 1968/2000 train_loss: 1.575371 acc: 0.868900\n",
      "Epoch 1969/2000 train_loss: 1.575361 acc: 0.869300\n",
      "Epoch 1970/2000 train_loss: 1.575360 acc: 0.869100\n",
      "Epoch 1971/2000 train_loss: 1.575345 acc: 0.869200\n",
      "Epoch 1972/2000 train_loss: 1.575338 acc: 0.869200\n",
      "Epoch 1973/2000 train_loss: 1.575336 acc: 0.869000\n",
      "Epoch 1974/2000 train_loss: 1.575327 acc: 0.869300\n",
      "Epoch 1975/2000 train_loss: 1.575321 acc: 0.869000\n",
      "Epoch 1976/2000 train_loss: 1.575314 acc: 0.869500\n",
      "Epoch 1977/2000 train_loss: 1.575311 acc: 0.869200\n",
      "Epoch 1978/2000 train_loss: 1.575298 acc: 0.868900\n",
      "Epoch 1979/2000 train_loss: 1.575287 acc: 0.869100\n",
      "Epoch 1980/2000 train_loss: 1.575287 acc: 0.869100\n",
      "Epoch 1981/2000 train_loss: 1.575279 acc: 0.869100\n",
      "Epoch 1982/2000 train_loss: 1.575269 acc: 0.869200\n",
      "Epoch 1983/2000 train_loss: 1.575267 acc: 0.869200\n",
      "Epoch 1984/2000 train_loss: 1.575260 acc: 0.869400\n",
      "Epoch 1985/2000 train_loss: 1.575251 acc: 0.869000\n",
      "Epoch 1986/2000 train_loss: 1.575245 acc: 0.869200\n",
      "Epoch 1987/2000 train_loss: 1.575235 acc: 0.869200\n",
      "Epoch 1988/2000 train_loss: 1.575232 acc: 0.869000\n",
      "Epoch 1989/2000 train_loss: 1.575223 acc: 0.869400\n",
      "Epoch 1990/2000 train_loss: 1.575217 acc: 0.869200\n",
      "Epoch 1991/2000 train_loss: 1.575211 acc: 0.869500\n",
      "Epoch 1992/2000 train_loss: 1.575204 acc: 0.869100\n",
      "Epoch 1993/2000 train_loss: 1.575195 acc: 0.869200\n",
      "Epoch 1994/2000 train_loss: 1.575189 acc: 0.869400\n",
      "Epoch 1995/2000 train_loss: 1.575183 acc: 0.869000\n",
      "Epoch 1996/2000 train_loss: 1.575174 acc: 0.869600\n",
      "Epoch 1997/2000 train_loss: 1.575168 acc: 0.869300\n",
      "Epoch 1998/2000 train_loss: 1.575162 acc: 0.869100\n",
      "Epoch 1999/2000 train_loss: 1.575153 acc: 0.869300\n",
      "Epoch 2000/2000 train_loss: 1.575150 acc: 0.869200\n"
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "\n",
    "for epoch in range(1,epochs+1):\n",
    "\n",
    "    loss_sum = 0.0\n",
    "\n",
    "    metric = MulticlassAccuracy()\n",
    "\n",
    "    for (train_features,train_labels),(test_features,test_labels) in zip(train_dataloader,test_dataloader) :\n",
    "\n",
    "        epoch_y = net(train_features)\n",
    "        loss = loss_fc(epoch_y.to(torch.float32),train_labels.to(torch.float32))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 학습중인 내용을 IMG로 보려 하였던 노력의 잔해.\n",
    "        \"\"\"if (epoch) % 100 == 0 :\n",
    "            net_idx = 0\n",
    "            for m,layer in zip(net.modules(),net.hidden_layer) :\n",
    "                if isinstance(m,nn.Linear):\n",
    "                    \n",
    "                    img_tensor = m.weight.clone().detach()\n",
    "                    img = resize_tensor(img_tensor.squeeze())\n",
    "                \n",
    "                    img = img.reshape(layer,-1,-1)\n",
    "                    np_arr = np.array(img,dtype=np.uint8)\n",
    "                                        \n",
    "                    for img_num ,i in zip(np_arr,range(len(np_arr))):\n",
    "                        writer.add_image(f'weight_{net_idx}_img_{i}',img_num,epoch,dataformats='WH')\n",
    "                    net_idx += 1\"\"\"\n",
    "\n",
    "        loss_sum += loss\n",
    "        \n",
    "        epoch_test_y = net(test_features)\n",
    "        epoch_class_y = torch.argmax(epoch_test_y,dim=-1)\n",
    "        test_class_y = torch.argmax(test_labels,dim=-1)\n",
    "\n",
    "        metric.update(epoch_class_y,test_class_y)\n",
    "        \n",
    "    acc = metric.compute()\n",
    "\n",
    "    loss = loss_sum / len(train_dataloader)\n",
    "\n",
    "    print('Epoch {:4d}/{} train_loss: {:.6f} acc: {:.6f}'.format(\n",
    "        epoch, epochs, loss, acc\n",
    "    ))\n",
    "\n",
    "    if loss < 0.1 :\n",
    "        \"\"\"net_idx = 0\n",
    "        for m,layer in zip(net.modules(),net.hidden_layer) :\n",
    "            if isinstance(m,nn.Linear):\n",
    "            \n",
    "                img_tensor = m.weight.clone().detach()\n",
    "                img = resize_tensor(img_tensor.squeeze())\n",
    "                \n",
    "                img = img.reshape(layer,-1,-1)\n",
    "                np_arr = np.array(img,dtype=np.uint8)\n",
    "                                        \n",
    "                for img_num ,i in zip(np_arr,range(len(np_arr))):\n",
    "                    writer.add_image(f'weight_{net_idx}_img_{i}',img_num,epoch,dataformats='WH')\n",
    "                net_idx += 1\"\"\"\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27542/3555199059.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  x = torch.tensor([test_dataset[40][0]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([test_dataset[40][0]])\n",
    "\n",
    "pre_y = net(x)\n",
    "\n",
    "pre_y = pre_y.squeeze()\n",
    "\n",
    "pre_max = 0\n",
    "temp = -1\n",
    "\n",
    "for pre_data,i in zip(pre_y,range(len(pre_y))):\n",
    "    \n",
    "    if pre_data > pre_max:\n",
    "        temp = i\n",
    "        pre_max = pre_data\n",
    "temp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.imshow(x.squeeze().reshape(28,28).numpy(),cmap='gray')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
